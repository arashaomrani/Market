{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "from numpy import concatenate\n",
    "from matplotlib import pyplot\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, scale\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras import optimizers\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.layers import Add, LSTM, Activation, Flatten, Dropout, Lambda, Conv1D, Input, BatchNormalization\n",
    "from keras.models import Model\n",
    "#import plotly.offline as py\n",
    "#import plotly.graph_objs as go\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import TimeseriesGenerator\n",
    "from sklearn.utils import shuffle\n",
    "#py.init_notebook_mode(connected=True)\n",
    "#from googlefinance.client import get_price_data, get_prices_data, get_prices_time_data\n",
    "#import fix_yahoo_finance as yf\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4297</th>\n",
       "      <td>2018-07-27</td>\n",
       "      <td>194.990005</td>\n",
       "      <td>195.190002</td>\n",
       "      <td>190.100006</td>\n",
       "      <td>190.979996</td>\n",
       "      <td>190.979996</td>\n",
       "      <td>24024000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4298</th>\n",
       "      <td>2018-07-30</td>\n",
       "      <td>191.899994</td>\n",
       "      <td>192.199997</td>\n",
       "      <td>189.070007</td>\n",
       "      <td>189.910004</td>\n",
       "      <td>189.910004</td>\n",
       "      <td>21029500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4299</th>\n",
       "      <td>2018-07-31</td>\n",
       "      <td>190.300003</td>\n",
       "      <td>192.139999</td>\n",
       "      <td>189.339996</td>\n",
       "      <td>190.289993</td>\n",
       "      <td>190.289993</td>\n",
       "      <td>39373000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4300</th>\n",
       "      <td>2018-08-01</td>\n",
       "      <td>199.130005</td>\n",
       "      <td>201.759995</td>\n",
       "      <td>197.309998</td>\n",
       "      <td>201.500000</td>\n",
       "      <td>201.500000</td>\n",
       "      <td>67841600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4301</th>\n",
       "      <td>2018-08-02</td>\n",
       "      <td>200.580002</td>\n",
       "      <td>208.380005</td>\n",
       "      <td>200.350006</td>\n",
       "      <td>207.800003</td>\n",
       "      <td>207.800003</td>\n",
       "      <td>50325158</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date        Open        High         Low       Close   Adj Close  \\\n",
       "4297  2018-07-27  194.990005  195.190002  190.100006  190.979996  190.979996   \n",
       "4298  2018-07-30  191.899994  192.199997  189.070007  189.910004  189.910004   \n",
       "4299  2018-07-31  190.300003  192.139999  189.339996  190.289993  190.289993   \n",
       "4300  2018-08-01  199.130005  201.759995  197.309998  201.500000  201.500000   \n",
       "4301  2018-08-02  200.580002  208.380005  200.350006  207.800003  207.800003   \n",
       "\n",
       "        Volume  \n",
       "4297  24024000  \n",
       "4298  21029500  \n",
       "4299  39373000  \n",
       "4300  67841600  \n",
       "4301  50325158  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Stock=\"AAPL\"\n",
    "param = {\n",
    "    'q': Stock, # Stock symbol (ex: \"AAPL\")\n",
    "    'i': \"86400\", # Interval size in seconds (\"86400\" = 1 day intervals)\n",
    "    # Stock exchange symbol on which stock is traded (ex: \"NASD\")\n",
    "     'p':'5Y'# Period (Ex: \"1Y\" = 1 year)\n",
    "}\n",
    "# get price data (return pandas dataframe)\n",
    "#df = get_price_data(param)\n",
    "  \n",
    "#df = yf.download(Stock,'2001-10-06')\n",
    "#df.to_csv(Stock+'.csv')\n",
    "df=pd.read_csv(Stock+'.csv')\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=df\n",
    "data['Close'].replace(0, np.nan, inplace=True)\n",
    "data['Close'].fillna(method='ffill', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Bolinger_Bands(stock_price, window_size, num_of_std):\n",
    "\n",
    "    rolling_mean = stock_price.rolling(window=window_size).mean()\n",
    "    rolling_std  = stock_price.rolling(window=window_size).std()\n",
    "    upper_band = rolling_mean + (rolling_std*num_of_std)\n",
    "    lower_band = rolling_mean - (rolling_std*num_of_std)\n",
    "\n",
    "    return rolling_mean, upper_band, lower_band\n",
    "Avg, Upper, Lower = Bolinger_Bands(data['Close'], 10, 2)\n",
    "def OBV(data):\n",
    "    last_obv = 0\n",
    "    obv = [last_obv]\n",
    "    for i in range(1,len(data)):\n",
    "        if data['Close'][i] >= data['Close'][i-1]:\n",
    "            obv.append(last_obv + data['Volume'][i])\n",
    "        else:\n",
    "            obv.append(last_obv - data['Volume'][i])\n",
    "        last_obv = obv[-1]\n",
    "    return pd.DataFrame(obv, columns=['OBV'])\n",
    "def Bias(data, period=6):\n",
    "    MA = data['Close'].rolling(window=period).mean()\n",
    "    bias=[]\n",
    "    for i in range(len(data)):\n",
    "        bias.append(((data['Close'][i]-MA[i])/MA[i])*100)\n",
    "    return pd.DataFrame(bias, columns=['Bias'])\n",
    "def PSY(data, period=12):\n",
    "    psy = [np.nan]*(period-1)\n",
    "    for i in range(len(data)-period+1):\n",
    "        diff = np.ediff1d(data['Close'][i:i+period])\n",
    "        psy.append((len(diff[diff>=0])/len(diff))*100)\n",
    "    return pd.DataFrame(psy, columns=['PSY'])\n",
    "def SY(data, i, p):\n",
    "    return ((data['Close'][i-p]-data['Close'][i-p-1])/data['Close'][i-p-1])*100\n",
    "\n",
    "def ASY(data, period):\n",
    "    if period == 1:\n",
    "        asy = [np.nan]*2\n",
    "        for i in range(2,len(data)):\n",
    "            asy.append(((data['Close'][i-1]-data['Close'][i-2])/data['Close'][i-2])*100)\n",
    "    else:\n",
    "        asy = [np.nan]*period\n",
    "        for i in range(period,len(data)):\n",
    "            A=0\n",
    "            for j in range(period):\n",
    "                A = A + SY(data, i,j)\n",
    "            A = A/(j+1)\n",
    "            asy.append(A)\n",
    "            \n",
    "    return pd.DataFrame(asy, columns=['ASY'+str(period)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aalahgholipour160413\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:189: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from stockstats import StockDataFrame\n",
    "indicators = ['close_50_sma','close_150_sma','close_20_ema','close_40_ema','boll','boll_ub','boll_lb',\\\n",
    "             'macd','kdjk','kdjd','kdjj','atr','adx','vr','rsi_14']\n",
    "for i in indicators:\n",
    "    df = StockDataFrame.retype(data)\n",
    "    df = df.get(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4302, 7)\n",
      "(4302, 7)\n"
     ]
    }
   ],
   "source": [
    "print(data.shape)\n",
    "data.replace([np.inf, -np.inf], np.nan)\n",
    "data = data.dropna()\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff=np.ediff1d(data.iloc[:,3])\n",
    "diff.shape\n",
    "Data = np.append(data.values[:-1,:], diff.reshape(-1,1), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4299, 57)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alahghol\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "#values = data[['Weighted'] + ['Volume (BTC)'] + ['Volume (Currency)']].values\n",
    "values = data[['Open'] + ['High'] + ['Low'] + ['Close'] + ['Volume']]\n",
    "# values.insert(5,'OBV',OBV(data))\n",
    "# #values.insert(2,'Volume',data['Volume'])#.rolling(window=50).mean())\n",
    "# values.insert(6,'MA5',data['Close'].rolling(window=5).mean())\n",
    "# values.insert(7,'Bias',Bias(data,6))\n",
    "# values.insert(8,'PSY12',PSY(data,12))\n",
    "# values.insert(9,'ASY1',ASY(data,1))\n",
    "# values.insert(10,'ASY2',ASY(data,2))\n",
    "# values.insert(11,'ASY3',ASY(data,3))\n",
    "# values.insert(12,'ASY4',ASY(data,4))\n",
    "# values.insert(13,'ASY5',ASY(data,5))\n",
    "# diff = np.ediff1d(data['Close'])\n",
    "# diff=np.append(np.array([0]),diff)\n",
    "# values.insert(14,'diff',diff)\n",
    "#values.insert(4,'MA150',data['Close'].rolling(window=150).mean())\n",
    "#values.insert(5, 'EMA20', data['Close'].ewm(span=20, adjust=False).mean())\n",
    "#values.insert(13, 'Lower', Lower)\n",
    "#values.insert(14, 'Avg', Avg)\n",
    "#values.insert(15, 'Upper', Upper)\n",
    "#values.insert(5,'open', data['Open'])\n",
    "#values=values.iloc[:,1:]\n",
    "values.dropna(inplace=True)\n",
    "#values=values.values\n",
    "values = values.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>OBV</th>\n",
       "      <th>MA5</th>\n",
       "      <th>Bias</th>\n",
       "      <th>PSY12</th>\n",
       "      <th>ASY1</th>\n",
       "      <th>ASY2</th>\n",
       "      <th>ASY3</th>\n",
       "      <th>ASY4</th>\n",
       "      <th>ASY5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.723571</td>\n",
       "      <td>1.786429</td>\n",
       "      <td>1.702857</td>\n",
       "      <td>1.775000</td>\n",
       "      <td>113685600.0</td>\n",
       "      <td>244389600.0</td>\n",
       "      <td>1.651286</td>\n",
       "      <td>8.341811</td>\n",
       "      <td>54.545456</td>\n",
       "      <td>8.074534</td>\n",
       "      <td>5.043014</td>\n",
       "      <td>5.569515</td>\n",
       "      <td>2.459068</td>\n",
       "      <td>2.575527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.777143</td>\n",
       "      <td>1.792857</td>\n",
       "      <td>1.707857</td>\n",
       "      <td>1.711429</td>\n",
       "      <td>69666800.0</td>\n",
       "      <td>174722800.0</td>\n",
       "      <td>1.669286</td>\n",
       "      <td>3.016857</td>\n",
       "      <td>45.454544</td>\n",
       "      <td>2.011494</td>\n",
       "      <td>-0.784985</td>\n",
       "      <td>2.168188</td>\n",
       "      <td>3.281770</td>\n",
       "      <td>1.250962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.712857</td>\n",
       "      <td>1.801429</td>\n",
       "      <td>1.643571</td>\n",
       "      <td>1.792857</td>\n",
       "      <td>161957600.0</td>\n",
       "      <td>336680384.0</td>\n",
       "      <td>1.725857</td>\n",
       "      <td>6.093684</td>\n",
       "      <td>54.545456</td>\n",
       "      <td>-3.581465</td>\n",
       "      <td>0.588215</td>\n",
       "      <td>1.062642</td>\n",
       "      <td>2.815615</td>\n",
       "      <td>3.576995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.555714</td>\n",
       "      <td>1.627143</td>\n",
       "      <td>1.458571</td>\n",
       "      <td>1.485000</td>\n",
       "      <td>284253184.0</td>\n",
       "      <td>52427200.0</td>\n",
       "      <td>1.700857</td>\n",
       "      <td>-11.906782</td>\n",
       "      <td>45.454544</td>\n",
       "      <td>4.757895</td>\n",
       "      <td>-6.206707</td>\n",
       "      <td>-5.331626</td>\n",
       "      <td>-3.495846</td>\n",
       "      <td>-1.181770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1.516429</td>\n",
       "      <td>1.530000</td>\n",
       "      <td>1.410714</td>\n",
       "      <td>1.425714</td>\n",
       "      <td>215284992.0</td>\n",
       "      <td>-162857792.0</td>\n",
       "      <td>1.638000</td>\n",
       "      <td>-13.854139</td>\n",
       "      <td>45.454544</td>\n",
       "      <td>-17.171309</td>\n",
       "      <td>-10.581816</td>\n",
       "      <td>-5.468579</td>\n",
       "      <td>-4.996800</td>\n",
       "      <td>-3.595141</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Open      High       Low     Close       Volume          OBV  \\\n",
       "11  1.723571  1.786429  1.702857  1.775000  113685600.0  244389600.0   \n",
       "12  1.777143  1.792857  1.707857  1.711429   69666800.0  174722800.0   \n",
       "13  1.712857  1.801429  1.643571  1.792857  161957600.0  336680384.0   \n",
       "14  1.555714  1.627143  1.458571  1.485000  284253184.0   52427200.0   \n",
       "15  1.516429  1.530000  1.410714  1.425714  215284992.0 -162857792.0   \n",
       "\n",
       "         MA5       Bias      PSY12       ASY1       ASY2      ASY3      ASY4  \\\n",
       "11  1.651286   8.341811  54.545456   8.074534   5.043014  5.569515  2.459068   \n",
       "12  1.669286   3.016857  45.454544   2.011494  -0.784985  2.168188  3.281770   \n",
       "13  1.725857   6.093684  54.545456  -3.581465   0.588215  1.062642  2.815615   \n",
       "14  1.700857 -11.906782  45.454544   4.757895  -6.206707 -5.331626 -3.495846   \n",
       "15  1.638000 -13.854139  45.454544 -17.171309 -10.581816 -5.468579 -4.996800   \n",
       "\n",
       "        ASY5  \n",
       "11  2.575527  \n",
       "12  1.250962  \n",
       "13  3.576995  \n",
       "14 -1.181770  \n",
       "15 -3.595141  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "#values = data.iloc[1:,:]\n",
    "values = values.values\n",
    "#values = Data\n",
    "#values = values[~np.isnan(values).any(axis=1)]\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaler.fit(values)\n",
    "scaled = scaler.transform(values)\n",
    "#scaler = StandardScaler()\n",
    "#scaler.fit(values)\n",
    "#scaled = scaler.transform(values)\n",
    "#scaled = normalize(values, norm='l2',axis=0)\n",
    "#scaled = scale(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled = (values.values[:-1,:]/values.values[1:,:])\n",
    "scaled = pd.DataFrame(scaled)\n",
    "#scaled.replace([np.inf, -np.inf], np.nan)\n",
    "#scaled = scaled.dropna()\n",
    "all_inf_or_nan = scaled.isin([np.inf, -np.inf, np.nan]).all(axis='columns')\n",
    "scaled = scaled[~all_inf_or_nan]\n",
    "scaled = scaled.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xf18d390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.hist(scaled[:,3])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alahghol\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4261, 15, 5) (4261, 2) (20, 15, 5) (20, 2)\n",
      "(4256, 15, 5) (4256, 2) (20, 15, 5) (20, 2)\n",
      "(4199, 15, 5) (4199, 2) (20, 15, 5) (20, 2)\n",
      "(4256, 15, 5) (4256, 2) (20, 15, 5) (20, 2)\n",
      "(4261, 15, 5) (4261, 2) (20, 15, 5) (20, 2)\n",
      "(4256, 15, 5) (4256, 2) (20, 15, 5) (20, 2)\n",
      "(4256, 15, 5) (4256, 2) (20, 15, 5) (20, 2)\n",
      "(4256, 15, 5) (4256, 2) (20, 15, 5) (20, 2)\n",
      "(4265, 15, 5) (4265, 2) (20, 15, 5) (20, 2)\n",
      "(4256, 15, 5) (4256, 2) (20, 15, 5) (20, 2)\n"
     ]
    }
   ],
   "source": [
    "stocks=['OI','NI','UPS','HSIC','CCE','IBM','MAS','EFX','AAPL','WDC']\n",
    "Train_X=np.empty((0,15,5))\n",
    "Train_y=np.empty((0,2))\n",
    "Test_X=np.empty((0,15,5))\n",
    "Test_y=np.empty((0,2))\n",
    "for Stock in stocks:\n",
    "    df=pd.read_csv(Stock+'.csv')\n",
    "    data=df\n",
    "    data['Close'].replace(0, np.nan, inplace=True)\n",
    "    data['Close'].fillna(method='ffill', inplace=True)\n",
    "    values = data[['Open'] + ['High'] + ['Low'] + ['Close'] + ['Volume']]\n",
    "    values.dropna(inplace=True)\n",
    "    values = values.astype('float32')\n",
    "    scaled = (values.values[:-1,:]/values.values[1:,:])\n",
    "    scaled = pd.DataFrame(scaled)\n",
    "    all_inf_or_nan = scaled.isin([np.inf, -np.inf, np.nan]).all(axis='columns')\n",
    "    scaled = scaled[~all_inf_or_nan]\n",
    "    scaled = scaled.values\n",
    "    scaled2=scaled.copy()\n",
    "    s=np.append(scaled, np.zeros((1,scaled.shape[1])),axis=0)\n",
    "    data_gen = TimeseriesGenerator(scaled[:], scaled[:,3],\n",
    "                                   length=15, sampling_rate=1,\n",
    "                                   stride=1, batch_size=len(s-1))\n",
    "    X, y = data_gen[0]\n",
    "    y = np.ediff1d(scaled[:,3])\n",
    "    data_gen = TimeseriesGenerator(y, y,\n",
    "                                  length=15, sampling_rate=1,\n",
    "                                  stride=1, batch_size=len(y))\n",
    "    _, y_seq = data_gen[0]\n",
    "    y=y_seq\n",
    "    X = X[:-1]\n",
    "    y1 = np.empty([len(y)], dtype=np.float32)\n",
    "    for i in range(len(y)):\n",
    "        if y[i] >= 0.0:\n",
    "            y1[i] = 1.0\n",
    "        else:\n",
    "            y1[i] = 0.0\n",
    "    y1=to_categorical(y1)\n",
    "    train_X = X[:-20]\n",
    "    train_y = y1[:-20]\n",
    "    train_X, train_y = shuffle(train_X, train_y, random_state = 1)\n",
    "    test_X = X[-20:]\n",
    "    test_y = y1[-20:]\n",
    "    print(train_X.shape, train_y.shape, test_X.shape, test_y.shape)\n",
    "    Train_X = np.concatenate((Train_X,train_X),axis=0)\n",
    "    Train_y = np.concatenate((Train_y,train_y),axis=0)\n",
    "    Test_X = np.concatenate((Test_X,test_X),axis=0)\n",
    "    Test_y = np.concatenate((Test_y,test_y),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42522, 15, 5) (42522, 2) (200, 15, 5) (200, 2)\n"
     ]
    }
   ],
   "source": [
    "print(Train_X.shape, Train_y.shape, Test_X.shape, Test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4286, 15, 5) (4286,)\n",
      "(4300,)\n",
      "(4285,)\n",
      "(4285, 15, 5) (4285,)\n",
      "(4285,)\n",
      "(4285, 2)\n",
      "(4265, 15, 5) (4265, 2) (300, 15, 5) (300, 2) (20, 15, 5) (20, 2)\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import TimeseriesGenerator\n",
    "from sklearn.utils import shuffle\n",
    "#scaled=values\n",
    "scaled2=scaled.copy()\n",
    "#scaled=scaled[:-15]\n",
    "s=np.append(scaled, np.zeros((1,scaled.shape[1])),axis=0)\n",
    "data_gen = TimeseriesGenerator(scaled[:], scaled[:,3],\n",
    "                               length=15, sampling_rate=1,\n",
    "                               stride=1, batch_size=len(s-1))\n",
    "X, y = data_gen[0]\n",
    "print(X.shape, y.shape)\n",
    "y = np.ediff1d(scaled[:,3])\n",
    "print(y.shape)\n",
    "data_gen = TimeseriesGenerator(y, y,\n",
    "                              length=15, sampling_rate=1,\n",
    "                              stride=1, batch_size=len(y))\n",
    "_, y_seq = data_gen[0]\n",
    "print(y_seq.shape)\n",
    "y=y_seq\n",
    "X = X[:-1]\n",
    "print(X.shape, y.shape)\n",
    "y1 = np.empty([len(y)], dtype=np.float32)\n",
    "for i in range(len(y)):\n",
    "    if y[i] >= 0.0:\n",
    "        y1[i] = 1.0\n",
    "    else:\n",
    "        y1[i] = 0.0\n",
    "\n",
    "#scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "print(y1.shape)\n",
    "#y1=y1[:,-1]\n",
    "y1=to_categorical(y1)\n",
    "print(y1.shape)\n",
    "# scaler = StandardScaler()\n",
    "#scaler.fit(y.reshape(-1, 1))\n",
    "#y1 = scaler.transform(y.reshape(-1, 1))\n",
    "# #y1 = scale(y)\n",
    "# print(X.shape, y1.shape)\n",
    "#y1=y1*20\n",
    "#y1=y\n",
    "#X, y1 = shuffle(X, y1, random_state = 0)\n",
    "train_X = X[:-20]\n",
    "train_y = y1[:-20]\n",
    "train_X, train_y = shuffle(train_X, train_y, random_state = 1)\n",
    "valid_X = X[3500:3800]\n",
    "valid_y = y1[3500:3800]\n",
    "test_X = X[-20:]\n",
    "test_y = y1[-20:]\n",
    "print(train_X.shape, train_y.shape, valid_X.shape, valid_y.shape, test_X.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1536/4265 [=========>....................] - ETA: 47s - loss: 0.8911 - acc: 0.5195"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-3b25bec6f991>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     48\u001b[0m                               patience=5, min_lr=1e-8)\n\u001b[0;32m     49\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'categorical_crossentropy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1035\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1036\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1037\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1038\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1039\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2664\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2665\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2666\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2667\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2668\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2634\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2635\u001b[0m                                 session)\n\u001b[1;32m-> 2636\u001b[1;33m         \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2637\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2638\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1380\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1381\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1382\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1383\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1384\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras import metrics\n",
    "from keras.initializers import he_normal, lecun_normal\n",
    "from keras.layers import Activation\n",
    "from keras import backend as K\n",
    "from keras.utils.generic_utils import get_custom_objects\n",
    "def custom_activation(x):\n",
    "    return (K.sigmoid(x*10)) \n",
    "\n",
    "get_custom_objects().update({'custom_activation': Activation(custom_activation)})\n",
    "\n",
    "def tcn(h,filters=500, kernel_size=3, padding='causal', activation='relu', dilation_rate=1):\n",
    "    main = BatchNormalization()(h)\n",
    "    main = h\n",
    "    init = lecun_normal(seed=1)\n",
    "    for i in range(1):\n",
    "        main = Conv1D(filters=filters,kernel_size=kernel_size,padding=padding, dilation_rate=dilation_rate, kernel_initializer=init)(main)\n",
    "        main = Activation('relu')(main)\n",
    "        main = BatchNormalization()(main)       \n",
    "        main = Dropout(0.25)(main)\n",
    "    side_path = Conv1D(filters=filters,kernel_size=1, padding='same', kernel_initializer=init)(h)\n",
    "    #side_path = BatchNormalization()(side_path)\n",
    "    return Add()([main,side_path])\n",
    "\n",
    "Inp = Input(shape=(15,5))\n",
    "inp = Inp\n",
    "D = [1,2,4,1,1]\n",
    "for i in range(3):\n",
    "    inp=tcn(inp,dilation_rate=D[i])\n",
    "    #inp = BatchNormalization()(inp)\n",
    "    #inp = Activation('relu')(inp)\n",
    "inp=Flatten()(inp)\n",
    "init = lecun_normal(seed=1)\n",
    "inp=Dense(100)(inp)\n",
    "inp = Activation('relu')(inp)\n",
    "inp = BatchNormalization()(inp)\n",
    "inp = Dropout(0.25)(inp)\n",
    "inp=Dense(10)(inp)\n",
    "inp = Activation('relu')(inp)\n",
    "inp = BatchNormalization()(inp)\n",
    "#inp = Dropout(0.25)(inp)\n",
    "out=Dense(2,activation='softmax')(inp)\n",
    "#inp = Lambda(lambda x: x * 2)(inp)\n",
    "#inp = Lambda(lambda x: x/100)(inp)\n",
    "model = Model(Inp,out)\n",
    "ad = optimizers.Adam(lr=0.001)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1,\n",
    "                              patience=5, min_lr=1e-8)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=ad, metrics=['accuracy'])\n",
    "history = model.fit(train_X, train_y, epochs=50, batch_size=128, verbose=1, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3784 samples, validate on 421 samples\n",
      "Epoch 1/1000\n",
      "3784/3784 [==============================] - 1s 235us/step - loss: 0.6903 - acc: 0.5285 - val_loss: 0.6915 - val_acc: 0.5392\n",
      "Epoch 2/1000\n",
      "3784/3784 [==============================] - 1s 237us/step - loss: 0.6910 - acc: 0.5280 - val_loss: 0.6910 - val_acc: 0.5416\n",
      "Epoch 3/1000\n",
      "3784/3784 [==============================] - 1s 233us/step - loss: 0.6894 - acc: 0.5254 - val_loss: 0.6906 - val_acc: 0.5487\n",
      "Epoch 4/1000\n",
      "3784/3784 [==============================] - 1s 235us/step - loss: 0.6894 - acc: 0.5262 - val_loss: 0.6916 - val_acc: 0.5368\n",
      "Epoch 5/1000\n",
      "3784/3784 [==============================] - 1s 234us/step - loss: 0.6895 - acc: 0.5307 - val_loss: 0.6916 - val_acc: 0.5321\n",
      "Epoch 6/1000\n",
      "3784/3784 [==============================] - 1s 234us/step - loss: 0.6910 - acc: 0.5246 - val_loss: 0.6917 - val_acc: 0.5321\n",
      "Epoch 7/1000\n",
      "3784/3784 [==============================] - 1s 234us/step - loss: 0.6923 - acc: 0.5206 - val_loss: 0.6906 - val_acc: 0.5392\n",
      "Epoch 8/1000\n",
      "3784/3784 [==============================] - 1s 241us/step - loss: 0.6892 - acc: 0.5312 - val_loss: 0.6905 - val_acc: 0.5392\n",
      "Epoch 9/1000\n",
      "3784/3784 [==============================] - 1s 251us/step - loss: 0.6892 - acc: 0.5288 - val_loss: 0.6901 - val_acc: 0.5392\n",
      "Epoch 10/1000\n",
      "3784/3784 [==============================] - 1s 236us/step - loss: 0.6910 - acc: 0.5270 - val_loss: 0.6898 - val_acc: 0.5392\n",
      "Epoch 11/1000\n",
      "3784/3784 [==============================] - 1s 234us/step - loss: 0.6895 - acc: 0.5235 - val_loss: 0.6895 - val_acc: 0.5392\n",
      "Epoch 12/1000\n",
      "3784/3784 [==============================] - 1s 251us/step - loss: 0.6890 - acc: 0.5272 - val_loss: 0.6903 - val_acc: 0.5392\n",
      "Epoch 13/1000\n",
      "3784/3784 [==============================] - 1s 235us/step - loss: 0.6911 - acc: 0.5246 - val_loss: 0.6903 - val_acc: 0.5392\n",
      "Epoch 14/1000\n",
      "3784/3784 [==============================] - 1s 237us/step - loss: 0.6907 - acc: 0.5262 - val_loss: 0.6904 - val_acc: 0.5416\n",
      "Epoch 15/1000\n",
      "3784/3784 [==============================] - 1s 237us/step - loss: 0.6901 - acc: 0.5291 - val_loss: 0.6903 - val_acc: 0.5392\n",
      "Epoch 16/1000\n",
      "3784/3784 [==============================] - 1s 237us/step - loss: 0.6915 - acc: 0.5254 - val_loss: 0.6894 - val_acc: 0.5487\n",
      "Epoch 17/1000\n",
      "3784/3784 [==============================] - 1s 234us/step - loss: 0.6887 - acc: 0.5299 - val_loss: 0.6900 - val_acc: 0.5392\n",
      "Epoch 18/1000\n",
      "3784/3784 [==============================] - 1s 245us/step - loss: 0.6883 - acc: 0.5277 - val_loss: 0.6901 - val_acc: 0.5416\n",
      "Epoch 19/1000\n",
      "3784/3784 [==============================] - 1s 233us/step - loss: 0.6882 - acc: 0.5307 - val_loss: 0.6922 - val_acc: 0.5344\n",
      "Epoch 20/1000\n",
      "3784/3784 [==============================] - 1s 245us/step - loss: 0.6898 - acc: 0.5283 - val_loss: 0.6911 - val_acc: 0.5368\n",
      "Epoch 21/1000\n",
      "3784/3784 [==============================] - 1s 237us/step - loss: 0.6903 - acc: 0.5243 - val_loss: 0.6910 - val_acc: 0.5392\n",
      "Epoch 22/1000\n",
      "3784/3784 [==============================] - 1s 236us/step - loss: 0.6908 - acc: 0.5238 - val_loss: 0.6910 - val_acc: 0.5368\n",
      "Epoch 23/1000\n",
      "3784/3784 [==============================] - 1s 234us/step - loss: 0.6915 - acc: 0.5235 - val_loss: 0.6905 - val_acc: 0.5392\n",
      "Epoch 24/1000\n",
      "3784/3784 [==============================] - 1s 249us/step - loss: 0.6877 - acc: 0.5285 - val_loss: 0.6905 - val_acc: 0.5416\n",
      "Epoch 25/1000\n",
      "3784/3784 [==============================] - 1s 238us/step - loss: 0.6886 - acc: 0.5248 - val_loss: 0.6917 - val_acc: 0.5392\n",
      "Epoch 26/1000\n",
      "3784/3784 [==============================] - 1s 237us/step - loss: 0.6897 - acc: 0.5251 - val_loss: 0.6916 - val_acc: 0.5392\n",
      "Epoch 27/1000\n",
      "3784/3784 [==============================] - 1s 234us/step - loss: 0.6866 - acc: 0.5262 - val_loss: 0.6918 - val_acc: 0.5392\n",
      "Epoch 28/1000\n",
      "3784/3784 [==============================] - 1s 236us/step - loss: 0.6893 - acc: 0.5275 - val_loss: 0.6918 - val_acc: 0.5392\n",
      "Epoch 29/1000\n",
      "3784/3784 [==============================] - 1s 236us/step - loss: 0.6890 - acc: 0.5277 - val_loss: 0.6926 - val_acc: 0.5392\n",
      "Epoch 30/1000\n",
      "3784/3784 [==============================] - 1s 234us/step - loss: 0.6888 - acc: 0.5285 - val_loss: 0.6928 - val_acc: 0.5392\n",
      "Epoch 31/1000\n",
      "3784/3784 [==============================] - 1s 237us/step - loss: 0.6890 - acc: 0.5217 - val_loss: 0.6932 - val_acc: 0.5344\n",
      "Epoch 32/1000\n",
      "3784/3784 [==============================] - 1s 250us/step - loss: 0.6881 - acc: 0.5309 - val_loss: 0.6936 - val_acc: 0.5392\n",
      "Epoch 33/1000\n",
      "3784/3784 [==============================] - 1s 234us/step - loss: 0.6879 - acc: 0.5296 - val_loss: 0.6928 - val_acc: 0.5344\n",
      "Epoch 34/1000\n",
      "3784/3784 [==============================] - 1s 235us/step - loss: 0.6903 - acc: 0.5280 - val_loss: 0.6937 - val_acc: 0.5344\n",
      "Epoch 35/1000\n",
      "3784/3784 [==============================] - 1s 246us/step - loss: 0.6877 - acc: 0.5251 - val_loss: 0.6928 - val_acc: 0.5368\n",
      "Epoch 36/1000\n",
      "3784/3784 [==============================] - 1s 244us/step - loss: 0.6900 - acc: 0.5270 - val_loss: 0.6921 - val_acc: 0.5392\n",
      "Epoch 37/1000\n",
      "3784/3784 [==============================] - 1s 248us/step - loss: 0.6879 - acc: 0.5272 - val_loss: 0.6929 - val_acc: 0.5392\n",
      "Epoch 38/1000\n",
      "3784/3784 [==============================] - 1s 254us/step - loss: 0.6869 - acc: 0.5307 - val_loss: 0.6947 - val_acc: 0.5392\n",
      "Epoch 39/1000\n",
      "3784/3784 [==============================] - 1s 237us/step - loss: 0.6889 - acc: 0.5312 - val_loss: 0.6935 - val_acc: 0.5392\n",
      "Epoch 40/1000\n",
      "3784/3784 [==============================] - 1s 235us/step - loss: 0.6896 - acc: 0.5325 - val_loss: 0.6927 - val_acc: 0.5392\n",
      "Epoch 41/1000\n",
      "3784/3784 [==============================] - 1s 235us/step - loss: 0.6894 - acc: 0.5235 - val_loss: 0.6936 - val_acc: 0.5344\n",
      "Epoch 42/1000\n",
      "3784/3784 [==============================] - 1s 237us/step - loss: 0.6882 - acc: 0.5275 - val_loss: 0.6944 - val_acc: 0.5321\n",
      "Epoch 43/1000\n",
      "3784/3784 [==============================] - 1s 233us/step - loss: 0.6878 - acc: 0.5336 - val_loss: 0.6938 - val_acc: 0.5344\n",
      "Epoch 44/1000\n",
      "3784/3784 [==============================] - 1s 234us/step - loss: 0.6877 - acc: 0.5312 - val_loss: 0.6949 - val_acc: 0.5249\n",
      "Epoch 45/1000\n",
      "3784/3784 [==============================] - 1s 236us/step - loss: 0.6868 - acc: 0.5280 - val_loss: 0.6937 - val_acc: 0.5368\n",
      "Epoch 46/1000\n",
      "3784/3784 [==============================] - 1s 242us/step - loss: 0.6860 - acc: 0.5346 - val_loss: 0.6935 - val_acc: 0.5416\n",
      "Epoch 47/1000\n",
      "3784/3784 [==============================] - 1s 238us/step - loss: 0.6881 - acc: 0.5317 - val_loss: 0.6947 - val_acc: 0.5463\n",
      "Epoch 48/1000\n",
      "3784/3784 [==============================] - 1s 243us/step - loss: 0.6866 - acc: 0.5314 - val_loss: 0.6937 - val_acc: 0.5273\n",
      "Epoch 49/1000\n",
      "3784/3784 [==============================] - 1s 244us/step - loss: 0.6880 - acc: 0.5333 - val_loss: 0.6943 - val_acc: 0.5321\n",
      "Epoch 50/1000\n",
      "3784/3784 [==============================] - 1s 233us/step - loss: 0.6896 - acc: 0.5299 - val_loss: 0.6952 - val_acc: 0.5321\n",
      "Epoch 51/1000\n",
      "3784/3784 [==============================] - 1s 235us/step - loss: 0.6858 - acc: 0.5288 - val_loss: 0.6944 - val_acc: 0.5392\n",
      "Epoch 52/1000\n",
      "3784/3784 [==============================] - 1s 237us/step - loss: 0.6834 - acc: 0.5325 - val_loss: 0.6960 - val_acc: 0.5321\n",
      "Epoch 53/1000\n",
      "3784/3784 [==============================] - ETA: 0s - loss: 0.6863 - acc: 0.536 - 1s 240us/step - loss: 0.6859 - acc: 0.5378 - val_loss: 0.6972 - val_acc: 0.5368\n",
      "Epoch 54/1000\n",
      "3784/3784 [==============================] - 1s 252us/step - loss: 0.6869 - acc: 0.5270 - val_loss: 0.6957 - val_acc: 0.5463\n",
      "Epoch 55/1000\n",
      "3784/3784 [==============================] - 1s 243us/step - loss: 0.6850 - acc: 0.5378 - val_loss: 0.6953 - val_acc: 0.5392\n",
      "Epoch 56/1000\n",
      "3784/3784 [==============================] - 1s 234us/step - loss: 0.6847 - acc: 0.5330 - val_loss: 0.6946 - val_acc: 0.5368\n",
      "Epoch 57/1000\n",
      "3784/3784 [==============================] - 1s 235us/step - loss: 0.6853 - acc: 0.5314 - val_loss: 0.6972 - val_acc: 0.5344\n",
      "Epoch 58/1000\n",
      "3784/3784 [==============================] - 1s 235us/step - loss: 0.6853 - acc: 0.5381 - val_loss: 0.6978 - val_acc: 0.5416\n",
      "Epoch 59/1000\n",
      "3784/3784 [==============================] - 1s 230us/step - loss: 0.6826 - acc: 0.5404 - val_loss: 0.6995 - val_acc: 0.5416\n",
      "Epoch 60/1000\n",
      "3784/3784 [==============================] - 1s 229us/step - loss: 0.6873 - acc: 0.5349 - val_loss: 0.6973 - val_acc: 0.5321\n",
      "Epoch 61/1000\n",
      "3784/3784 [==============================] - 1s 240us/step - loss: 0.6830 - acc: 0.5314 - val_loss: 0.6939 - val_acc: 0.5463\n",
      "Epoch 62/1000\n",
      "3784/3784 [==============================] - 1s 231us/step - loss: 0.6860 - acc: 0.5391 - val_loss: 0.6924 - val_acc: 0.5439\n",
      "Epoch 63/1000\n",
      "3784/3784 [==============================] - 1s 239us/step - loss: 0.6838 - acc: 0.5375 - val_loss: 0.6929 - val_acc: 0.5416\n",
      "Epoch 64/1000\n",
      "3784/3784 [==============================] - 1s 231us/step - loss: 0.6827 - acc: 0.5330 - val_loss: 0.6967 - val_acc: 0.5368\n",
      "Epoch 65/1000\n",
      "3784/3784 [==============================] - 1s 234us/step - loss: 0.6875 - acc: 0.5322 - val_loss: 0.6957 - val_acc: 0.5439\n",
      "Epoch 66/1000\n",
      "3784/3784 [==============================] - 1s 229us/step - loss: 0.6860 - acc: 0.5256 - val_loss: 0.6961 - val_acc: 0.5463\n",
      "Epoch 67/1000\n",
      "3784/3784 [==============================] - 1s 229us/step - loss: 0.6819 - acc: 0.5367 - val_loss: 0.6952 - val_acc: 0.5439\n",
      "Epoch 68/1000\n",
      "3784/3784 [==============================] - 1s 229us/step - loss: 0.6857 - acc: 0.5333 - val_loss: 0.6946 - val_acc: 0.5558\n",
      "Epoch 69/1000\n",
      "3784/3784 [==============================] - 1s 233us/step - loss: 0.6840 - acc: 0.5375 - val_loss: 0.6953 - val_acc: 0.5416\n",
      "Epoch 70/1000\n",
      "3784/3784 [==============================] - 1s 231us/step - loss: 0.6861 - acc: 0.5359 - val_loss: 0.6960 - val_acc: 0.5534\n",
      "Epoch 71/1000\n",
      "3784/3784 [==============================] - 1s 241us/step - loss: 0.6825 - acc: 0.5375 - val_loss: 0.6955 - val_acc: 0.5439\n",
      "Epoch 72/1000\n",
      "3784/3784 [==============================] - 1s 231us/step - loss: 0.6843 - acc: 0.5375 - val_loss: 0.6982 - val_acc: 0.5297\n",
      "Epoch 73/1000\n",
      "3784/3784 [==============================] - 1s 231us/step - loss: 0.6846 - acc: 0.5396 - val_loss: 0.6966 - val_acc: 0.5368\n",
      "Epoch 74/1000\n",
      "3784/3784 [==============================] - 1s 249us/step - loss: 0.6785 - acc: 0.5381 - val_loss: 0.6980 - val_acc: 0.5273\n",
      "Epoch 75/1000\n",
      "3784/3784 [==============================] - 1s 239us/step - loss: 0.6837 - acc: 0.5484 - val_loss: 0.6973 - val_acc: 0.5368\n",
      "Epoch 76/1000\n",
      "3784/3784 [==============================] - 1s 239us/step - loss: 0.6816 - acc: 0.5359 - val_loss: 0.6967 - val_acc: 0.5416\n",
      "Epoch 77/1000\n",
      "3784/3784 [==============================] - 1s 239us/step - loss: 0.6836 - acc: 0.5354 - val_loss: 0.6968 - val_acc: 0.5392\n",
      "Epoch 78/1000\n",
      "3784/3784 [==============================] - 1s 241us/step - loss: 0.6848 - acc: 0.5296 - val_loss: 0.6963 - val_acc: 0.5439\n",
      "Epoch 79/1000\n",
      "3784/3784 [==============================] - 1s 246us/step - loss: 0.6822 - acc: 0.5396 - val_loss: 0.6961 - val_acc: 0.5392\n",
      "Epoch 80/1000\n",
      "3784/3784 [==============================] - 1s 241us/step - loss: 0.6812 - acc: 0.5394 - val_loss: 0.6960 - val_acc: 0.5392\n",
      "Epoch 81/1000\n",
      "3784/3784 [==============================] - 1s 243us/step - loss: 0.6803 - acc: 0.5396 - val_loss: 0.6979 - val_acc: 0.5416\n",
      "Epoch 82/1000\n",
      "3784/3784 [==============================] - 1s 243us/step - loss: 0.6840 - acc: 0.5365 - val_loss: 0.6959 - val_acc: 0.5439\n",
      "Epoch 83/1000\n",
      "3784/3784 [==============================] - 1s 236us/step - loss: 0.6829 - acc: 0.5328 - val_loss: 0.6947 - val_acc: 0.5463\n",
      "Epoch 84/1000\n",
      "3784/3784 [==============================] - 1s 244us/step - loss: 0.6804 - acc: 0.5391 - val_loss: 0.6975 - val_acc: 0.5439\n",
      "Epoch 85/1000\n",
      "3784/3784 [==============================] - 1s 240us/step - loss: 0.6771 - acc: 0.5439 - val_loss: 0.7016 - val_acc: 0.5344\n",
      "Epoch 86/1000\n",
      "3784/3784 [==============================] - 1s 241us/step - loss: 0.6816 - acc: 0.5396 - val_loss: 0.6990 - val_acc: 0.5416\n",
      "Epoch 87/1000\n",
      "3784/3784 [==============================] - 1s 235us/step - loss: 0.6791 - acc: 0.5436 - val_loss: 0.6977 - val_acc: 0.5392\n",
      "Epoch 88/1000\n",
      "3784/3784 [==============================] - 1s 239us/step - loss: 0.6776 - acc: 0.5428 - val_loss: 0.6979 - val_acc: 0.5344\n",
      "Epoch 89/1000\n",
      "3784/3784 [==============================] - 1s 247us/step - loss: 0.6784 - acc: 0.5436 - val_loss: 0.6981 - val_acc: 0.5321\n",
      "Epoch 90/1000\n",
      "3784/3784 [==============================] - 1s 235us/step - loss: 0.6819 - acc: 0.5388 - val_loss: 0.6978 - val_acc: 0.5344\n",
      "Epoch 91/1000\n",
      "3784/3784 [==============================] - 1s 237us/step - loss: 0.6753 - acc: 0.5452 - val_loss: 0.7003 - val_acc: 0.5416\n",
      "Epoch 92/1000\n",
      "3784/3784 [==============================] - 1s 236us/step - loss: 0.6780 - acc: 0.5436 - val_loss: 0.7006 - val_acc: 0.5368\n",
      "Epoch 93/1000\n",
      "3784/3784 [==============================] - 1s 235us/step - loss: 0.6778 - acc: 0.5351 - val_loss: 0.7007 - val_acc: 0.5392\n",
      "Epoch 94/1000\n",
      "3784/3784 [==============================] - 1s 235us/step - loss: 0.6776 - acc: 0.5484 - val_loss: 0.6995 - val_acc: 0.5344\n",
      "Epoch 95/1000\n",
      "3784/3784 [==============================] - 1s 236us/step - loss: 0.6774 - acc: 0.5330 - val_loss: 0.7021 - val_acc: 0.5226\n",
      "Epoch 96/1000\n",
      "3784/3784 [==============================] - 1s 236us/step - loss: 0.6745 - acc: 0.5436 - val_loss: 0.7027 - val_acc: 0.5511\n",
      "Epoch 97/1000\n",
      "3784/3784 [==============================] - 1s 237us/step - loss: 0.6749 - acc: 0.5433 - val_loss: 0.7008 - val_acc: 0.5368\n",
      "Epoch 98/1000\n",
      "3784/3784 [==============================] - 1s 238us/step - loss: 0.6766 - acc: 0.5423 - val_loss: 0.6978 - val_acc: 0.5439\n",
      "Epoch 99/1000\n",
      "3784/3784 [==============================] - 1s 247us/step - loss: 0.6798 - acc: 0.5481 - val_loss: 0.7003 - val_acc: 0.5392\n",
      "Epoch 100/1000\n",
      "3784/3784 [==============================] - 1s 235us/step - loss: 0.6763 - acc: 0.5441 - val_loss: 0.6985 - val_acc: 0.5392\n",
      "Epoch 101/1000\n",
      "3784/3784 [==============================] - 1s 244us/step - loss: 0.6748 - acc: 0.5526 - val_loss: 0.6979 - val_acc: 0.5392\n",
      "Epoch 102/1000\n",
      "3784/3784 [==============================] - 1s 242us/step - loss: 0.6778 - acc: 0.5423 - val_loss: 0.6988 - val_acc: 0.5368\n",
      "Epoch 103/1000\n",
      "3784/3784 [==============================] - 1s 241us/step - loss: 0.6783 - acc: 0.5468 - val_loss: 0.6999 - val_acc: 0.5416\n",
      "Epoch 104/1000\n",
      "3784/3784 [==============================] - 1s 241us/step - loss: 0.6732 - acc: 0.5431 - val_loss: 0.6976 - val_acc: 0.5344\n",
      "Epoch 105/1000\n",
      "3784/3784 [==============================] - 1s 238us/step - loss: 0.6738 - acc: 0.5481 - val_loss: 0.6948 - val_acc: 0.5511\n",
      "Epoch 106/1000\n",
      "3784/3784 [==============================] - 1s 236us/step - loss: 0.6772 - acc: 0.5462 - val_loss: 0.7017 - val_acc: 0.5558\n",
      "Epoch 107/1000\n",
      "3784/3784 [==============================] - 1s 251us/step - loss: 0.6745 - acc: 0.5449 - val_loss: 0.7012 - val_acc: 0.5558\n",
      "Epoch 108/1000\n",
      "3784/3784 [==============================] - 1s 243us/step - loss: 0.6765 - acc: 0.5439 - val_loss: 0.7003 - val_acc: 0.5439\n",
      "Epoch 109/1000\n",
      "3784/3784 [==============================] - 1s 248us/step - loss: 0.6713 - acc: 0.5468 - val_loss: 0.7059 - val_acc: 0.5368\n",
      "Epoch 110/1000\n",
      "3784/3784 [==============================] - 1s 250us/step - loss: 0.6726 - acc: 0.5563 - val_loss: 0.7098 - val_acc: 0.5297\n",
      "Epoch 111/1000\n",
      "3784/3784 [==============================] - 1s 250us/step - loss: 0.6723 - acc: 0.5465 - val_loss: 0.7050 - val_acc: 0.5273\n",
      "Epoch 112/1000\n",
      "3784/3784 [==============================] - 1s 252us/step - loss: 0.6686 - acc: 0.5523 - val_loss: 0.7059 - val_acc: 0.5321\n",
      "Epoch 113/1000\n",
      "3784/3784 [==============================] - 1s 243us/step - loss: 0.6718 - acc: 0.5484 - val_loss: 0.7044 - val_acc: 0.5297\n",
      "Epoch 114/1000\n",
      "3784/3784 [==============================] - 1s 242us/step - loss: 0.6716 - acc: 0.5455 - val_loss: 0.7043 - val_acc: 0.5226\n",
      "Epoch 115/1000\n",
      "3784/3784 [==============================] - 1s 234us/step - loss: 0.6691 - acc: 0.5505 - val_loss: 0.7030 - val_acc: 0.5297\n",
      "Epoch 116/1000\n",
      "3784/3784 [==============================] - 1s 236us/step - loss: 0.6687 - acc: 0.5433 - val_loss: 0.7045 - val_acc: 0.5368\n",
      "Epoch 117/1000\n",
      "3784/3784 [==============================] - 1s 236us/step - loss: 0.6722 - acc: 0.5425 - val_loss: 0.7040 - val_acc: 0.5273\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 118/1000\n",
      "3784/3784 [==============================] - 1s 230us/step - loss: 0.6699 - acc: 0.5462 - val_loss: 0.7030 - val_acc: 0.5226\n",
      "Epoch 119/1000\n",
      "3784/3784 [==============================] - 1s 231us/step - loss: 0.6675 - acc: 0.5481 - val_loss: 0.7025 - val_acc: 0.5273\n",
      "Epoch 120/1000\n",
      "3784/3784 [==============================] - 1s 231us/step - loss: 0.6744 - acc: 0.5513 - val_loss: 0.7047 - val_acc: 0.5321\n",
      "Epoch 121/1000\n",
      "3784/3784 [==============================] - 1s 229us/step - loss: 0.6645 - acc: 0.5579 - val_loss: 0.7063 - val_acc: 0.5297\n",
      "Epoch 122/1000\n",
      "3784/3784 [==============================] - 1s 229us/step - loss: 0.6710 - acc: 0.5566 - val_loss: 0.7102 - val_acc: 0.5202\n",
      "Epoch 123/1000\n",
      "3784/3784 [==============================] - 1s 229us/step - loss: 0.6643 - acc: 0.5544 - val_loss: 0.7076 - val_acc: 0.5392\n",
      "Epoch 124/1000\n",
      "3784/3784 [==============================] - 1s 229us/step - loss: 0.6668 - acc: 0.5507 - val_loss: 0.7092 - val_acc: 0.5273\n",
      "Epoch 125/1000\n",
      "3784/3784 [==============================] - 1s 230us/step - loss: 0.6661 - acc: 0.5539 - val_loss: 0.7110 - val_acc: 0.5178\n",
      "Epoch 126/1000\n",
      "3784/3784 [==============================] - 1s 231us/step - loss: 0.6645 - acc: 0.5610 - val_loss: 0.7132 - val_acc: 0.5226\n",
      "Epoch 127/1000\n",
      "3784/3784 [==============================] - 1s 231us/step - loss: 0.6656 - acc: 0.5502 - val_loss: 0.7138 - val_acc: 0.5154\n",
      "Epoch 128/1000\n",
      "3784/3784 [==============================] - 1s 234us/step - loss: 0.6667 - acc: 0.5558 - val_loss: 0.7124 - val_acc: 0.5273\n",
      "Epoch 129/1000\n",
      "3784/3784 [==============================] - 1s 234us/step - loss: 0.6688 - acc: 0.5550 - val_loss: 0.7112 - val_acc: 0.5226\n",
      "Epoch 130/1000\n",
      "3784/3784 [==============================] - 1s 233us/step - loss: 0.6640 - acc: 0.5608 - val_loss: 0.7079 - val_acc: 0.5297\n",
      "Epoch 131/1000\n",
      "3784/3784 [==============================] - 1s 235us/step - loss: 0.6638 - acc: 0.5542 - val_loss: 0.7086 - val_acc: 0.5368\n",
      "Epoch 132/1000\n",
      "3784/3784 [==============================] - 1s 238us/step - loss: 0.6647 - acc: 0.5621 - val_loss: 0.7105 - val_acc: 0.5321\n",
      "Epoch 133/1000\n",
      "3784/3784 [==============================] - 1s 235us/step - loss: 0.6661 - acc: 0.5494 - val_loss: 0.7126 - val_acc: 0.5249\n",
      "Epoch 134/1000\n",
      "3784/3784 [==============================] - 1s 236us/step - loss: 0.6613 - acc: 0.5579 - val_loss: 0.7212 - val_acc: 0.5249\n",
      "Epoch 135/1000\n",
      "3784/3784 [==============================] - 1s 236us/step - loss: 0.6658 - acc: 0.5573 - val_loss: 0.7146 - val_acc: 0.5392\n",
      "Epoch 136/1000\n",
      "3784/3784 [==============================] - 1s 238us/step - loss: 0.6611 - acc: 0.5640 - val_loss: 0.7120 - val_acc: 0.5226\n",
      "Epoch 137/1000\n",
      "3784/3784 [==============================] - 1s 236us/step - loss: 0.6605 - acc: 0.5523 - val_loss: 0.7085 - val_acc: 0.5321\n",
      "Epoch 138/1000\n",
      "3784/3784 [==============================] - 1s 236us/step - loss: 0.6618 - acc: 0.5608 - val_loss: 0.7107 - val_acc: 0.5344\n",
      "Epoch 139/1000\n",
      "3784/3784 [==============================] - 1s 235us/step - loss: 0.6578 - acc: 0.5613 - val_loss: 0.7147 - val_acc: 0.5297\n",
      "Epoch 140/1000\n",
      "3784/3784 [==============================] - 1s 242us/step - loss: 0.6574 - acc: 0.5618 - val_loss: 0.7095 - val_acc: 0.5321\n",
      "Epoch 141/1000\n",
      "3784/3784 [==============================] - 1s 234us/step - loss: 0.6595 - acc: 0.5711 - val_loss: 0.7086 - val_acc: 0.5321\n",
      "Epoch 142/1000\n",
      "3784/3784 [==============================] - 1s 237us/step - loss: 0.6619 - acc: 0.5584 - val_loss: 0.7097 - val_acc: 0.5154\n",
      "Epoch 143/1000\n",
      "3784/3784 [==============================] - 1s 235us/step - loss: 0.6623 - acc: 0.5581 - val_loss: 0.7090 - val_acc: 0.5321\n",
      "Epoch 144/1000\n",
      "3784/3784 [==============================] - 1s 235us/step - loss: 0.6641 - acc: 0.5478 - val_loss: 0.7076 - val_acc: 0.5226\n",
      "Epoch 145/1000\n",
      "3784/3784 [==============================] - 1s 234us/step - loss: 0.6617 - acc: 0.5571 - val_loss: 0.7128 - val_acc: 0.5297\n",
      "Epoch 146/1000\n",
      "3784/3784 [==============================] - 1s 234us/step - loss: 0.6592 - acc: 0.5603 - val_loss: 0.7129 - val_acc: 0.5297\n",
      "Epoch 147/1000\n",
      "3784/3784 [==============================] - 1s 235us/step - loss: 0.6591 - acc: 0.5555 - val_loss: 0.7105 - val_acc: 0.5344\n",
      "Epoch 148/1000\n",
      "3784/3784 [==============================] - 1s 245us/step - loss: 0.6608 - acc: 0.5560 - val_loss: 0.7098 - val_acc: 0.5273\n",
      "Epoch 149/1000\n",
      "3784/3784 [==============================] - 1s 239us/step - loss: 0.6575 - acc: 0.5587 - val_loss: 0.7166 - val_acc: 0.5368\n",
      "Epoch 150/1000\n",
      "3784/3784 [==============================] - 1s 253us/step - loss: 0.6611 - acc: 0.5634 - val_loss: 0.7123 - val_acc: 0.5439\n",
      "Epoch 151/1000\n",
      "3784/3784 [==============================] - 1s 234us/step - loss: 0.6560 - acc: 0.5634 - val_loss: 0.7181 - val_acc: 0.5297\n",
      "Epoch 152/1000\n",
      "3784/3784 [==============================] - 1s 242us/step - loss: 0.6547 - acc: 0.5729 - val_loss: 0.7222 - val_acc: 0.5226\n",
      "Epoch 153/1000\n",
      "3784/3784 [==============================] - 1s 245us/step - loss: 0.6594 - acc: 0.5663 - val_loss: 0.7189 - val_acc: 0.5249\n",
      "Epoch 154/1000\n",
      "3784/3784 [==============================] - 1s 238us/step - loss: 0.6509 - acc: 0.5714 - val_loss: 0.7188 - val_acc: 0.5178\n",
      "Epoch 155/1000\n",
      "3784/3784 [==============================] - 1s 237us/step - loss: 0.6560 - acc: 0.5573 - val_loss: 0.7180 - val_acc: 0.5154\n",
      "Epoch 156/1000\n",
      "3784/3784 [==============================] - 1s 246us/step - loss: 0.6535 - acc: 0.5684 - val_loss: 0.7179 - val_acc: 0.5273\n",
      "Epoch 157/1000\n",
      "3784/3784 [==============================] - 1s 237us/step - loss: 0.6571 - acc: 0.5626 - val_loss: 0.7165 - val_acc: 0.5226\n",
      "Epoch 158/1000\n",
      "3784/3784 [==============================] - 1s 236us/step - loss: 0.6545 - acc: 0.5671 - val_loss: 0.7181 - val_acc: 0.5344\n",
      "Epoch 159/1000\n",
      "3784/3784 [==============================] - 1s 237us/step - loss: 0.6574 - acc: 0.5692 - val_loss: 0.7219 - val_acc: 0.5321\n",
      "Epoch 160/1000\n",
      "3784/3784 [==============================] - 1s 251us/step - loss: 0.6572 - acc: 0.5716 - val_loss: 0.7158 - val_acc: 0.5202\n",
      "Epoch 161/1000\n",
      "3784/3784 [==============================] - 1s 250us/step - loss: 0.6559 - acc: 0.5719 - val_loss: 0.7145 - val_acc: 0.5249\n",
      "Epoch 162/1000\n",
      "3784/3784 [==============================] - 1s 248us/step - loss: 0.6547 - acc: 0.5597 - val_loss: 0.7149 - val_acc: 0.5202\n",
      "Epoch 163/1000\n",
      "3784/3784 [==============================] - 1s 233us/step - loss: 0.6531 - acc: 0.5719 - val_loss: 0.7162 - val_acc: 0.5416\n",
      "Epoch 164/1000\n",
      "3784/3784 [==============================] - 1s 235us/step - loss: 0.6527 - acc: 0.5748 - val_loss: 0.7158 - val_acc: 0.5249\n",
      "Epoch 165/1000\n",
      "3784/3784 [==============================] - 1s 234us/step - loss: 0.6536 - acc: 0.5714 - val_loss: 0.7183 - val_acc: 0.5273\n",
      "Epoch 166/1000\n",
      "3784/3784 [==============================] - 1s 242us/step - loss: 0.6517 - acc: 0.5806 - val_loss: 0.7249 - val_acc: 0.5273\n",
      "Epoch 167/1000\n",
      "3784/3784 [==============================] - 1s 239us/step - loss: 0.6420 - acc: 0.5769 - val_loss: 0.7263 - val_acc: 0.5249\n",
      "Epoch 168/1000\n",
      "3784/3784 [==============================] - 1s 246us/step - loss: 0.6446 - acc: 0.5793 - val_loss: 0.7280 - val_acc: 0.5321\n",
      "Epoch 169/1000\n",
      "3784/3784 [==============================] - 1s 229us/step - loss: 0.6481 - acc: 0.5772 - val_loss: 0.7217 - val_acc: 0.5297\n",
      "Epoch 170/1000\n",
      "3784/3784 [==============================] - 1s 245us/step - loss: 0.6472 - acc: 0.5692 - val_loss: 0.7250 - val_acc: 0.5344\n",
      "Epoch 171/1000\n",
      "3784/3784 [==============================] - 1s 230us/step - loss: 0.6523 - acc: 0.5737 - val_loss: 0.7234 - val_acc: 0.5273\n",
      "Epoch 172/1000\n",
      "3784/3784 [==============================] - 1s 227us/step - loss: 0.6485 - acc: 0.5666 - val_loss: 0.7264 - val_acc: 0.5202\n",
      "Epoch 173/1000\n",
      "3784/3784 [==============================] - 1s 227us/step - loss: 0.6449 - acc: 0.5803 - val_loss: 0.7413 - val_acc: 0.5083\n",
      "Epoch 174/1000\n",
      "3784/3784 [==============================] - 1s 228us/step - loss: 0.6442 - acc: 0.5729 - val_loss: 0.7282 - val_acc: 0.5226\n",
      "Epoch 175/1000\n",
      "3784/3784 [==============================] - 1s 227us/step - loss: 0.6476 - acc: 0.5745 - val_loss: 0.7224 - val_acc: 0.5273\n",
      "Epoch 176/1000\n",
      "3784/3784 [==============================] - 1s 227us/step - loss: 0.6435 - acc: 0.5846 - val_loss: 0.7257 - val_acc: 0.5416\n",
      "Epoch 177/1000\n",
      "3784/3784 [==============================] - 1s 227us/step - loss: 0.6496 - acc: 0.5748 - val_loss: 0.7232 - val_acc: 0.5321\n",
      "Epoch 178/1000\n",
      "3784/3784 [==============================] - 1s 227us/step - loss: 0.6465 - acc: 0.5785 - val_loss: 0.7275 - val_acc: 0.5202\n",
      "Epoch 179/1000\n",
      "3784/3784 [==============================] - 1s 227us/step - loss: 0.6464 - acc: 0.5751 - val_loss: 0.7295 - val_acc: 0.5297\n",
      "Epoch 180/1000\n",
      "3784/3784 [==============================] - 1s 228us/step - loss: 0.6488 - acc: 0.5737 - val_loss: 0.7296 - val_acc: 0.5154\n",
      "Epoch 181/1000\n",
      "3784/3784 [==============================] - 1s 236us/step - loss: 0.6447 - acc: 0.5827 - val_loss: 0.7281 - val_acc: 0.5178\n",
      "Epoch 182/1000\n",
      "3784/3784 [==============================] - 1s 241us/step - loss: 0.6442 - acc: 0.5835 - val_loss: 0.7310 - val_acc: 0.5154\n",
      "Epoch 183/1000\n",
      "3784/3784 [==============================] - 1s 232us/step - loss: 0.6377 - acc: 0.5811 - val_loss: 0.7246 - val_acc: 0.5368\n",
      "Epoch 184/1000\n",
      "3784/3784 [==============================] - 1s 231us/step - loss: 0.6435 - acc: 0.5859 - val_loss: 0.7323 - val_acc: 0.5344\n",
      "Epoch 185/1000\n",
      "3784/3784 [==============================] - 1s 232us/step - loss: 0.6440 - acc: 0.5790 - val_loss: 0.7285 - val_acc: 0.5344\n",
      "Epoch 186/1000\n",
      "3784/3784 [==============================] - 1s 231us/step - loss: 0.6428 - acc: 0.5827 - val_loss: 0.7314 - val_acc: 0.5344\n",
      "Epoch 187/1000\n",
      "3784/3784 [==============================] - 1s 229us/step - loss: 0.6380 - acc: 0.5830 - val_loss: 0.7318 - val_acc: 0.5249\n",
      "Epoch 188/1000\n",
      "3784/3784 [==============================] - 1s 230us/step - loss: 0.6374 - acc: 0.5835 - val_loss: 0.7423 - val_acc: 0.5249\n",
      "Epoch 189/1000\n",
      "3784/3784 [==============================] - 1s 238us/step - loss: 0.6386 - acc: 0.5893 - val_loss: 0.7359 - val_acc: 0.5249\n",
      "Epoch 190/1000\n",
      "3784/3784 [==============================] - 1s 234us/step - loss: 0.6419 - acc: 0.5809 - val_loss: 0.7382 - val_acc: 0.5202\n",
      "Epoch 191/1000\n",
      "3784/3784 [==============================] - 1s 229us/step - loss: 0.6361 - acc: 0.5825 - val_loss: 0.7382 - val_acc: 0.5154\n",
      "Epoch 192/1000\n",
      "3784/3784 [==============================] - 1s 247us/step - loss: 0.6395 - acc: 0.5811 - val_loss: 0.7407 - val_acc: 0.5012\n",
      "Epoch 193/1000\n",
      "3784/3784 [==============================] - 1s 230us/step - loss: 0.6379 - acc: 0.5811 - val_loss: 0.7471 - val_acc: 0.5083\n",
      "Epoch 194/1000\n",
      "3784/3784 [==============================] - 1s 228us/step - loss: 0.6405 - acc: 0.5785 - val_loss: 0.7402 - val_acc: 0.5154\n",
      "Epoch 195/1000\n",
      "3784/3784 [==============================] - 1s 230us/step - loss: 0.6436 - acc: 0.5904 - val_loss: 0.7377 - val_acc: 0.4988\n",
      "Epoch 196/1000\n",
      "3784/3784 [==============================] - 1s 234us/step - loss: 0.6394 - acc: 0.5848 - val_loss: 0.7364 - val_acc: 0.5107\n",
      "Epoch 197/1000\n",
      "3784/3784 [==============================] - 1s 235us/step - loss: 0.6345 - acc: 0.5830 - val_loss: 0.7383 - val_acc: 0.5059\n",
      "Epoch 198/1000\n",
      "3784/3784 [==============================] - 1s 230us/step - loss: 0.6361 - acc: 0.5843 - val_loss: 0.7409 - val_acc: 0.5178\n",
      "Epoch 199/1000\n",
      "3784/3784 [==============================] - 1s 234us/step - loss: 0.6335 - acc: 0.5801 - val_loss: 0.7418 - val_acc: 0.5154\n",
      "Epoch 200/1000\n",
      "3784/3784 [==============================] - 1s 228us/step - loss: 0.6314 - acc: 0.5893 - val_loss: 0.7485 - val_acc: 0.5012\n",
      "Epoch 201/1000\n",
      "3784/3784 [==============================] - 1s 228us/step - loss: 0.6287 - acc: 0.5930 - val_loss: 0.7444 - val_acc: 0.5154\n",
      "Epoch 202/1000\n",
      "3784/3784 [==============================] - 1s 228us/step - loss: 0.6328 - acc: 0.5880 - val_loss: 0.7466 - val_acc: 0.5249\n",
      "Epoch 203/1000\n",
      "3784/3784 [==============================] - 1s 229us/step - loss: 0.6402 - acc: 0.5843 - val_loss: 0.7436 - val_acc: 0.5083\n",
      "Epoch 204/1000\n",
      "3784/3784 [==============================] - 1s 227us/step - loss: 0.6399 - acc: 0.5777 - val_loss: 0.7437 - val_acc: 0.5202\n",
      "Epoch 205/1000\n",
      "3784/3784 [==============================] - 1s 228us/step - loss: 0.6226 - acc: 0.5986 - val_loss: 0.7496 - val_acc: 0.5202\n",
      "Epoch 206/1000\n",
      "3784/3784 [==============================] - 1s 229us/step - loss: 0.6281 - acc: 0.5938 - val_loss: 0.7533 - val_acc: 0.5178\n",
      "Epoch 207/1000\n",
      "3784/3784 [==============================] - 1s 228us/step - loss: 0.6399 - acc: 0.5848 - val_loss: 0.7424 - val_acc: 0.5321\n",
      "Epoch 208/1000\n",
      "3784/3784 [==============================] - 1s 229us/step - loss: 0.6350 - acc: 0.5872 - val_loss: 0.7483 - val_acc: 0.5107\n",
      "Epoch 209/1000\n",
      "3784/3784 [==============================] - 1s 228us/step - loss: 0.6345 - acc: 0.5851 - val_loss: 0.7378 - val_acc: 0.5131\n",
      "Epoch 210/1000\n",
      "3784/3784 [==============================] - 1s 229us/step - loss: 0.6326 - acc: 0.5838 - val_loss: 0.7430 - val_acc: 0.5226\n",
      "Epoch 211/1000\n",
      "3784/3784 [==============================] - 1s 229us/step - loss: 0.6283 - acc: 0.5917 - val_loss: 0.7556 - val_acc: 0.5202\n",
      "Epoch 212/1000\n",
      "3784/3784 [==============================] - 1s 227us/step - loss: 0.6306 - acc: 0.5928 - val_loss: 0.7543 - val_acc: 0.5036\n",
      "Epoch 213/1000\n",
      "3784/3784 [==============================] - 1s 226us/step - loss: 0.6252 - acc: 0.5893 - val_loss: 0.7580 - val_acc: 0.5178\n",
      "Epoch 214/1000\n",
      "3784/3784 [==============================] - 1s 227us/step - loss: 0.6271 - acc: 0.5943 - val_loss: 0.7531 - val_acc: 0.5273\n",
      "Epoch 215/1000\n",
      "3784/3784 [==============================] - 1s 227us/step - loss: 0.6258 - acc: 0.5880 - val_loss: 0.7521 - val_acc: 0.5249\n",
      "Epoch 216/1000\n",
      "3784/3784 [==============================] - 1s 226us/step - loss: 0.6245 - acc: 0.5959 - val_loss: 0.7536 - val_acc: 0.5131\n",
      "Epoch 217/1000\n",
      "3784/3784 [==============================] - 1s 229us/step - loss: 0.6260 - acc: 0.5851 - val_loss: 0.7571 - val_acc: 0.5012\n",
      "Epoch 218/1000\n",
      "3784/3784 [==============================] - 1s 235us/step - loss: 0.6205 - acc: 0.6023 - val_loss: 0.7600 - val_acc: 0.5059\n",
      "Epoch 219/1000\n",
      "3784/3784 [==============================] - 1s 228us/step - loss: 0.6243 - acc: 0.5875 - val_loss: 0.7641 - val_acc: 0.4964\n",
      "Epoch 220/1000\n",
      "3784/3784 [==============================] - 1s 228us/step - loss: 0.6240 - acc: 0.5967 - val_loss: 0.7669 - val_acc: 0.4964\n",
      "Epoch 221/1000\n",
      "3784/3784 [==============================] - 1s 231us/step - loss: 0.6276 - acc: 0.5938 - val_loss: 0.7589 - val_acc: 0.4917\n",
      "Epoch 222/1000\n",
      "3784/3784 [==============================] - 1s 228us/step - loss: 0.6286 - acc: 0.5941 - val_loss: 0.7668 - val_acc: 0.5226\n",
      "Epoch 223/1000\n",
      "3784/3784 [==============================] - 1s 226us/step - loss: 0.6228 - acc: 0.6060 - val_loss: 0.7722 - val_acc: 0.5226\n",
      "Epoch 224/1000\n",
      "3784/3784 [==============================] - 1s 227us/step - loss: 0.6249 - acc: 0.5994 - val_loss: 0.7628 - val_acc: 0.5249\n",
      "Epoch 225/1000\n",
      "3784/3784 [==============================] - 1s 229us/step - loss: 0.6301 - acc: 0.5906 - val_loss: 0.7638 - val_acc: 0.5321\n",
      "Epoch 226/1000\n",
      "3784/3784 [==============================] - 1s 226us/step - loss: 0.6299 - acc: 0.5928 - val_loss: 0.7653 - val_acc: 0.5321\n",
      "Epoch 227/1000\n",
      "3784/3784 [==============================] - 1s 228us/step - loss: 0.6227 - acc: 0.5957 - val_loss: 0.7669 - val_acc: 0.5249\n",
      "Epoch 228/1000\n",
      "3784/3784 [==============================] - 1s 228us/step - loss: 0.6192 - acc: 0.5999 - val_loss: 0.7665 - val_acc: 0.5297\n",
      "Epoch 229/1000\n",
      "3784/3784 [==============================] - 1s 226us/step - loss: 0.6222 - acc: 0.5962 - val_loss: 0.7642 - val_acc: 0.5249\n",
      "Epoch 230/1000\n",
      "3784/3784 [==============================] - 1s 227us/step - loss: 0.6216 - acc: 0.5941 - val_loss: 0.7613 - val_acc: 0.5249\n",
      "Epoch 231/1000\n",
      "3784/3784 [==============================] - 1s 227us/step - loss: 0.6203 - acc: 0.6002 - val_loss: 0.7657 - val_acc: 0.5178\n",
      "Epoch 232/1000\n",
      "3784/3784 [==============================] - 1s 227us/step - loss: 0.6193 - acc: 0.5983 - val_loss: 0.7802 - val_acc: 0.5321\n",
      "Epoch 233/1000\n",
      "3784/3784 [==============================] - 1s 226us/step - loss: 0.6163 - acc: 0.6099 - val_loss: 0.7752 - val_acc: 0.5178\n",
      "Epoch 234/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3784/3784 [==============================] - 1s 228us/step - loss: 0.6237 - acc: 0.5933 - val_loss: 0.7779 - val_acc: 0.5249\n",
      "Epoch 235/1000\n",
      "3784/3784 [==============================] - 1s 227us/step - loss: 0.6261 - acc: 0.5973 - val_loss: 0.7703 - val_acc: 0.5273\n",
      "Epoch 236/1000\n",
      "3784/3784 [==============================] - 1s 225us/step - loss: 0.6216 - acc: 0.5965 - val_loss: 0.7796 - val_acc: 0.5012\n",
      "Epoch 237/1000\n",
      "3784/3784 [==============================] - 1s 228us/step - loss: 0.6148 - acc: 0.6033 - val_loss: 0.7788 - val_acc: 0.5273\n",
      "Epoch 238/1000\n",
      "3784/3784 [==============================] - 1s 232us/step - loss: 0.6226 - acc: 0.5928 - val_loss: 0.7683 - val_acc: 0.5297\n",
      "Epoch 239/1000\n",
      "3784/3784 [==============================] - 1s 226us/step - loss: 0.6184 - acc: 0.6012 - val_loss: 0.7668 - val_acc: 0.5297\n",
      "Epoch 240/1000\n",
      "3784/3784 [==============================] - 1s 227us/step - loss: 0.6162 - acc: 0.6081 - val_loss: 0.7754 - val_acc: 0.5321\n",
      "Epoch 241/1000\n",
      "3784/3784 [==============================] - 1s 226us/step - loss: 0.6251 - acc: 0.5978 - val_loss: 0.7617 - val_acc: 0.5273\n",
      "Epoch 242/1000\n",
      "3784/3784 [==============================] - 1s 228us/step - loss: 0.6138 - acc: 0.6031 - val_loss: 0.7694 - val_acc: 0.5226\n",
      "Epoch 243/1000\n",
      "3784/3784 [==============================] - 1s 225us/step - loss: 0.6208 - acc: 0.5994 - val_loss: 0.7673 - val_acc: 0.5202\n",
      "Epoch 244/1000\n",
      "3784/3784 [==============================] - 1s 225us/step - loss: 0.6221 - acc: 0.6039 - val_loss: 0.7775 - val_acc: 0.5202\n",
      "Epoch 245/1000\n",
      "3784/3784 [==============================] - 1s 237us/step - loss: 0.6223 - acc: 0.5965 - val_loss: 0.7769 - val_acc: 0.5297\n",
      "Epoch 246/1000\n",
      "3784/3784 [==============================] - 1s 224us/step - loss: 0.6175 - acc: 0.6025 - val_loss: 0.7764 - val_acc: 0.5131\n",
      "Epoch 247/1000\n",
      "3784/3784 [==============================] - 1s 226us/step - loss: 0.6147 - acc: 0.6015 - val_loss: 0.7819 - val_acc: 0.5202\n",
      "Epoch 248/1000\n",
      "3784/3784 [==============================] - 1s 228us/step - loss: 0.6170 - acc: 0.5978 - val_loss: 0.7791 - val_acc: 0.5202\n",
      "Epoch 249/1000\n",
      "3784/3784 [==============================] - 1s 228us/step - loss: 0.6199 - acc: 0.5973 - val_loss: 0.7798 - val_acc: 0.5273\n",
      "Epoch 250/1000\n",
      "3784/3784 [==============================] - 1s 227us/step - loss: 0.6092 - acc: 0.6176 - val_loss: 0.7751 - val_acc: 0.5178\n",
      "Epoch 251/1000\n",
      "3784/3784 [==============================] - 1s 227us/step - loss: 0.6181 - acc: 0.6054 - val_loss: 0.7687 - val_acc: 0.5273\n",
      "Epoch 252/1000\n",
      "3784/3784 [==============================] - 1s 228us/step - loss: 0.6101 - acc: 0.6102 - val_loss: 0.7807 - val_acc: 0.5249\n",
      "Epoch 253/1000\n",
      "3784/3784 [==============================] - 1s 228us/step - loss: 0.6162 - acc: 0.5986 - val_loss: 0.7750 - val_acc: 0.5131\n",
      "Epoch 254/1000\n",
      "3784/3784 [==============================] - 1s 232us/step - loss: 0.6115 - acc: 0.6089 - val_loss: 0.7743 - val_acc: 0.5202\n",
      "Epoch 255/1000\n",
      "3784/3784 [==============================] - 1s 228us/step - loss: 0.6145 - acc: 0.6036 - val_loss: 0.7743 - val_acc: 0.5202\n",
      "Epoch 256/1000\n",
      "3784/3784 [==============================] - 1s 227us/step - loss: 0.6066 - acc: 0.6150 - val_loss: 0.7814 - val_acc: 0.5202\n",
      "Epoch 257/1000\n",
      "3784/3784 [==============================] - 1s 229us/step - loss: 0.6094 - acc: 0.6136 - val_loss: 0.7849 - val_acc: 0.5273\n",
      "Epoch 258/1000\n",
      "3784/3784 [==============================] - 1s 227us/step - loss: 0.6134 - acc: 0.6028 - val_loss: 0.7727 - val_acc: 0.5154\n",
      "Epoch 259/1000\n",
      "3784/3784 [==============================] - 1s 227us/step - loss: 0.6130 - acc: 0.6002 - val_loss: 0.7792 - val_acc: 0.5154\n",
      "Epoch 260/1000\n",
      "3784/3784 [==============================] - 1s 226us/step - loss: 0.6100 - acc: 0.5986 - val_loss: 0.7855 - val_acc: 0.5202\n",
      "Epoch 261/1000\n",
      "3784/3784 [==============================] - 1s 227us/step - loss: 0.6074 - acc: 0.6134 - val_loss: 0.7955 - val_acc: 0.5344\n",
      "Epoch 262/1000\n",
      "3784/3784 [==============================] - 1s 227us/step - loss: 0.6147 - acc: 0.6020 - val_loss: 0.7876 - val_acc: 0.5368\n",
      "Epoch 263/1000\n",
      "3784/3784 [==============================] - 1s 227us/step - loss: 0.6062 - acc: 0.6060 - val_loss: 0.7935 - val_acc: 0.5297\n",
      "Epoch 264/1000\n",
      "3784/3784 [==============================] - 1s 227us/step - loss: 0.6112 - acc: 0.6070 - val_loss: 0.7839 - val_acc: 0.5321\n",
      "Epoch 265/1000\n",
      "3784/3784 [==============================] - 1s 228us/step - loss: 0.6135 - acc: 0.6049 - val_loss: 0.7835 - val_acc: 0.5083\n",
      "Epoch 266/1000\n",
      "3784/3784 [==============================] - 1s 228us/step - loss: 0.6088 - acc: 0.6163 - val_loss: 0.7860 - val_acc: 0.5202\n",
      "Epoch 267/1000\n",
      "3784/3784 [==============================] - 1s 228us/step - loss: 0.6034 - acc: 0.6160 - val_loss: 0.8039 - val_acc: 0.5273\n",
      "Epoch 268/1000\n",
      "3784/3784 [==============================] - 1s 230us/step - loss: 0.6094 - acc: 0.6054 - val_loss: 0.7946 - val_acc: 0.5226\n",
      "Epoch 269/1000\n",
      "3784/3784 [==============================] - 1s 233us/step - loss: 0.6066 - acc: 0.6107 - val_loss: 0.7937 - val_acc: 0.5321\n",
      "Epoch 270/1000\n",
      "3784/3784 [==============================] - 1s 230us/step - loss: 0.6090 - acc: 0.6187 - val_loss: 0.7927 - val_acc: 0.5297\n",
      "Epoch 271/1000\n",
      "3784/3784 [==============================] - 1s 227us/step - loss: 0.6093 - acc: 0.6165 - val_loss: 0.8028 - val_acc: 0.5202\n",
      "Epoch 272/1000\n",
      "3784/3784 [==============================] - 1s 235us/step - loss: 0.6075 - acc: 0.6028 - val_loss: 0.8016 - val_acc: 0.5344\n",
      "Epoch 273/1000\n",
      "3784/3784 [==============================] - 1s 229us/step - loss: 0.6054 - acc: 0.6147 - val_loss: 0.7979 - val_acc: 0.5273\n",
      "Epoch 274/1000\n",
      "3784/3784 [==============================] - 1s 227us/step - loss: 0.6135 - acc: 0.6039 - val_loss: 0.7898 - val_acc: 0.5416\n",
      "Epoch 275/1000\n",
      "3784/3784 [==============================] - 1s 226us/step - loss: 0.6004 - acc: 0.6113 - val_loss: 0.8025 - val_acc: 0.5344\n",
      "Epoch 276/1000\n",
      "3784/3784 [==============================] - 1s 228us/step - loss: 0.6049 - acc: 0.6110 - val_loss: 0.7982 - val_acc: 0.5273\n",
      "Epoch 277/1000\n",
      "3784/3784 [==============================] - 1s 229us/step - loss: 0.5956 - acc: 0.6195 - val_loss: 0.8018 - val_acc: 0.5297\n",
      "Epoch 278/1000\n",
      "3784/3784 [==============================] - 1s 227us/step - loss: 0.6066 - acc: 0.6158 - val_loss: 0.8106 - val_acc: 0.5463\n",
      "Epoch 279/1000\n",
      "3784/3784 [==============================] - 1s 227us/step - loss: 0.5991 - acc: 0.6128 - val_loss: 0.7993 - val_acc: 0.5463\n",
      "Epoch 280/1000\n",
      "3784/3784 [==============================] - 1s 227us/step - loss: 0.6052 - acc: 0.6068 - val_loss: 0.7954 - val_acc: 0.5416\n",
      "Epoch 281/1000\n",
      "3784/3784 [==============================] - 1s 229us/step - loss: 0.6018 - acc: 0.6089 - val_loss: 0.8029 - val_acc: 0.5416\n",
      "Epoch 282/1000\n",
      "3784/3784 [==============================] - 1s 229us/step - loss: 0.6040 - acc: 0.6094 - val_loss: 0.7912 - val_acc: 0.5273\n",
      "Epoch 283/1000\n",
      "3784/3784 [==============================] - 1s 226us/step - loss: 0.5930 - acc: 0.6189 - val_loss: 0.8046 - val_acc: 0.5226\n",
      "Epoch 284/1000\n",
      "3784/3784 [==============================] - 1s 227us/step - loss: 0.5989 - acc: 0.6144 - val_loss: 0.8094 - val_acc: 0.5249\n",
      "Epoch 285/1000\n",
      "3784/3784 [==============================] - 1s 227us/step - loss: 0.6050 - acc: 0.6107 - val_loss: 0.8001 - val_acc: 0.5297\n",
      "Epoch 286/1000\n",
      "3784/3784 [==============================] - 1s 229us/step - loss: 0.5985 - acc: 0.6155 - val_loss: 0.7957 - val_acc: 0.5297\n",
      "Epoch 287/1000\n",
      "3784/3784 [==============================] - 1s 227us/step - loss: 0.6014 - acc: 0.6123 - val_loss: 0.8033 - val_acc: 0.5344\n",
      "Epoch 288/1000\n",
      "3784/3784 [==============================] - 1s 229us/step - loss: 0.6007 - acc: 0.6123 - val_loss: 0.8058 - val_acc: 0.5416\n",
      "Epoch 289/1000\n",
      "3784/3784 [==============================] - 1s 227us/step - loss: 0.5965 - acc: 0.6192 - val_loss: 0.8027 - val_acc: 0.5416\n",
      "Epoch 290/1000\n",
      "3784/3784 [==============================] - 1s 229us/step - loss: 0.6068 - acc: 0.6070 - val_loss: 0.7942 - val_acc: 0.5202\n",
      "Epoch 291/1000\n",
      "3784/3784 [==============================] - 1s 228us/step - loss: 0.6006 - acc: 0.6147 - val_loss: 0.7947 - val_acc: 0.5107\n",
      "Epoch 292/1000\n",
      "3784/3784 [==============================] - 1s 228us/step - loss: 0.5962 - acc: 0.6152 - val_loss: 0.7993 - val_acc: 0.5202\n",
      "Epoch 293/1000\n",
      "3784/3784 [==============================] - 1s 227us/step - loss: 0.6030 - acc: 0.6197 - val_loss: 0.7951 - val_acc: 0.5178\n",
      "Epoch 294/1000\n",
      "3784/3784 [==============================] - 1s 228us/step - loss: 0.5964 - acc: 0.6181 - val_loss: 0.7959 - val_acc: 0.5321\n",
      "Epoch 295/1000\n",
      "3784/3784 [==============================] - 1s 231us/step - loss: 0.5963 - acc: 0.6202 - val_loss: 0.8021 - val_acc: 0.5273\n",
      "Epoch 296/1000\n",
      "3784/3784 [==============================] - 1s 242us/step - loss: 0.5964 - acc: 0.6210 - val_loss: 0.8090 - val_acc: 0.5297\n",
      "Epoch 297/1000\n",
      "3784/3784 [==============================] - 1s 232us/step - loss: 0.5983 - acc: 0.6155 - val_loss: 0.7909 - val_acc: 0.5249\n",
      "Epoch 298/1000\n",
      "3784/3784 [==============================] - 1s 242us/step - loss: 0.6001 - acc: 0.6245 - val_loss: 0.7883 - val_acc: 0.5344\n",
      "Epoch 299/1000\n",
      "3784/3784 [==============================] - 1s 247us/step - loss: 0.5942 - acc: 0.6173 - val_loss: 0.7989 - val_acc: 0.5392\n",
      "Epoch 300/1000\n",
      "3784/3784 [==============================] - 1s 231us/step - loss: 0.6047 - acc: 0.6155 - val_loss: 0.7886 - val_acc: 0.5297\n",
      "Epoch 301/1000\n",
      "3784/3784 [==============================] - 1s 251us/step - loss: 0.6012 - acc: 0.6126 - val_loss: 0.7921 - val_acc: 0.5297\n",
      "Epoch 302/1000\n",
      "3784/3784 [==============================] - 1s 232us/step - loss: 0.6032 - acc: 0.6134 - val_loss: 0.8044 - val_acc: 0.5344\n",
      "Epoch 303/1000\n",
      "3784/3784 [==============================] - 1s 258us/step - loss: 0.5946 - acc: 0.6229 - val_loss: 0.8000 - val_acc: 0.5392\n",
      "Epoch 304/1000\n",
      "3784/3784 [==============================] - 1s 248us/step - loss: 0.6128 - acc: 0.6068 - val_loss: 0.7886 - val_acc: 0.5154\n",
      "Epoch 305/1000\n",
      "3784/3784 [==============================] - 1s 240us/step - loss: 0.5930 - acc: 0.6192 - val_loss: 0.7991 - val_acc: 0.5083\n",
      "Epoch 306/1000\n",
      "3784/3784 [==============================] - 1s 246us/step - loss: 0.5933 - acc: 0.6184 - val_loss: 0.8023 - val_acc: 0.5368\n",
      "Epoch 307/1000\n",
      "3784/3784 [==============================] - 1s 256us/step - loss: 0.5959 - acc: 0.6123 - val_loss: 0.8042 - val_acc: 0.5226\n",
      "Epoch 308/1000\n",
      "3784/3784 [==============================] - 1s 246us/step - loss: 0.5907 - acc: 0.6216 - val_loss: 0.8119 - val_acc: 0.5273\n",
      "Epoch 309/1000\n",
      "3784/3784 [==============================] - 1s 232us/step - loss: 0.5967 - acc: 0.6176 - val_loss: 0.8151 - val_acc: 0.5154\n",
      "Epoch 310/1000\n",
      "3784/3784 [==============================] - 1s 239us/step - loss: 0.6001 - acc: 0.6158 - val_loss: 0.8121 - val_acc: 0.5178\n",
      "Epoch 311/1000\n",
      "3784/3784 [==============================] - 1s 252us/step - loss: 0.5902 - acc: 0.6192 - val_loss: 0.8182 - val_acc: 0.5154\n",
      "Epoch 312/1000\n",
      "3784/3784 [==============================] - 1s 253us/step - loss: 0.5981 - acc: 0.6097 - val_loss: 0.8072 - val_acc: 0.5202\n",
      "Epoch 313/1000\n",
      "3784/3784 [==============================] - 1s 244us/step - loss: 0.5920 - acc: 0.6213 - val_loss: 0.8098 - val_acc: 0.5321\n",
      "Epoch 314/1000\n",
      "3784/3784 [==============================] - 1s 245us/step - loss: 0.5886 - acc: 0.6263 - val_loss: 0.8198 - val_acc: 0.5463\n",
      "Epoch 315/1000\n",
      "3784/3784 [==============================] - 1s 236us/step - loss: 0.5938 - acc: 0.6197 - val_loss: 0.8142 - val_acc: 0.5392\n",
      "Epoch 316/1000\n",
      "3784/3784 [==============================] - 1s 236us/step - loss: 0.5901 - acc: 0.6213 - val_loss: 0.8119 - val_acc: 0.5344\n",
      "Epoch 317/1000\n",
      "3784/3784 [==============================] - 1s 235us/step - loss: 0.5906 - acc: 0.6242 - val_loss: 0.8222 - val_acc: 0.5392\n",
      "Epoch 318/1000\n",
      "3784/3784 [==============================] - 1s 241us/step - loss: 0.5852 - acc: 0.6250 - val_loss: 0.8244 - val_acc: 0.5416\n",
      "Epoch 319/1000\n",
      "3784/3784 [==============================] - 1s 243us/step - loss: 0.5884 - acc: 0.6276 - val_loss: 0.8193 - val_acc: 0.5439\n",
      "Epoch 320/1000\n",
      "3784/3784 [==============================] - 1s 236us/step - loss: 0.5882 - acc: 0.6176 - val_loss: 0.8248 - val_acc: 0.5297\n",
      "Epoch 321/1000\n",
      "3784/3784 [==============================] - 1s 236us/step - loss: 0.5896 - acc: 0.6189 - val_loss: 0.8151 - val_acc: 0.5344\n",
      "Epoch 322/1000\n",
      "3784/3784 [==============================] - 1s 234us/step - loss: 0.5913 - acc: 0.6152 - val_loss: 0.8143 - val_acc: 0.5273\n",
      "Epoch 323/1000\n",
      "3784/3784 [==============================] - 1s 235us/step - loss: 0.5888 - acc: 0.6165 - val_loss: 0.8104 - val_acc: 0.5154\n",
      "Epoch 324/1000\n",
      "3784/3784 [==============================] - 1s 243us/step - loss: 0.5893 - acc: 0.6292 - val_loss: 0.8238 - val_acc: 0.5226\n",
      "Epoch 325/1000\n",
      "3784/3784 [==============================] - 1s 235us/step - loss: 0.5853 - acc: 0.6329 - val_loss: 0.8293 - val_acc: 0.5344\n",
      "Epoch 326/1000\n",
      "3784/3784 [==============================] - 1s 233us/step - loss: 0.5943 - acc: 0.6189 - val_loss: 0.8181 - val_acc: 0.5321\n",
      "Epoch 327/1000\n",
      "3784/3784 [==============================] - 1s 235us/step - loss: 0.5915 - acc: 0.6192 - val_loss: 0.8201 - val_acc: 0.5416\n",
      "Epoch 328/1000\n",
      "3784/3784 [==============================] - 1s 236us/step - loss: 0.5961 - acc: 0.6165 - val_loss: 0.8246 - val_acc: 0.5249\n",
      "Epoch 329/1000\n",
      "3784/3784 [==============================] - 1s 242us/step - loss: 0.5881 - acc: 0.6245 - val_loss: 0.8266 - val_acc: 0.5344\n",
      "Epoch 330/1000\n",
      "3784/3784 [==============================] - 1s 247us/step - loss: 0.5913 - acc: 0.6126 - val_loss: 0.8230 - val_acc: 0.5368\n",
      "Epoch 331/1000\n",
      "3784/3784 [==============================] - 1s 250us/step - loss: 0.5877 - acc: 0.6221 - val_loss: 0.8193 - val_acc: 0.5273\n",
      "Epoch 332/1000\n",
      "3784/3784 [==============================] - 1s 253us/step - loss: 0.5883 - acc: 0.6171 - val_loss: 0.8158 - val_acc: 0.5344\n",
      "Epoch 333/1000\n",
      "3784/3784 [==============================] - 1s 245us/step - loss: 0.5893 - acc: 0.6197 - val_loss: 0.8245 - val_acc: 0.5321\n",
      "Epoch 334/1000\n",
      "3784/3784 [==============================] - 1s 253us/step - loss: 0.5866 - acc: 0.6263 - val_loss: 0.8279 - val_acc: 0.5321\n",
      "Epoch 335/1000\n",
      "3784/3784 [==============================] - 1s 241us/step - loss: 0.5859 - acc: 0.6295 - val_loss: 0.8224 - val_acc: 0.5226\n",
      "Epoch 336/1000\n",
      "3784/3784 [==============================] - 1s 256us/step - loss: 0.5806 - acc: 0.6308 - val_loss: 0.8238 - val_acc: 0.5154\n",
      "Epoch 337/1000\n",
      "3784/3784 [==============================] - 1s 241us/step - loss: 0.5817 - acc: 0.6263 - val_loss: 0.8346 - val_acc: 0.5178\n",
      "Epoch 338/1000\n",
      "3784/3784 [==============================] - 1s 240us/step - loss: 0.5868 - acc: 0.6208 - val_loss: 0.8281 - val_acc: 0.5178\n",
      "Epoch 339/1000\n",
      "3784/3784 [==============================] - 1s 235us/step - loss: 0.5853 - acc: 0.6171 - val_loss: 0.8309 - val_acc: 0.5202\n",
      "Epoch 340/1000\n",
      "3784/3784 [==============================] - 1s 228us/step - loss: 0.5852 - acc: 0.6258 - val_loss: 0.8416 - val_acc: 0.5036\n",
      "Epoch 341/1000\n",
      "3784/3784 [==============================] - 1s 228us/step - loss: 0.5823 - acc: 0.6282 - val_loss: 0.8350 - val_acc: 0.5178\n",
      "Epoch 342/1000\n",
      "3784/3784 [==============================] - 1s 225us/step - loss: 0.5811 - acc: 0.6300 - val_loss: 0.8327 - val_acc: 0.5273\n",
      "Epoch 343/1000\n",
      "3784/3784 [==============================] - 1s 227us/step - loss: 0.5811 - acc: 0.6287 - val_loss: 0.8377 - val_acc: 0.5202\n",
      "Epoch 344/1000\n",
      "3784/3784 [==============================] - 1s 226us/step - loss: 0.5842 - acc: 0.6335 - val_loss: 0.8387 - val_acc: 0.5178\n",
      "Epoch 345/1000\n",
      "3784/3784 [==============================] - 1s 225us/step - loss: 0.5808 - acc: 0.6295 - val_loss: 0.8286 - val_acc: 0.5202\n",
      "Epoch 346/1000\n",
      "3784/3784 [==============================] - 1s 228us/step - loss: 0.5797 - acc: 0.6337 - val_loss: 0.8436 - val_acc: 0.5226\n",
      "Epoch 347/1000\n",
      "3784/3784 [==============================] - 1s 226us/step - loss: 0.5888 - acc: 0.6229 - val_loss: 0.8229 - val_acc: 0.5131\n",
      "Epoch 348/1000\n",
      "3784/3784 [==============================] - 1s 225us/step - loss: 0.5871 - acc: 0.6253 - val_loss: 0.8281 - val_acc: 0.5202\n",
      "Epoch 349/1000\n",
      "3784/3784 [==============================] - 1s 234us/step - loss: 0.5807 - acc: 0.6271 - val_loss: 0.8263 - val_acc: 0.5249\n",
      "Epoch 350/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3784/3784 [==============================] - 1s 224us/step - loss: 0.5870 - acc: 0.6218 - val_loss: 0.8267 - val_acc: 0.5273\n",
      "Epoch 351/1000\n",
      "3784/3784 [==============================] - 1s 225us/step - loss: 0.5825 - acc: 0.6239 - val_loss: 0.8263 - val_acc: 0.5249\n",
      "Epoch 352/1000\n",
      "3784/3784 [==============================] - 1s 232us/step - loss: 0.5785 - acc: 0.6258 - val_loss: 0.8290 - val_acc: 0.5178\n",
      "Epoch 353/1000\n",
      "3784/3784 [==============================] - 1s 225us/step - loss: 0.5749 - acc: 0.6369 - val_loss: 0.8382 - val_acc: 0.5273\n",
      "Epoch 354/1000\n",
      "3784/3784 [==============================] - 1s 226us/step - loss: 0.5733 - acc: 0.6374 - val_loss: 0.8281 - val_acc: 0.5321\n",
      "Epoch 355/1000\n",
      "3784/3784 [==============================] - 1s 227us/step - loss: 0.5770 - acc: 0.6356 - val_loss: 0.8490 - val_acc: 0.5178\n",
      "Epoch 356/1000\n",
      "3784/3784 [==============================] - 1s 227us/step - loss: 0.5760 - acc: 0.6284 - val_loss: 0.8351 - val_acc: 0.5226\n",
      "Epoch 357/1000\n",
      "3784/3784 [==============================] - 1s 225us/step - loss: 0.5834 - acc: 0.6284 - val_loss: 0.8389 - val_acc: 0.5178\n",
      "Epoch 358/1000\n",
      "3784/3784 [==============================] - 1s 226us/step - loss: 0.5747 - acc: 0.6379 - val_loss: 0.8396 - val_acc: 0.5178\n",
      "Epoch 359/1000\n",
      "3784/3784 [==============================] - 1s 225us/step - loss: 0.5834 - acc: 0.6239 - val_loss: 0.8328 - val_acc: 0.5249\n",
      "Epoch 360/1000\n",
      "3784/3784 [==============================] - 1s 226us/step - loss: 0.5742 - acc: 0.6290 - val_loss: 0.8359 - val_acc: 0.5202\n",
      "Epoch 361/1000\n",
      "3784/3784 [==============================] - 1s 224us/step - loss: 0.5858 - acc: 0.6208 - val_loss: 0.8340 - val_acc: 0.5392\n",
      "Epoch 362/1000\n",
      "3784/3784 [==============================] - 1s 226us/step - loss: 0.5728 - acc: 0.6340 - val_loss: 0.8367 - val_acc: 0.5368\n",
      "Epoch 363/1000\n",
      "3784/3784 [==============================] - 1s 226us/step - loss: 0.5768 - acc: 0.6382 - val_loss: 0.8440 - val_acc: 0.5344\n",
      "Epoch 364/1000\n",
      "3784/3784 [==============================] - 1s 226us/step - loss: 0.5773 - acc: 0.6313 - val_loss: 0.8239 - val_acc: 0.5368\n",
      "Epoch 365/1000\n",
      "3784/3784 [==============================] - 1s 225us/step - loss: 0.5715 - acc: 0.6353 - val_loss: 0.8425 - val_acc: 0.5392\n",
      "Epoch 366/1000\n",
      "3784/3784 [==============================] - 1s 227us/step - loss: 0.5684 - acc: 0.6364 - val_loss: 0.8599 - val_acc: 0.5297\n",
      "Epoch 367/1000\n",
      "3784/3784 [==============================] - 1s 226us/step - loss: 0.5739 - acc: 0.6319 - val_loss: 0.8580 - val_acc: 0.5297\n",
      "Epoch 368/1000\n",
      "3784/3784 [==============================] - 1s 225us/step - loss: 0.5831 - acc: 0.6247 - val_loss: 0.8477 - val_acc: 0.5321\n",
      "Epoch 369/1000\n",
      "3784/3784 [==============================] - 1s 226us/step - loss: 0.5699 - acc: 0.6313 - val_loss: 0.8487 - val_acc: 0.5392\n",
      "Epoch 370/1000\n",
      "3784/3784 [==============================] - 1s 226us/step - loss: 0.5701 - acc: 0.6379 - val_loss: 0.8517 - val_acc: 0.5368\n",
      "Epoch 371/1000\n",
      "3784/3784 [==============================] - 1s 227us/step - loss: 0.5746 - acc: 0.6250 - val_loss: 0.8510 - val_acc: 0.5368\n",
      "Epoch 372/1000\n",
      "3784/3784 [==============================] - 1s 225us/step - loss: 0.5732 - acc: 0.6327 - val_loss: 0.8536 - val_acc: 0.5344\n",
      "Epoch 373/1000\n",
      "3784/3784 [==============================] - 1s 224us/step - loss: 0.5804 - acc: 0.6237 - val_loss: 0.8395 - val_acc: 0.5439\n",
      "Epoch 374/1000\n",
      "3784/3784 [==============================] - 1s 226us/step - loss: 0.5736 - acc: 0.6337 - val_loss: 0.8395 - val_acc: 0.5487\n",
      "Epoch 375/1000\n",
      "3784/3784 [==============================] - 1s 226us/step - loss: 0.5750 - acc: 0.6300 - val_loss: 0.8284 - val_acc: 0.5249\n",
      "Epoch 376/1000\n",
      "3784/3784 [==============================] - 1s 234us/step - loss: 0.5753 - acc: 0.6321 - val_loss: 0.8454 - val_acc: 0.5392\n",
      "Epoch 377/1000\n",
      "3784/3784 [==============================] - 1s 231us/step - loss: 0.5700 - acc: 0.6308 - val_loss: 0.8404 - val_acc: 0.5368\n",
      "Epoch 378/1000\n",
      "3784/3784 [==============================] - 1s 228us/step - loss: 0.5770 - acc: 0.6274 - val_loss: 0.8484 - val_acc: 0.5344\n",
      "Epoch 379/1000\n",
      "3784/3784 [==============================] - 1s 229us/step - loss: 0.5703 - acc: 0.6321 - val_loss: 0.8507 - val_acc: 0.5321\n",
      "Epoch 380/1000\n",
      "3784/3784 [==============================] - 1s 229us/step - loss: 0.5750 - acc: 0.6300 - val_loss: 0.8437 - val_acc: 0.5344\n",
      "Epoch 381/1000\n",
      "3784/3784 [==============================] - 1s 224us/step - loss: 0.5707 - acc: 0.6300 - val_loss: 0.8423 - val_acc: 0.5297\n",
      "Epoch 382/1000\n",
      "3784/3784 [==============================] - 1s 226us/step - loss: 0.5704 - acc: 0.6345 - val_loss: 0.8318 - val_acc: 0.5273\n",
      "Epoch 383/1000\n",
      "3784/3784 [==============================] - 1s 227us/step - loss: 0.5745 - acc: 0.6327 - val_loss: 0.8399 - val_acc: 0.5226\n",
      "Epoch 384/1000\n",
      "3784/3784 [==============================] - 1s 226us/step - loss: 0.5749 - acc: 0.6403 - val_loss: 0.8563 - val_acc: 0.5273\n",
      "Epoch 385/1000\n",
      "3784/3784 [==============================] - 1s 226us/step - loss: 0.5679 - acc: 0.6427 - val_loss: 0.8513 - val_acc: 0.5297\n",
      "Epoch 386/1000\n",
      "3784/3784 [==============================] - 1s 227us/step - loss: 0.5624 - acc: 0.6422 - val_loss: 0.8524 - val_acc: 0.5344\n",
      "Epoch 387/1000\n",
      "3784/3784 [==============================] - 1s 226us/step - loss: 0.5726 - acc: 0.6313 - val_loss: 0.8437 - val_acc: 0.5273\n",
      "Epoch 388/1000\n",
      "3784/3784 [==============================] - 1s 224us/step - loss: 0.5675 - acc: 0.6340 - val_loss: 0.8494 - val_acc: 0.5273\n",
      "Epoch 389/1000\n",
      "3784/3784 [==============================] - 1s 226us/step - loss: 0.5679 - acc: 0.6356 - val_loss: 0.8730 - val_acc: 0.5297\n",
      "Epoch 390/1000\n",
      "3784/3784 [==============================] - 1s 224us/step - loss: 0.5653 - acc: 0.6366 - val_loss: 0.8554 - val_acc: 0.5178\n",
      "Epoch 391/1000\n",
      "3784/3784 [==============================] - 1s 228us/step - loss: 0.5703 - acc: 0.6422 - val_loss: 0.8468 - val_acc: 0.5297\n",
      "Epoch 392/1000\n",
      "3784/3784 [==============================] - 1s 226us/step - loss: 0.5679 - acc: 0.6485 - val_loss: 0.8693 - val_acc: 0.5511\n",
      "Epoch 393/1000\n",
      "3784/3784 [==============================] - 1s 224us/step - loss: 0.5671 - acc: 0.6274 - val_loss: 0.8674 - val_acc: 0.5463\n",
      "Epoch 394/1000\n",
      "3784/3784 [==============================] - 1s 228us/step - loss: 0.5664 - acc: 0.6366 - val_loss: 0.8635 - val_acc: 0.5297\n",
      "Epoch 395/1000\n",
      "3784/3784 [==============================] - 1s 225us/step - loss: 0.5639 - acc: 0.6361 - val_loss: 0.8799 - val_acc: 0.5416\n",
      "Epoch 396/1000\n",
      "3784/3784 [==============================] - 1s 225us/step - loss: 0.5625 - acc: 0.6448 - val_loss: 0.8737 - val_acc: 0.5392\n",
      "Epoch 397/1000\n",
      "3784/3784 [==============================] - 1s 227us/step - loss: 0.5698 - acc: 0.6364 - val_loss: 0.8632 - val_acc: 0.5344\n",
      "Epoch 398/1000\n",
      "3784/3784 [==============================] - 1s 226us/step - loss: 0.5663 - acc: 0.6345 - val_loss: 0.8515 - val_acc: 0.5344\n",
      "Epoch 399/1000\n",
      "3784/3784 [==============================] - 1s 225us/step - loss: 0.5645 - acc: 0.6430 - val_loss: 0.8504 - val_acc: 0.5416\n",
      "Epoch 400/1000\n",
      "3784/3784 [==============================] - 1s 226us/step - loss: 0.5632 - acc: 0.6422 - val_loss: 0.8639 - val_acc: 0.5297\n",
      "Epoch 401/1000\n",
      "3784/3784 [==============================] - 1s 226us/step - loss: 0.5679 - acc: 0.6353 - val_loss: 0.8509 - val_acc: 0.5416\n",
      "Epoch 402/1000\n",
      "3784/3784 [==============================] - 1s 225us/step - loss: 0.5730 - acc: 0.6324 - val_loss: 0.8560 - val_acc: 0.5487\n",
      "Epoch 403/1000\n",
      "3784/3784 [==============================] - 1s 233us/step - loss: 0.5612 - acc: 0.6456 - val_loss: 0.8707 - val_acc: 0.5463\n",
      "Epoch 404/1000\n",
      "3784/3784 [==============================] - 1s 223us/step - loss: 0.5598 - acc: 0.6430 - val_loss: 0.8636 - val_acc: 0.5321\n",
      "Epoch 405/1000\n",
      "3784/3784 [==============================] - 1s 225us/step - loss: 0.5665 - acc: 0.6329 - val_loss: 0.8670 - val_acc: 0.5321\n",
      "Epoch 406/1000\n",
      "3784/3784 [==============================] - 1s 230us/step - loss: 0.5616 - acc: 0.6401 - val_loss: 0.8650 - val_acc: 0.5226\n",
      "Epoch 407/1000\n",
      "3784/3784 [==============================] - 1s 226us/step - loss: 0.5652 - acc: 0.6321 - val_loss: 0.8706 - val_acc: 0.5321\n",
      "Epoch 408/1000\n",
      "3784/3784 [==============================] - 1s 225us/step - loss: 0.5601 - acc: 0.6398 - val_loss: 0.8716 - val_acc: 0.5273\n",
      "Epoch 409/1000\n",
      "3784/3784 [==============================] - 1s 226us/step - loss: 0.5633 - acc: 0.6411 - val_loss: 0.8682 - val_acc: 0.5368\n",
      "Epoch 410/1000\n",
      "3784/3784 [==============================] - 1s 227us/step - loss: 0.5671 - acc: 0.6350 - val_loss: 0.8770 - val_acc: 0.5178\n",
      "Epoch 411/1000\n",
      "3784/3784 [==============================] - 1s 224us/step - loss: 0.5684 - acc: 0.6403 - val_loss: 0.8650 - val_acc: 0.5416\n",
      "Epoch 412/1000\n",
      "3784/3784 [==============================] - 1s 227us/step - loss: 0.5613 - acc: 0.6477 - val_loss: 0.8655 - val_acc: 0.5511\n",
      "Epoch 413/1000\n",
      "3784/3784 [==============================] - 1s 226us/step - loss: 0.5565 - acc: 0.6419 - val_loss: 0.8622 - val_acc: 0.5487\n",
      "Epoch 414/1000\n",
      "3784/3784 [==============================] - 1s 229us/step - loss: 0.5550 - acc: 0.6422 - val_loss: 0.8653 - val_acc: 0.5416\n",
      "Epoch 415/1000\n",
      "3784/3784 [==============================] - 1s 225us/step - loss: 0.5652 - acc: 0.6390 - val_loss: 0.8549 - val_acc: 0.5416\n",
      "Epoch 416/1000\n",
      "3784/3784 [==============================] - 1s 226us/step - loss: 0.5570 - acc: 0.6387 - val_loss: 0.8640 - val_acc: 0.5511\n",
      "Epoch 417/1000\n",
      "3784/3784 [==============================] - 1s 226us/step - loss: 0.5523 - acc: 0.6496 - val_loss: 0.8611 - val_acc: 0.5463\n",
      "Epoch 418/1000\n",
      "3784/3784 [==============================] - 1s 226us/step - loss: 0.5690 - acc: 0.6411 - val_loss: 0.8500 - val_acc: 0.5416\n",
      "Epoch 419/1000\n",
      "3784/3784 [==============================] - 1s 224us/step - loss: 0.5550 - acc: 0.6488 - val_loss: 0.8609 - val_acc: 0.5249\n",
      "Epoch 420/1000\n",
      "3784/3784 [==============================] - 1s 226us/step - loss: 0.5559 - acc: 0.6448 - val_loss: 0.8688 - val_acc: 0.5416\n",
      "Epoch 421/1000\n",
      "3784/3784 [==============================] - 1s 225us/step - loss: 0.5548 - acc: 0.6480 - val_loss: 0.8657 - val_acc: 0.5487\n",
      "Epoch 422/1000\n",
      "3784/3784 [==============================] - 1s 225us/step - loss: 0.5575 - acc: 0.6427 - val_loss: 0.8619 - val_acc: 0.5297\n",
      "Epoch 423/1000\n",
      "3784/3784 [==============================] - 1s 225us/step - loss: 0.5595 - acc: 0.6358 - val_loss: 0.8616 - val_acc: 0.5273\n",
      "Epoch 424/1000\n",
      "3784/3784 [==============================] - 1s 225us/step - loss: 0.5549 - acc: 0.6451 - val_loss: 0.8823 - val_acc: 0.5178\n",
      "Epoch 425/1000\n",
      "3784/3784 [==============================] - 1s 227us/step - loss: 0.5694 - acc: 0.6353 - val_loss: 0.8563 - val_acc: 0.5249\n",
      "Epoch 426/1000\n",
      "3784/3784 [==============================] - 1s 226us/step - loss: 0.5637 - acc: 0.6364 - val_loss: 0.8547 - val_acc: 0.5321\n",
      "Epoch 427/1000\n",
      "3784/3784 [==============================] - 1s 226us/step - loss: 0.5570 - acc: 0.6477 - val_loss: 0.8770 - val_acc: 0.5439\n",
      "Epoch 428/1000\n",
      "3784/3784 [==============================] - 1s 226us/step - loss: 0.5538 - acc: 0.6477 - val_loss: 0.8916 - val_acc: 0.5368\n",
      "Epoch 429/1000\n",
      "3784/3784 [==============================] - 1s 225us/step - loss: 0.5581 - acc: 0.6422 - val_loss: 0.8684 - val_acc: 0.5249\n",
      "Epoch 430/1000\n",
      "3784/3784 [==============================] - 1s 233us/step - loss: 0.5591 - acc: 0.6422 - val_loss: 0.8676 - val_acc: 0.5273\n",
      "Epoch 431/1000\n",
      "3784/3784 [==============================] - 1s 225us/step - loss: 0.5605 - acc: 0.6451 - val_loss: 0.8770 - val_acc: 0.5273\n",
      "Epoch 432/1000\n",
      "3784/3784 [==============================] - 1s 227us/step - loss: 0.5584 - acc: 0.6379 - val_loss: 0.8685 - val_acc: 0.5226\n",
      "Epoch 433/1000\n",
      "3784/3784 [==============================] - 1s 225us/step - loss: 0.5531 - acc: 0.6424 - val_loss: 0.8863 - val_acc: 0.5202\n",
      "Epoch 434/1000\n",
      "3784/3784 [==============================] - 1s 226us/step - loss: 0.5548 - acc: 0.6435 - val_loss: 0.8796 - val_acc: 0.5178\n",
      "Epoch 435/1000\n",
      "3784/3784 [==============================] - 1s 227us/step - loss: 0.5543 - acc: 0.6451 - val_loss: 0.8805 - val_acc: 0.5226\n",
      "Epoch 436/1000\n",
      "3784/3784 [==============================] - 1s 225us/step - loss: 0.5539 - acc: 0.6506 - val_loss: 0.8734 - val_acc: 0.5226\n",
      "Epoch 437/1000\n",
      "3784/3784 [==============================] - 1s 227us/step - loss: 0.5606 - acc: 0.6472 - val_loss: 0.8735 - val_acc: 0.5273\n",
      "Epoch 438/1000\n",
      "3784/3784 [==============================] - 1s 226us/step - loss: 0.5482 - acc: 0.6480 - val_loss: 0.8934 - val_acc: 0.5297\n",
      "Epoch 439/1000\n",
      "3784/3784 [==============================] - 1s 227us/step - loss: 0.5559 - acc: 0.6435 - val_loss: 0.8913 - val_acc: 0.5249\n",
      "Epoch 440/1000\n",
      "3784/3784 [==============================] - 1s 227us/step - loss: 0.5680 - acc: 0.6438 - val_loss: 0.8757 - val_acc: 0.5368\n",
      "Epoch 441/1000\n",
      "3784/3784 [==============================] - 1s 227us/step - loss: 0.5595 - acc: 0.6409 - val_loss: 0.8666 - val_acc: 0.5321\n",
      "Epoch 442/1000\n",
      "3784/3784 [==============================] - 1s 226us/step - loss: 0.5550 - acc: 0.6406 - val_loss: 0.8659 - val_acc: 0.5249\n",
      "Epoch 443/1000\n",
      "3784/3784 [==============================] - 1s 225us/step - loss: 0.5531 - acc: 0.6432 - val_loss: 0.8764 - val_acc: 0.5416\n",
      "Epoch 444/1000\n",
      "3784/3784 [==============================] - 1s 225us/step - loss: 0.5544 - acc: 0.6456 - val_loss: 0.8910 - val_acc: 0.5392\n",
      "Epoch 445/1000\n",
      "3784/3784 [==============================] - 1s 226us/step - loss: 0.5525 - acc: 0.6501 - val_loss: 0.8797 - val_acc: 0.5368\n",
      "Epoch 446/1000\n",
      "3784/3784 [==============================] - 1s 224us/step - loss: 0.5551 - acc: 0.6477 - val_loss: 0.8807 - val_acc: 0.5416\n",
      "Epoch 447/1000\n",
      "3784/3784 [==============================] - 1s 227us/step - loss: 0.5473 - acc: 0.6401 - val_loss: 0.8849 - val_acc: 0.5344\n",
      "Epoch 448/1000\n",
      "3784/3784 [==============================] - 1s 228us/step - loss: 0.5541 - acc: 0.6467 - val_loss: 0.8867 - val_acc: 0.5392\n",
      "Epoch 449/1000\n",
      "3784/3784 [==============================] - 1s 226us/step - loss: 0.5531 - acc: 0.6483 - val_loss: 0.8730 - val_acc: 0.5297\n",
      "Epoch 450/1000\n",
      "3784/3784 [==============================] - 1s 226us/step - loss: 0.5555 - acc: 0.6416 - val_loss: 0.8891 - val_acc: 0.5297\n",
      "Epoch 451/1000\n",
      "3784/3784 [==============================] - 1s 225us/step - loss: 0.5624 - acc: 0.6472 - val_loss: 0.8828 - val_acc: 0.5226\n",
      "Epoch 452/1000\n",
      "3784/3784 [==============================] - 1s 227us/step - loss: 0.5444 - acc: 0.6493 - val_loss: 0.8867 - val_acc: 0.5297\n",
      "Epoch 453/1000\n",
      "3784/3784 [==============================] - 1s 227us/step - loss: 0.5447 - acc: 0.6522 - val_loss: 0.8884 - val_acc: 0.5439\n",
      "Epoch 454/1000\n",
      "3784/3784 [==============================] - 1s 226us/step - loss: 0.5498 - acc: 0.6504 - val_loss: 0.9053 - val_acc: 0.5368\n",
      "Epoch 455/1000\n",
      "3784/3784 [==============================] - 1s 226us/step - loss: 0.5487 - acc: 0.6490 - val_loss: 0.9077 - val_acc: 0.5368\n",
      "Epoch 456/1000\n",
      "3784/3784 [==============================] - 1s 227us/step - loss: 0.5490 - acc: 0.6520 - val_loss: 0.8956 - val_acc: 0.5344\n",
      "Epoch 457/1000\n",
      "3784/3784 [==============================] - 1s 234us/step - loss: 0.5477 - acc: 0.6493 - val_loss: 0.9022 - val_acc: 0.5439\n",
      "Epoch 458/1000\n",
      "3784/3784 [==============================] - 1s 225us/step - loss: 0.5465 - acc: 0.6469 - val_loss: 0.8851 - val_acc: 0.5416\n",
      "Epoch 459/1000\n",
      "3784/3784 [==============================] - 1s 226us/step - loss: 0.5454 - acc: 0.6522 - val_loss: 0.8912 - val_acc: 0.5392\n",
      "Epoch 460/1000\n",
      "3784/3784 [==============================] - 1s 226us/step - loss: 0.5547 - acc: 0.6451 - val_loss: 0.8945 - val_acc: 0.5344\n",
      "Epoch 461/1000\n",
      "3784/3784 [==============================] - 1s 225us/step - loss: 0.5522 - acc: 0.6496 - val_loss: 0.8915 - val_acc: 0.5463\n",
      "Epoch 462/1000\n",
      "3784/3784 [==============================] - 1s 225us/step - loss: 0.5533 - acc: 0.6480 - val_loss: 0.8967 - val_acc: 0.5344\n",
      "Epoch 463/1000\n",
      "3784/3784 [==============================] - 1s 225us/step - loss: 0.5486 - acc: 0.6496 - val_loss: 0.8913 - val_acc: 0.5344\n",
      "Epoch 464/1000\n",
      "3784/3784 [==============================] - 1s 225us/step - loss: 0.5467 - acc: 0.6490 - val_loss: 0.9046 - val_acc: 0.5297\n",
      "Epoch 465/1000\n",
      "3784/3784 [==============================] - 1s 227us/step - loss: 0.5439 - acc: 0.6538 - val_loss: 0.8876 - val_acc: 0.5344\n",
      "Epoch 466/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3784/3784 [==============================] - 1s 226us/step - loss: 0.5444 - acc: 0.6567 - val_loss: 0.9071 - val_acc: 0.5321\n",
      "Epoch 467/1000\n",
      "3784/3784 [==============================] - 1s 225us/step - loss: 0.5453 - acc: 0.6501 - val_loss: 0.9223 - val_acc: 0.5273\n",
      "Epoch 468/1000\n",
      "3784/3784 [==============================] - 1s 226us/step - loss: 0.5394 - acc: 0.6517 - val_loss: 0.9045 - val_acc: 0.5344\n",
      "Epoch 469/1000\n",
      "3784/3784 [==============================] - 1s 224us/step - loss: 0.5398 - acc: 0.6564 - val_loss: 0.9006 - val_acc: 0.5297\n",
      "Epoch 470/1000\n",
      "3784/3784 [==============================] - 1s 225us/step - loss: 0.5423 - acc: 0.6498 - val_loss: 0.9008 - val_acc: 0.5273\n",
      "Epoch 471/1000\n",
      "3784/3784 [==============================] - 1s 225us/step - loss: 0.5427 - acc: 0.6501 - val_loss: 0.8819 - val_acc: 0.5273\n",
      "Epoch 472/1000\n",
      "3784/3784 [==============================] - 1s 227us/step - loss: 0.5496 - acc: 0.6469 - val_loss: 0.8943 - val_acc: 0.5273\n",
      "Epoch 473/1000\n",
      "3784/3784 [==============================] - 1s 224us/step - loss: 0.5454 - acc: 0.6483 - val_loss: 0.9014 - val_acc: 0.5273\n",
      "Epoch 474/1000\n",
      "3784/3784 [==============================] - 1s 227us/step - loss: 0.5417 - acc: 0.6498 - val_loss: 0.9029 - val_acc: 0.5202\n",
      "Epoch 475/1000\n",
      "3784/3784 [==============================] - 1s 227us/step - loss: 0.5501 - acc: 0.6469 - val_loss: 0.8931 - val_acc: 0.5273\n",
      "Epoch 476/1000\n",
      "3784/3784 [==============================] - 1s 227us/step - loss: 0.5524 - acc: 0.6504 - val_loss: 0.8976 - val_acc: 0.5178\n",
      "Epoch 477/1000\n",
      "3784/3784 [==============================] - 1s 231us/step - loss: 0.5475 - acc: 0.6525 - val_loss: 0.8947 - val_acc: 0.5297\n",
      "Epoch 478/1000\n",
      "3784/3784 [==============================] - 1s 228us/step - loss: 0.5432 - acc: 0.6580 - val_loss: 0.8995 - val_acc: 0.5368\n",
      "Epoch 479/1000\n",
      "3784/3784 [==============================] - 1s 227us/step - loss: 0.5448 - acc: 0.6628 - val_loss: 0.9023 - val_acc: 0.5273\n",
      "Epoch 480/1000\n",
      "3784/3784 [==============================] - 1s 227us/step - loss: 0.5447 - acc: 0.6520 - val_loss: 0.9121 - val_acc: 0.5226\n",
      "Epoch 481/1000\n",
      "3784/3784 [==============================] - 1s 225us/step - loss: 0.5359 - acc: 0.6541 - val_loss: 0.9190 - val_acc: 0.5321\n",
      "Epoch 482/1000\n",
      "3784/3784 [==============================] - 1s 227us/step - loss: 0.5436 - acc: 0.6554 - val_loss: 0.9096 - val_acc: 0.5321\n",
      "Epoch 483/1000\n",
      "3784/3784 [==============================] - 1s 225us/step - loss: 0.5495 - acc: 0.6533 - val_loss: 0.9109 - val_acc: 0.5344\n",
      "Epoch 484/1000\n",
      "3784/3784 [==============================] - 1s 234us/step - loss: 0.5425 - acc: 0.6493 - val_loss: 0.9145 - val_acc: 0.5297\n",
      "Epoch 485/1000\n",
      "3784/3784 [==============================] - 1s 226us/step - loss: 0.5451 - acc: 0.6586 - val_loss: 0.9187 - val_acc: 0.5392\n",
      "Epoch 486/1000\n",
      "3784/3784 [==============================] - 1s 228us/step - loss: 0.5496 - acc: 0.6467 - val_loss: 0.9385 - val_acc: 0.5416\n",
      "Epoch 487/1000\n",
      "3784/3784 [==============================] - 1s 226us/step - loss: 0.5430 - acc: 0.6570 - val_loss: 0.9152 - val_acc: 0.5439\n",
      "Epoch 488/1000\n",
      "3784/3784 [==============================] - 1s 224us/step - loss: 0.5406 - acc: 0.6541 - val_loss: 0.9281 - val_acc: 0.5249\n",
      "Epoch 489/1000\n",
      "3784/3784 [==============================] - 1s 226us/step - loss: 0.5430 - acc: 0.6517 - val_loss: 0.9228 - val_acc: 0.5439\n",
      "Epoch 490/1000\n",
      "3784/3784 [==============================] - 1s 225us/step - loss: 0.5360 - acc: 0.6525 - val_loss: 0.9310 - val_acc: 0.5321\n",
      "Epoch 491/1000\n",
      "3784/3784 [==============================] - 1s 226us/step - loss: 0.5469 - acc: 0.6533 - val_loss: 0.9157 - val_acc: 0.5344\n",
      "Epoch 492/1000\n",
      "3784/3784 [==============================] - 1s 229us/step - loss: 0.5399 - acc: 0.6591 - val_loss: 0.9227 - val_acc: 0.5368\n",
      "Epoch 493/1000\n",
      "3784/3784 [==============================] - 1s 225us/step - loss: 0.5468 - acc: 0.6490 - val_loss: 0.9210 - val_acc: 0.5463\n",
      "Epoch 494/1000\n",
      "3784/3784 [==============================] - 1s 227us/step - loss: 0.5486 - acc: 0.6472 - val_loss: 0.9060 - val_acc: 0.5416\n",
      "Epoch 495/1000\n",
      "3784/3784 [==============================] - 1s 227us/step - loss: 0.5451 - acc: 0.6594 - val_loss: 0.9110 - val_acc: 0.5392\n",
      "Epoch 496/1000\n",
      "3784/3784 [==============================] - 1s 229us/step - loss: 0.5353 - acc: 0.6557 - val_loss: 0.9182 - val_acc: 0.5297\n",
      "Epoch 497/1000\n",
      "3784/3784 [==============================] - 1s 226us/step - loss: 0.5553 - acc: 0.6467 - val_loss: 0.9015 - val_acc: 0.5344\n",
      "Epoch 498/1000\n",
      "3784/3784 [==============================] - 1s 227us/step - loss: 0.5364 - acc: 0.6527 - val_loss: 0.9177 - val_acc: 0.5368\n",
      "Epoch 499/1000\n",
      "3784/3784 [==============================] - 1s 228us/step - loss: 0.5426 - acc: 0.6504 - val_loss: 0.9088 - val_acc: 0.5439\n",
      "Epoch 500/1000\n",
      "3784/3784 [==============================] - 1s 225us/step - loss: 0.5396 - acc: 0.6580 - val_loss: 0.9037 - val_acc: 0.5439\n",
      "Epoch 501/1000\n",
      "3784/3784 [==============================] - 1s 226us/step - loss: 0.5331 - acc: 0.6583 - val_loss: 0.9276 - val_acc: 0.5368\n",
      "Epoch 502/1000\n",
      "3784/3784 [==============================] - 1s 227us/step - loss: 0.5458 - acc: 0.6546 - val_loss: 0.9192 - val_acc: 0.5392\n",
      "Epoch 503/1000\n",
      "3784/3784 [==============================] - 1s 227us/step - loss: 0.5484 - acc: 0.6504 - val_loss: 0.9015 - val_acc: 0.5392\n",
      "Epoch 504/1000\n",
      "3784/3784 [==============================] - 1s 227us/step - loss: 0.5470 - acc: 0.6562 - val_loss: 0.9209 - val_acc: 0.5534\n",
      "Epoch 505/1000\n",
      "3784/3784 [==============================] - 1s 226us/step - loss: 0.5354 - acc: 0.6609 - val_loss: 0.9241 - val_acc: 0.5582\n",
      "Epoch 506/1000\n",
      "3784/3784 [==============================] - 1s 225us/step - loss: 0.5404 - acc: 0.6509 - val_loss: 0.9230 - val_acc: 0.5606\n",
      "Epoch 507/1000\n",
      "3784/3784 [==============================] - 1s 225us/step - loss: 0.5321 - acc: 0.6596 - val_loss: 0.9233 - val_acc: 0.5534\n",
      "Epoch 508/1000\n",
      "3784/3784 [==============================] - 1s 227us/step - loss: 0.5394 - acc: 0.6493 - val_loss: 0.9107 - val_acc: 0.5558\n",
      "Epoch 509/1000\n",
      "3784/3784 [==============================] - 1s 230us/step - loss: 0.5374 - acc: 0.6522 - val_loss: 0.9138 - val_acc: 0.5511\n",
      "Epoch 510/1000\n",
      "3784/3784 [==============================] - 1s 227us/step - loss: 0.5358 - acc: 0.6567 - val_loss: 0.9204 - val_acc: 0.5368\n",
      "Epoch 511/1000\n",
      "3784/3784 [==============================] - 1s 233us/step - loss: 0.5383 - acc: 0.6541 - val_loss: 0.9228 - val_acc: 0.5416\n",
      "Epoch 512/1000\n",
      "3784/3784 [==============================] - 1s 225us/step - loss: 0.5350 - acc: 0.6506 - val_loss: 0.9154 - val_acc: 0.5321\n",
      "Epoch 513/1000\n",
      "3784/3784 [==============================] - 1s 228us/step - loss: 0.5271 - acc: 0.6736 - val_loss: 0.9310 - val_acc: 0.5416\n",
      "Epoch 514/1000\n",
      "3784/3784 [==============================] - 1s 226us/step - loss: 0.5367 - acc: 0.6533 - val_loss: 0.9340 - val_acc: 0.5368\n",
      "Epoch 515/1000\n",
      "3784/3784 [==============================] - 1s 227us/step - loss: 0.5423 - acc: 0.6541 - val_loss: 0.9047 - val_acc: 0.5392\n",
      "Epoch 516/1000\n",
      "3784/3784 [==============================] - 1s 225us/step - loss: 0.5384 - acc: 0.6488 - val_loss: 0.9177 - val_acc: 0.5249\n",
      "Epoch 517/1000\n",
      "3784/3784 [==============================] - 1s 226us/step - loss: 0.5371 - acc: 0.6580 - val_loss: 0.9214 - val_acc: 0.5416\n",
      "Epoch 518/1000\n",
      "3784/3784 [==============================] - 1s 226us/step - loss: 0.5224 - acc: 0.6652 - val_loss: 0.9386 - val_acc: 0.5392\n",
      "Epoch 519/1000\n",
      "3784/3784 [==============================] - 1s 225us/step - loss: 0.5316 - acc: 0.6533 - val_loss: 0.9343 - val_acc: 0.5368\n",
      "Epoch 520/1000\n",
      "3784/3784 [==============================] - 1s 226us/step - loss: 0.5419 - acc: 0.6533 - val_loss: 0.9344 - val_acc: 0.5392\n",
      "Epoch 521/1000\n",
      "3784/3784 [==============================] - 1s 225us/step - loss: 0.5357 - acc: 0.6498 - val_loss: 0.9486 - val_acc: 0.5297\n",
      "Epoch 522/1000\n",
      "3784/3784 [==============================] - 1s 225us/step - loss: 0.5314 - acc: 0.6596 - val_loss: 0.9418 - val_acc: 0.5368\n",
      "Epoch 523/1000\n",
      "3784/3784 [==============================] - 1s 226us/step - loss: 0.5332 - acc: 0.6546 - val_loss: 0.9466 - val_acc: 0.5321\n",
      "Epoch 524/1000\n",
      "3784/3784 [==============================] - 1s 226us/step - loss: 0.5316 - acc: 0.6628 - val_loss: 0.9423 - val_acc: 0.5297\n",
      "Epoch 525/1000\n",
      "3784/3784 [==============================] - 1s 226us/step - loss: 0.5284 - acc: 0.6609 - val_loss: 0.9428 - val_acc: 0.5344\n",
      "Epoch 526/1000\n",
      "3784/3784 [==============================] - 1s 227us/step - loss: 0.5376 - acc: 0.6522 - val_loss: 0.9432 - val_acc: 0.5226\n",
      "Epoch 527/1000\n",
      "3784/3784 [==============================] - 1s 224us/step - loss: 0.5310 - acc: 0.6578 - val_loss: 0.9406 - val_acc: 0.5249\n",
      "Epoch 528/1000\n",
      "3784/3784 [==============================] - 1s 226us/step - loss: 0.5388 - acc: 0.6578 - val_loss: 0.9228 - val_acc: 0.5297\n",
      "Epoch 529/1000\n",
      "3784/3784 [==============================] - 1s 227us/step - loss: 0.5358 - acc: 0.6557 - val_loss: 0.9343 - val_acc: 0.5321\n",
      "Epoch 530/1000\n",
      "3784/3784 [==============================] - 1s 224us/step - loss: 0.5371 - acc: 0.6546 - val_loss: 0.9436 - val_acc: 0.5344\n",
      "Epoch 531/1000\n",
      "3784/3784 [==============================] - 1s 227us/step - loss: 0.5317 - acc: 0.6620 - val_loss: 0.9396 - val_acc: 0.5344\n",
      "Epoch 532/1000\n",
      "3784/3784 [==============================] - 1s 225us/step - loss: 0.5290 - acc: 0.6641 - val_loss: 0.9411 - val_acc: 0.5392\n",
      "Epoch 533/1000\n",
      "3784/3784 [==============================] - 1s 225us/step - loss: 0.5284 - acc: 0.6657 - val_loss: 0.9670 - val_acc: 0.5416\n",
      "Epoch 534/1000\n",
      "3784/3784 [==============================] - 1s 225us/step - loss: 0.5290 - acc: 0.6654 - val_loss: 0.9770 - val_acc: 0.5368\n",
      "Epoch 535/1000\n",
      "3784/3784 [==============================] - 1s 225us/step - loss: 0.5291 - acc: 0.6668 - val_loss: 0.9671 - val_acc: 0.5344\n",
      "Epoch 536/1000\n",
      "3784/3784 [==============================] - 1s 225us/step - loss: 0.5321 - acc: 0.6607 - val_loss: 0.9501 - val_acc: 0.5416\n",
      "Epoch 537/1000\n",
      "3784/3784 [==============================] - 1s 226us/step - loss: 0.5236 - acc: 0.6705 - val_loss: 0.9604 - val_acc: 0.5368\n",
      "Epoch 538/1000\n",
      "3784/3784 [==============================] - 1s 232us/step - loss: 0.5283 - acc: 0.6646 - val_loss: 0.9648 - val_acc: 0.5439\n",
      "Epoch 539/1000\n",
      "3784/3784 [==============================] - 1s 226us/step - loss: 0.5254 - acc: 0.6683 - val_loss: 0.9584 - val_acc: 0.5416\n",
      "Epoch 540/1000\n",
      "3784/3784 [==============================] - 1s 226us/step - loss: 0.5290 - acc: 0.6641 - val_loss: 0.9591 - val_acc: 0.5416\n",
      "Epoch 541/1000\n",
      "3784/3784 [==============================] - 1s 224us/step - loss: 0.5182 - acc: 0.6757 - val_loss: 0.9798 - val_acc: 0.5534\n",
      "Epoch 542/1000\n",
      "3784/3784 [==============================] - 1s 225us/step - loss: 0.5387 - acc: 0.6572 - val_loss: 0.9587 - val_acc: 0.5463\n",
      "Epoch 543/1000\n",
      "3784/3784 [==============================] - 1s 227us/step - loss: 0.5338 - acc: 0.6538 - val_loss: 0.9653 - val_acc: 0.5463\n",
      "Epoch 544/1000\n",
      "3784/3784 [==============================] - 1s 226us/step - loss: 0.5295 - acc: 0.6557 - val_loss: 0.9549 - val_acc: 0.5511\n",
      "Epoch 545/1000\n",
      "3784/3784 [==============================] - 1s 225us/step - loss: 0.5234 - acc: 0.6633 - val_loss: 0.9481 - val_acc: 0.5487\n",
      "Epoch 546/1000\n",
      "3784/3784 [==============================] - 1s 224us/step - loss: 0.5339 - acc: 0.6596 - val_loss: 0.9509 - val_acc: 0.5439\n",
      "Epoch 547/1000\n",
      "3784/3784 [==============================] - 1s 232us/step - loss: 0.5359 - acc: 0.6564 - val_loss: 0.9503 - val_acc: 0.5392\n",
      "Epoch 548/1000\n",
      "3784/3784 [==============================] - 1s 226us/step - loss: 0.5255 - acc: 0.6562 - val_loss: 0.9620 - val_acc: 0.5487\n",
      "Epoch 549/1000\n",
      "3784/3784 [==============================] - 1s 226us/step - loss: 0.5320 - acc: 0.6596 - val_loss: 0.9478 - val_acc: 0.5273\n",
      "Epoch 550/1000\n",
      "3784/3784 [==============================] - 1s 226us/step - loss: 0.5216 - acc: 0.6638 - val_loss: 0.9601 - val_acc: 0.5439\n",
      "Epoch 551/1000\n",
      "3784/3784 [==============================] - 1s 225us/step - loss: 0.5249 - acc: 0.6657 - val_loss: 0.9666 - val_acc: 0.5368\n",
      "Epoch 552/1000\n",
      "3784/3784 [==============================] - 1s 225us/step - loss: 0.5350 - acc: 0.6535 - val_loss: 0.9699 - val_acc: 0.5344\n",
      "Epoch 553/1000\n",
      "3784/3784 [==============================] - 1s 226us/step - loss: 0.5170 - acc: 0.6662 - val_loss: 0.9620 - val_acc: 0.5416\n",
      "Epoch 554/1000\n",
      "3784/3784 [==============================] - 1s 226us/step - loss: 0.5356 - acc: 0.6575 - val_loss: 0.9578 - val_acc: 0.5344\n",
      "Epoch 555/1000\n",
      "3784/3784 [==============================] - 1s 224us/step - loss: 0.5279 - acc: 0.6594 - val_loss: 0.9685 - val_acc: 0.5368\n",
      "Epoch 556/1000\n",
      "3784/3784 [==============================] - 1s 225us/step - loss: 0.5249 - acc: 0.6599 - val_loss: 0.9774 - val_acc: 0.5392\n",
      "Epoch 557/1000\n",
      "3784/3784 [==============================] - 1s 225us/step - loss: 0.5274 - acc: 0.6617 - val_loss: 0.9915 - val_acc: 0.5368\n",
      "Epoch 558/1000\n",
      "3784/3784 [==============================] - 1s 226us/step - loss: 0.5282 - acc: 0.6631 - val_loss: 0.9803 - val_acc: 0.5416\n",
      "Epoch 559/1000\n",
      "3784/3784 [==============================] - 1s 226us/step - loss: 0.5238 - acc: 0.6628 - val_loss: 0.9885 - val_acc: 0.5368\n",
      "Epoch 560/1000\n",
      "3784/3784 [==============================] - 1s 227us/step - loss: 0.5222 - acc: 0.6644 - val_loss: 0.9796 - val_acc: 0.5439\n",
      "Epoch 561/1000\n",
      "3784/3784 [==============================] - 1s 225us/step - loss: 0.5237 - acc: 0.6628 - val_loss: 0.9694 - val_acc: 0.5487\n",
      "Epoch 562/1000\n",
      "3784/3784 [==============================] - 1s 230us/step - loss: 0.5208 - acc: 0.6641 - val_loss: 0.9861 - val_acc: 0.5463\n",
      "Epoch 563/1000\n",
      "3784/3784 [==============================] - 1s 227us/step - loss: 0.5315 - acc: 0.6617 - val_loss: 0.9575 - val_acc: 0.5392\n",
      "Epoch 564/1000\n",
      "3784/3784 [==============================] - 1s 226us/step - loss: 0.5274 - acc: 0.6594 - val_loss: 0.9670 - val_acc: 0.5368\n",
      "Epoch 565/1000\n",
      "3784/3784 [==============================] - 1s 232us/step - loss: 0.5208 - acc: 0.6607 - val_loss: 0.9792 - val_acc: 0.5226\n",
      "Epoch 566/1000\n",
      "3784/3784 [==============================] - 1s 225us/step - loss: 0.5289 - acc: 0.6575 - val_loss: 0.9672 - val_acc: 0.5368\n",
      "Epoch 567/1000\n",
      "3784/3784 [==============================] - 1s 230us/step - loss: 0.5277 - acc: 0.6660 - val_loss: 0.9751 - val_acc: 0.5392\n",
      "Epoch 568/1000\n",
      "3784/3784 [==============================] - 1s 225us/step - loss: 0.5283 - acc: 0.6641 - val_loss: 0.9670 - val_acc: 0.5344\n",
      "Epoch 569/1000\n",
      "3784/3784 [==============================] - 1s 228us/step - loss: 0.5207 - acc: 0.6641 - val_loss: 0.9636 - val_acc: 0.5368\n",
      "Epoch 570/1000\n",
      "3784/3784 [==============================] - 1s 226us/step - loss: 0.5287 - acc: 0.6638 - val_loss: 0.9832 - val_acc: 0.5297\n",
      "Epoch 571/1000\n",
      "3784/3784 [==============================] - 1s 225us/step - loss: 0.5279 - acc: 0.6678 - val_loss: 0.9775 - val_acc: 0.5321\n",
      "Epoch 572/1000\n",
      "3784/3784 [==============================] - 1s 226us/step - loss: 0.5177 - acc: 0.6739 - val_loss: 0.9759 - val_acc: 0.5321\n",
      "Epoch 573/1000\n",
      "3784/3784 [==============================] - 1s 227us/step - loss: 0.5179 - acc: 0.6628 - val_loss: 0.9872 - val_acc: 0.5249\n",
      "Epoch 574/1000\n",
      "3784/3784 [==============================] - 1s 234us/step - loss: 0.5169 - acc: 0.6739 - val_loss: 0.9876 - val_acc: 0.5368\n",
      "Epoch 575/1000\n",
      "3784/3784 [==============================] - 1s 226us/step - loss: 0.5232 - acc: 0.6652 - val_loss: 0.9881 - val_acc: 0.5249\n",
      "Epoch 576/1000\n",
      "3784/3784 [==============================] - 1s 227us/step - loss: 0.5228 - acc: 0.6620 - val_loss: 0.9656 - val_acc: 0.5321\n",
      "Epoch 577/1000\n",
      "3784/3784 [==============================] - 1s 226us/step - loss: 0.5182 - acc: 0.6673 - val_loss: 0.9736 - val_acc: 0.5416\n",
      "Epoch 578/1000\n",
      "3784/3784 [==============================] - 1s 225us/step - loss: 0.5255 - acc: 0.6617 - val_loss: 0.9703 - val_acc: 0.5439\n",
      "Epoch 579/1000\n",
      "3784/3784 [==============================] - 1s 227us/step - loss: 0.5156 - acc: 0.6747 - val_loss: 0.9754 - val_acc: 0.5487\n",
      "Epoch 580/1000\n",
      "3784/3784 [==============================] - 1s 225us/step - loss: 0.5156 - acc: 0.6755 - val_loss: 0.9698 - val_acc: 0.5392\n",
      "Epoch 581/1000\n",
      "3784/3784 [==============================] - 1s 229us/step - loss: 0.5256 - acc: 0.6673 - val_loss: 0.9623 - val_acc: 0.5487\n",
      "Epoch 582/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3784/3784 [==============================] - 1s 225us/step - loss: 0.5225 - acc: 0.6625 - val_loss: 0.9702 - val_acc: 0.5534\n",
      "Epoch 583/1000\n",
      "3784/3784 [==============================] - 1s 227us/step - loss: 0.5157 - acc: 0.6657 - val_loss: 0.9855 - val_acc: 0.5487\n",
      "Epoch 584/1000\n",
      "3784/3784 [==============================] - 1s 224us/step - loss: 0.5213 - acc: 0.6718 - val_loss: 0.9869 - val_acc: 0.5392\n",
      "Epoch 585/1000\n",
      "3784/3784 [==============================] - 1s 224us/step - loss: 0.5132 - acc: 0.6784 - val_loss: 0.9898 - val_acc: 0.5439\n",
      "Epoch 586/1000\n",
      "3784/3784 [==============================] - 1s 227us/step - loss: 0.5176 - acc: 0.6691 - val_loss: 0.9961 - val_acc: 0.5463\n",
      "Epoch 587/1000\n",
      "3784/3784 [==============================] - 1s 225us/step - loss: 0.5253 - acc: 0.6652 - val_loss: 0.9608 - val_acc: 0.5439\n",
      "Epoch 588/1000\n",
      "3784/3784 [==============================] - 1s 226us/step - loss: 0.5234 - acc: 0.6670 - val_loss: 0.9639 - val_acc: 0.5226\n",
      "Epoch 589/1000\n",
      "3784/3784 [==============================] - 1s 226us/step - loss: 0.5248 - acc: 0.6617 - val_loss: 0.9750 - val_acc: 0.5368\n",
      "Epoch 590/1000\n",
      "3784/3784 [==============================] - 1s 226us/step - loss: 0.5175 - acc: 0.6665 - val_loss: 0.9750 - val_acc: 0.5582\n",
      "Epoch 591/1000\n",
      "3784/3784 [==============================] - 1s 233us/step - loss: 0.5204 - acc: 0.6652 - val_loss: 0.9950 - val_acc: 0.5439\n",
      "Epoch 592/1000\n",
      "3784/3784 [==============================] - 1s 226us/step - loss: 0.5194 - acc: 0.6633 - val_loss: 0.9933 - val_acc: 0.5511\n",
      "Epoch 593/1000\n",
      "3784/3784 [==============================] - 1s 226us/step - loss: 0.5166 - acc: 0.6771 - val_loss: 0.9984 - val_acc: 0.5439\n",
      "Epoch 594/1000\n",
      "3784/3784 [==============================] - 1s 226us/step - loss: 0.5189 - acc: 0.6728 - val_loss: 0.9910 - val_acc: 0.5297\n",
      "Epoch 595/1000\n",
      "3784/3784 [==============================] - 1s 226us/step - loss: 0.5095 - acc: 0.6715 - val_loss: 0.9924 - val_acc: 0.5297\n",
      "Epoch 596/1000\n",
      "3784/3784 [==============================] - 1s 225us/step - loss: 0.5146 - acc: 0.6757 - val_loss: 0.9888 - val_acc: 0.5416\n",
      "Epoch 597/1000\n",
      "3784/3784 [==============================] - 1s 226us/step - loss: 0.5250 - acc: 0.6636 - val_loss: 0.9773 - val_acc: 0.5344\n",
      "Epoch 598/1000\n",
      "3784/3784 [==============================] - 1s 226us/step - loss: 0.5182 - acc: 0.6628 - val_loss: 1.0000 - val_acc: 0.5368\n",
      "Epoch 599/1000\n",
      "3784/3784 [==============================] - 1s 224us/step - loss: 0.5118 - acc: 0.6731 - val_loss: 0.9946 - val_acc: 0.5463\n",
      "Epoch 600/1000\n",
      "3784/3784 [==============================] - 1s 225us/step - loss: 0.5198 - acc: 0.6612 - val_loss: 0.9869 - val_acc: 0.5392\n",
      "Epoch 601/1000\n",
      "3784/3784 [==============================] - 1s 226us/step - loss: 0.5192 - acc: 0.6683 - val_loss: 0.9866 - val_acc: 0.5416\n",
      "Epoch 602/1000\n",
      "3784/3784 [==============================] - 1s 226us/step - loss: 0.5226 - acc: 0.6668 - val_loss: 0.9935 - val_acc: 0.5344\n",
      "Epoch 603/1000\n",
      "3784/3784 [==============================] - 1s 226us/step - loss: 0.5080 - acc: 0.6752 - val_loss: 0.9917 - val_acc: 0.5297\n",
      "Epoch 604/1000\n",
      "3784/3784 [==============================] - 1s 228us/step - loss: 0.5134 - acc: 0.6784 - val_loss: 0.9996 - val_acc: 0.5368\n",
      "Epoch 605/1000\n",
      "3784/3784 [==============================] - 1s 226us/step - loss: 0.5188 - acc: 0.6623 - val_loss: 1.0001 - val_acc: 0.5368\n",
      "Epoch 606/1000\n",
      "3784/3784 [==============================] - 1s 226us/step - loss: 0.5119 - acc: 0.6765 - val_loss: 1.0097 - val_acc: 0.5439\n",
      "Epoch 607/1000\n",
      "3784/3784 [==============================] - 1s 226us/step - loss: 0.5141 - acc: 0.6712 - val_loss: 1.0254 - val_acc: 0.5368\n",
      "Epoch 608/1000\n",
      "3784/3784 [==============================] - 1s 226us/step - loss: 0.5186 - acc: 0.6670 - val_loss: 1.0022 - val_acc: 0.5368\n",
      "Epoch 609/1000\n",
      "3784/3784 [==============================] - 1s 226us/step - loss: 0.5135 - acc: 0.6710 - val_loss: 1.0126 - val_acc: 0.5344\n",
      "Epoch 610/1000\n",
      "3784/3784 [==============================] - 1s 226us/step - loss: 0.5210 - acc: 0.6686 - val_loss: 0.9962 - val_acc: 0.5416\n",
      "Epoch 611/1000\n",
      "3784/3784 [==============================] - 1s 228us/step - loss: 0.5187 - acc: 0.6681 - val_loss: 1.0033 - val_acc: 0.5368\n",
      "Epoch 612/1000\n",
      "3784/3784 [==============================] - 1s 226us/step - loss: 0.5230 - acc: 0.6689 - val_loss: 0.9862 - val_acc: 0.5368\n",
      "Epoch 613/1000\n",
      "3784/3784 [==============================] - 1s 224us/step - loss: 0.5163 - acc: 0.6720 - val_loss: 0.9892 - val_acc: 0.5368\n",
      "Epoch 614/1000\n",
      "3784/3784 [==============================] - 1s 226us/step - loss: 0.5123 - acc: 0.6705 - val_loss: 0.9956 - val_acc: 0.5344\n",
      "Epoch 615/1000\n",
      "3784/3784 [==============================] - 1s 226us/step - loss: 0.5185 - acc: 0.6691 - val_loss: 1.0040 - val_acc: 0.5392\n",
      "Epoch 616/1000\n",
      "3784/3784 [==============================] - 1s 227us/step - loss: 0.5158 - acc: 0.6628 - val_loss: 1.0040 - val_acc: 0.5392\n",
      "Epoch 617/1000\n",
      "3784/3784 [==============================] - 1s 229us/step - loss: 0.5071 - acc: 0.6678 - val_loss: 1.0133 - val_acc: 0.5392\n",
      "Epoch 618/1000\n",
      "3784/3784 [==============================] - 1s 231us/step - loss: 0.5223 - acc: 0.6644 - val_loss: 1.0023 - val_acc: 0.5273\n",
      "Epoch 619/1000\n",
      "3784/3784 [==============================] - 1s 224us/step - loss: 0.5122 - acc: 0.6654 - val_loss: 1.0101 - val_acc: 0.5368\n",
      "Epoch 620/1000\n",
      "3784/3784 [==============================] - 1s 225us/step - loss: 0.5107 - acc: 0.6689 - val_loss: 0.9983 - val_acc: 0.5321\n",
      "Epoch 621/1000\n",
      "3784/3784 [==============================] - 1s 230us/step - loss: 0.5049 - acc: 0.6839 - val_loss: 1.0196 - val_acc: 0.5321\n",
      "Epoch 622/1000\n",
      "3784/3784 [==============================] - 1s 234us/step - loss: 0.5131 - acc: 0.6752 - val_loss: 1.0188 - val_acc: 0.5273\n",
      "Epoch 623/1000\n",
      "3784/3784 [==============================] - 1s 233us/step - loss: 0.5127 - acc: 0.6707 - val_loss: 1.0381 - val_acc: 0.5416\n",
      "Epoch 624/1000\n",
      "3784/3784 [==============================] - 1s 233us/step - loss: 0.5190 - acc: 0.6652 - val_loss: 1.0062 - val_acc: 0.5368\n",
      "Epoch 625/1000\n",
      "3784/3784 [==============================] - 1s 233us/step - loss: 0.5131 - acc: 0.6771 - val_loss: 1.0233 - val_acc: 0.5178\n",
      "Epoch 626/1000\n",
      "3784/3784 [==============================] - 1s 230us/step - loss: 0.5113 - acc: 0.6686 - val_loss: 1.0177 - val_acc: 0.5463\n",
      "Epoch 627/1000\n",
      "3784/3784 [==============================] - 1s 224us/step - loss: 0.5105 - acc: 0.6771 - val_loss: 1.0045 - val_acc: 0.5297\n",
      "Epoch 628/1000\n",
      "3784/3784 [==============================] - 1s 226us/step - loss: 0.5176 - acc: 0.6686 - val_loss: 1.0151 - val_acc: 0.5321\n",
      "Epoch 629/1000\n",
      "3784/3784 [==============================] - 1s 226us/step - loss: 0.5143 - acc: 0.6747 - val_loss: 1.0173 - val_acc: 0.5273\n",
      "Epoch 630/1000\n",
      "3784/3784 [==============================] - 1s 226us/step - loss: 0.5117 - acc: 0.6726 - val_loss: 1.0257 - val_acc: 0.5321\n",
      "Epoch 631/1000\n",
      "3784/3784 [==============================] - 1s 226us/step - loss: 0.5036 - acc: 0.6768 - val_loss: 1.0262 - val_acc: 0.5249\n",
      "Epoch 632/1000\n",
      "3784/3784 [==============================] - 1s 227us/step - loss: 0.5125 - acc: 0.6768 - val_loss: 1.0156 - val_acc: 0.5416\n",
      "Epoch 633/1000\n",
      "3784/3784 [==============================] - 1s 228us/step - loss: 0.5091 - acc: 0.6784 - val_loss: 1.0005 - val_acc: 0.5368\n",
      "Epoch 634/1000\n",
      "3784/3784 [==============================] - 1s 228us/step - loss: 0.5113 - acc: 0.6694 - val_loss: 1.0221 - val_acc: 0.5392\n",
      "Epoch 635/1000\n",
      "3784/3784 [==============================] - 1s 226us/step - loss: 0.5113 - acc: 0.6736 - val_loss: 1.0113 - val_acc: 0.5392\n",
      "Epoch 636/1000\n",
      "3784/3784 [==============================] - 1s 227us/step - loss: 0.5122 - acc: 0.6720 - val_loss: 1.0016 - val_acc: 0.5297\n",
      "Epoch 637/1000\n",
      "3784/3784 [==============================] - 1s 228us/step - loss: 0.5078 - acc: 0.6779 - val_loss: 1.0103 - val_acc: 0.5297\n",
      "Epoch 638/1000\n",
      "3784/3784 [==============================] - 1s 227us/step - loss: 0.5110 - acc: 0.6760 - val_loss: 1.0204 - val_acc: 0.5439\n",
      "Epoch 639/1000\n",
      "3784/3784 [==============================] - 1s 226us/step - loss: 0.5176 - acc: 0.6731 - val_loss: 0.9960 - val_acc: 0.5273\n",
      "Epoch 640/1000\n",
      "3784/3784 [==============================] - 1s 225us/step - loss: 0.5103 - acc: 0.6736 - val_loss: 1.0175 - val_acc: 0.5249\n",
      "Epoch 641/1000\n",
      "3784/3784 [==============================] - 1s 226us/step - loss: 0.5098 - acc: 0.6686 - val_loss: 1.0120 - val_acc: 0.5202\n",
      "Epoch 642/1000\n",
      "3784/3784 [==============================] - 1s 226us/step - loss: 0.5094 - acc: 0.6739 - val_loss: 1.0257 - val_acc: 0.5344\n",
      "Epoch 643/1000\n",
      "3784/3784 [==============================] - 1s 225us/step - loss: 0.5184 - acc: 0.6715 - val_loss: 1.0050 - val_acc: 0.5249\n",
      "Epoch 644/1000\n",
      "3784/3784 [==============================] - 1s 226us/step - loss: 0.5054 - acc: 0.6789 - val_loss: 1.0298 - val_acc: 0.5344\n",
      "Epoch 645/1000\n",
      "3784/3784 [==============================] - 1s 235us/step - loss: 0.5138 - acc: 0.6686 - val_loss: 1.0133 - val_acc: 0.5321\n",
      "Epoch 646/1000\n",
      "3784/3784 [==============================] - 1s 225us/step - loss: 0.5051 - acc: 0.6718 - val_loss: 1.0236 - val_acc: 0.5368\n",
      "Epoch 647/1000\n",
      "3784/3784 [==============================] - 1s 227us/step - loss: 0.5111 - acc: 0.6697 - val_loss: 1.0062 - val_acc: 0.5321\n",
      "Epoch 648/1000\n",
      "3784/3784 [==============================] - 1s 225us/step - loss: 0.5149 - acc: 0.6763 - val_loss: 1.0000 - val_acc: 0.5273\n",
      "Epoch 649/1000\n",
      "3784/3784 [==============================] - 1s 227us/step - loss: 0.5076 - acc: 0.6736 - val_loss: 1.0241 - val_acc: 0.5321\n",
      "Epoch 650/1000\n",
      "3784/3784 [==============================] - 1s 225us/step - loss: 0.5137 - acc: 0.6691 - val_loss: 1.0019 - val_acc: 0.5273\n",
      "Epoch 651/1000\n",
      "3784/3784 [==============================] - 1s 224us/step - loss: 0.5055 - acc: 0.6845 - val_loss: 1.0082 - val_acc: 0.5297\n",
      "Epoch 652/1000\n",
      "3784/3784 [==============================] - 1s 226us/step - loss: 0.5019 - acc: 0.6850 - val_loss: 1.0108 - val_acc: 0.5297\n",
      "Epoch 653/1000\n",
      "3784/3784 [==============================] - 1s 226us/step - loss: 0.5108 - acc: 0.6757 - val_loss: 1.0175 - val_acc: 0.5368\n",
      "Epoch 654/1000\n",
      "3784/3784 [==============================] - 1s 225us/step - loss: 0.5056 - acc: 0.6720 - val_loss: 1.0268 - val_acc: 0.5368\n",
      "Epoch 655/1000\n",
      "3784/3784 [==============================] - 1s 225us/step - loss: 0.5085 - acc: 0.6797 - val_loss: 1.0129 - val_acc: 0.5344\n",
      "Epoch 656/1000\n",
      "3784/3784 [==============================] - 1s 225us/step - loss: 0.5076 - acc: 0.6779 - val_loss: 1.0374 - val_acc: 0.5344\n",
      "Epoch 657/1000\n",
      "3784/3784 [==============================] - 1s 227us/step - loss: 0.5075 - acc: 0.6749 - val_loss: 1.0290 - val_acc: 0.5344\n",
      "Epoch 658/1000\n",
      "3784/3784 [==============================] - 1s 227us/step - loss: 0.5031 - acc: 0.6747 - val_loss: 1.0478 - val_acc: 0.5273\n",
      "Epoch 659/1000\n",
      "3784/3784 [==============================] - 1s 226us/step - loss: 0.5035 - acc: 0.6749 - val_loss: 1.0335 - val_acc: 0.5154\n",
      "Epoch 660/1000\n",
      "3784/3784 [==============================] - 1s 225us/step - loss: 0.5140 - acc: 0.6715 - val_loss: 1.0191 - val_acc: 0.5368\n",
      "Epoch 661/1000\n",
      "3784/3784 [==============================] - 1s 228us/step - loss: 0.5090 - acc: 0.6810 - val_loss: 1.0229 - val_acc: 0.5297\n",
      "Epoch 662/1000\n",
      "3784/3784 [==============================] - ETA: 0s - loss: 0.5093 - acc: 0.671 - 1s 226us/step - loss: 0.5094 - acc: 0.6720 - val_loss: 1.0183 - val_acc: 0.5344\n",
      "Epoch 663/1000\n",
      "3784/3784 [==============================] - 1s 228us/step - loss: 0.5071 - acc: 0.6789 - val_loss: 1.0326 - val_acc: 0.5344\n",
      "Epoch 664/1000\n",
      "3784/3784 [==============================] - 1s 226us/step - loss: 0.5017 - acc: 0.6808 - val_loss: 1.0332 - val_acc: 0.5392\n",
      "Epoch 665/1000\n",
      "3784/3784 [==============================] - 1s 225us/step - loss: 0.5031 - acc: 0.6734 - val_loss: 1.0423 - val_acc: 0.5321\n",
      "Epoch 666/1000\n",
      "3784/3784 [==============================] - 1s 228us/step - loss: 0.5051 - acc: 0.6781 - val_loss: 1.0331 - val_acc: 0.5178\n",
      "Epoch 667/1000\n",
      "3784/3784 [==============================] - 1s 226us/step - loss: 0.5139 - acc: 0.6665 - val_loss: 0.9964 - val_acc: 0.5344\n",
      "Epoch 668/1000\n",
      "3784/3784 [==============================] - 1s 226us/step - loss: 0.5045 - acc: 0.6763 - val_loss: 1.0262 - val_acc: 0.5344\n",
      "Epoch 669/1000\n",
      "3784/3784 [==============================] - 1s 227us/step - loss: 0.5020 - acc: 0.6810 - val_loss: 1.0346 - val_acc: 0.5249\n",
      "Epoch 670/1000\n",
      "3784/3784 [==============================] - 1s 226us/step - loss: 0.4997 - acc: 0.6800 - val_loss: 1.0487 - val_acc: 0.5368\n",
      "Epoch 671/1000\n",
      "3784/3784 [==============================] - 1s 225us/step - loss: 0.4978 - acc: 0.6805 - val_loss: 1.0632 - val_acc: 0.5321\n",
      "Epoch 672/1000\n",
      "3784/3784 [==============================] - 1s 232us/step - loss: 0.4973 - acc: 0.6821 - val_loss: 1.0584 - val_acc: 0.5273\n",
      "Epoch 673/1000\n",
      "3784/3784 [==============================] - 1s 227us/step - loss: 0.4983 - acc: 0.6810 - val_loss: 1.0613 - val_acc: 0.5439\n",
      "Epoch 674/1000\n",
      "3784/3784 [==============================] - 1s 225us/step - loss: 0.5007 - acc: 0.6839 - val_loss: 1.0442 - val_acc: 0.5344\n",
      "Epoch 675/1000\n",
      "3784/3784 [==============================] - 1s 227us/step - loss: 0.5139 - acc: 0.6731 - val_loss: 1.0305 - val_acc: 0.5321\n",
      "Epoch 676/1000\n",
      "3784/3784 [==============================] - 1s 227us/step - loss: 0.5041 - acc: 0.6773 - val_loss: 1.0399 - val_acc: 0.5249\n",
      "Epoch 677/1000\n",
      "3784/3784 [==============================] - 1s 226us/step - loss: 0.4983 - acc: 0.6744 - val_loss: 1.0395 - val_acc: 0.5249\n",
      "Epoch 678/1000\n",
      "3784/3784 [==============================] - 1s 226us/step - loss: 0.5053 - acc: 0.6720 - val_loss: 1.0081 - val_acc: 0.5249\n",
      "Epoch 679/1000\n",
      "3784/3784 [==============================] - 1s 224us/step - loss: 0.5014 - acc: 0.6816 - val_loss: 1.0088 - val_acc: 0.5297\n",
      "Epoch 680/1000\n",
      "3784/3784 [==============================] - 1s 226us/step - loss: 0.4994 - acc: 0.6834 - val_loss: 1.0407 - val_acc: 0.5368\n",
      "Epoch 681/1000\n",
      "3784/3784 [==============================] - 1s 226us/step - loss: 0.4961 - acc: 0.6789 - val_loss: 1.0493 - val_acc: 0.5297\n",
      "Epoch 682/1000\n",
      "3784/3784 [==============================] - 1s 225us/step - loss: 0.5008 - acc: 0.6792 - val_loss: 1.0359 - val_acc: 0.5202\n",
      "Epoch 683/1000\n",
      "3784/3784 [==============================] - 1s 227us/step - loss: 0.5041 - acc: 0.6776 - val_loss: 1.0188 - val_acc: 0.5297\n",
      "Epoch 684/1000\n",
      "3784/3784 [==============================] - 1s 227us/step - loss: 0.5014 - acc: 0.6760 - val_loss: 1.0415 - val_acc: 0.5321\n",
      "Epoch 685/1000\n",
      "3784/3784 [==============================] - 1s 226us/step - loss: 0.4988 - acc: 0.6768 - val_loss: 1.0422 - val_acc: 0.5321\n",
      "Epoch 686/1000\n",
      "3784/3784 [==============================] - 1s 226us/step - loss: 0.4929 - acc: 0.6786 - val_loss: 1.0321 - val_acc: 0.5439\n",
      "Epoch 687/1000\n",
      "3784/3784 [==============================] - 1s 227us/step - loss: 0.4964 - acc: 0.6831 - val_loss: 1.0356 - val_acc: 0.5297\n",
      "Epoch 688/1000\n",
      "3784/3784 [==============================] - 1s 225us/step - loss: 0.5039 - acc: 0.6747 - val_loss: 1.0550 - val_acc: 0.5273\n",
      "Epoch 689/1000\n",
      "3784/3784 [==============================] - 1s 227us/step - loss: 0.4917 - acc: 0.6823 - val_loss: 1.0810 - val_acc: 0.5344\n",
      "Epoch 690/1000\n",
      "3784/3784 [==============================] - 1s 227us/step - loss: 0.4941 - acc: 0.6829 - val_loss: 1.0407 - val_acc: 0.5321\n",
      "Epoch 691/1000\n",
      "3784/3784 [==============================] - 1s 225us/step - loss: 0.4941 - acc: 0.6808 - val_loss: 1.0807 - val_acc: 0.5154\n",
      "Epoch 692/1000\n",
      "3784/3784 [==============================] - 1s 226us/step - loss: 0.4905 - acc: 0.6895 - val_loss: 1.0604 - val_acc: 0.5368\n",
      "Epoch 693/1000\n",
      "3784/3784 [==============================] - 1s 226us/step - loss: 0.4953 - acc: 0.6792 - val_loss: 1.0413 - val_acc: 0.5273\n",
      "Epoch 694/1000\n",
      "3784/3784 [==============================] - 1s 225us/step - loss: 0.5072 - acc: 0.6760 - val_loss: 1.0541 - val_acc: 0.5321\n",
      "Epoch 695/1000\n",
      "3784/3784 [==============================] - 1s 227us/step - loss: 0.5000 - acc: 0.6863 - val_loss: 1.0542 - val_acc: 0.5321\n",
      "Epoch 696/1000\n",
      "3784/3784 [==============================] - 1s 225us/step - loss: 0.4838 - acc: 0.6913 - val_loss: 1.0513 - val_acc: 0.5273\n",
      "Epoch 697/1000\n",
      "3784/3784 [==============================] - 1s 225us/step - loss: 0.5004 - acc: 0.6794 - val_loss: 1.0562 - val_acc: 0.5297\n",
      "Epoch 698/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3784/3784 [==============================] - 1s 229us/step - loss: 0.4922 - acc: 0.6834 - val_loss: 1.0610 - val_acc: 0.5416\n",
      "Epoch 699/1000\n",
      "3784/3784 [==============================] - 1s 234us/step - loss: 0.4941 - acc: 0.6818 - val_loss: 1.0540 - val_acc: 0.5202\n",
      "Epoch 700/1000\n",
      "3784/3784 [==============================] - 1s 227us/step - loss: 0.4977 - acc: 0.6882 - val_loss: 1.0590 - val_acc: 0.5226\n",
      "Epoch 701/1000\n",
      "3784/3784 [==============================] - 1s 227us/step - loss: 0.5001 - acc: 0.6853 - val_loss: 1.0524 - val_acc: 0.5392\n",
      "Epoch 702/1000\n",
      "3784/3784 [==============================] - 1s 225us/step - loss: 0.4968 - acc: 0.6839 - val_loss: 1.0495 - val_acc: 0.5487\n",
      "Epoch 703/1000\n",
      "3784/3784 [==============================] - 1s 226us/step - loss: 0.4936 - acc: 0.6818 - val_loss: 1.0645 - val_acc: 0.5344\n",
      "Epoch 704/1000\n",
      "3784/3784 [==============================] - 1s 224us/step - loss: 0.5031 - acc: 0.6871 - val_loss: 1.0683 - val_acc: 0.5439\n",
      "Epoch 705/1000\n",
      "3784/3784 [==============================] - 1s 225us/step - loss: 0.4941 - acc: 0.6797 - val_loss: 1.0378 - val_acc: 0.5368\n",
      "Epoch 706/1000\n",
      "3784/3784 [==============================] - 1s 226us/step - loss: 0.4913 - acc: 0.6845 - val_loss: 1.0496 - val_acc: 0.5297\n",
      "Epoch 707/1000\n",
      "3784/3784 [==============================] - 1s 226us/step - loss: 0.4958 - acc: 0.6921 - val_loss: 1.0446 - val_acc: 0.5416\n",
      "Epoch 708/1000\n",
      "3784/3784 [==============================] - 1s 226us/step - loss: 0.4869 - acc: 0.6845 - val_loss: 1.0496 - val_acc: 0.5463\n",
      "Epoch 709/1000\n",
      "3784/3784 [==============================] - 1s 226us/step - loss: 0.4972 - acc: 0.6890 - val_loss: 1.0219 - val_acc: 0.5416\n",
      "Epoch 710/1000\n",
      "3784/3784 [==============================] - 1s 225us/step - loss: 0.4901 - acc: 0.6868 - val_loss: 1.0605 - val_acc: 0.5463\n",
      "Epoch 711/1000\n",
      "3784/3784 [==============================] - 1s 227us/step - loss: 0.4995 - acc: 0.6805 - val_loss: 1.0287 - val_acc: 0.5416\n",
      "Epoch 712/1000\n",
      "3784/3784 [==============================] - 1s 227us/step - loss: 0.4864 - acc: 0.6884 - val_loss: 1.0367 - val_acc: 0.5368\n",
      "Epoch 713/1000\n",
      "3784/3784 [==============================] - 1s 233us/step - loss: 0.4873 - acc: 0.6884 - val_loss: 1.0629 - val_acc: 0.5511\n",
      "Epoch 714/1000\n",
      "3784/3784 [==============================] - 1s 231us/step - loss: 0.4980 - acc: 0.6755 - val_loss: 1.0467 - val_acc: 0.5463\n",
      "Epoch 715/1000\n",
      "3784/3784 [==============================] - 1s 229us/step - loss: 0.4879 - acc: 0.6916 - val_loss: 1.0710 - val_acc: 0.5368\n",
      "Epoch 716/1000\n",
      "3784/3784 [==============================] - 1s 236us/step - loss: 0.4867 - acc: 0.6874 - val_loss: 1.0587 - val_acc: 0.5344\n",
      "Epoch 717/1000\n",
      "3784/3784 [==============================] - 1s 231us/step - loss: 0.4976 - acc: 0.6771 - val_loss: 1.0546 - val_acc: 0.5392\n",
      "Epoch 718/1000\n",
      "3784/3784 [==============================] - 1s 233us/step - loss: 0.4922 - acc: 0.6876 - val_loss: 1.0653 - val_acc: 0.5392\n",
      "Epoch 719/1000\n",
      "3784/3784 [==============================] - 1s 229us/step - loss: 0.4997 - acc: 0.6802 - val_loss: 1.0290 - val_acc: 0.5463\n",
      "Epoch 720/1000\n",
      "3784/3784 [==============================] - 1s 235us/step - loss: 0.4970 - acc: 0.6784 - val_loss: 1.0273 - val_acc: 0.5321\n",
      "Epoch 721/1000\n",
      "3784/3784 [==============================] - 1s 232us/step - loss: 0.4879 - acc: 0.6821 - val_loss: 1.0406 - val_acc: 0.5321\n",
      "Epoch 722/1000\n",
      "3784/3784 [==============================] - 1s 234us/step - loss: 0.4974 - acc: 0.6829 - val_loss: 1.0400 - val_acc: 0.5416\n",
      "Epoch 723/1000\n",
      "3784/3784 [==============================] - 1s 232us/step - loss: 0.4945 - acc: 0.6845 - val_loss: 1.0656 - val_acc: 0.5534\n",
      "Epoch 724/1000\n",
      "3784/3784 [==============================] - 1s 230us/step - loss: 0.4943 - acc: 0.6781 - val_loss: 1.0736 - val_acc: 0.5534\n",
      "Epoch 725/1000\n",
      "3784/3784 [==============================] - 1s 240us/step - loss: 0.4896 - acc: 0.6908 - val_loss: 1.0687 - val_acc: 0.5368\n",
      "Epoch 726/1000\n",
      "3784/3784 [==============================] - 1s 230us/step - loss: 0.4954 - acc: 0.6868 - val_loss: 1.0654 - val_acc: 0.5534\n",
      "Epoch 727/1000\n",
      "3784/3784 [==============================] - 1s 235us/step - loss: 0.4917 - acc: 0.6858 - val_loss: 1.0576 - val_acc: 0.5439\n",
      "Epoch 728/1000\n",
      "3784/3784 [==============================] - 1s 231us/step - loss: 0.4807 - acc: 0.6879 - val_loss: 1.0746 - val_acc: 0.5511\n",
      "Epoch 729/1000\n",
      "3784/3784 [==============================] - 1s 227us/step - loss: 0.4931 - acc: 0.6860 - val_loss: 1.0673 - val_acc: 0.5416\n",
      "Epoch 730/1000\n",
      "3784/3784 [==============================] - 1s 228us/step - loss: 0.4893 - acc: 0.6908 - val_loss: 1.0698 - val_acc: 0.5439\n",
      "Epoch 731/1000\n",
      "3784/3784 [==============================] - 1s 231us/step - loss: 0.4962 - acc: 0.6786 - val_loss: 1.0345 - val_acc: 0.5344\n",
      "Epoch 732/1000\n",
      "3784/3784 [==============================] - 1s 228us/step - loss: 0.4944 - acc: 0.6771 - val_loss: 1.0581 - val_acc: 0.5226\n",
      "Epoch 733/1000\n",
      "3784/3784 [==============================] - 1s 230us/step - loss: 0.4794 - acc: 0.6956 - val_loss: 1.0885 - val_acc: 0.5439\n",
      "Epoch 734/1000\n",
      "3784/3784 [==============================] - 1s 230us/step - loss: 0.4925 - acc: 0.6842 - val_loss: 1.0693 - val_acc: 0.5368\n",
      "Epoch 735/1000\n",
      "3784/3784 [==============================] - 1s 236us/step - loss: 0.4922 - acc: 0.6834 - val_loss: 1.0585 - val_acc: 0.5368\n",
      "Epoch 736/1000\n",
      "3784/3784 [==============================] - 1s 230us/step - loss: 0.4888 - acc: 0.6895 - val_loss: 1.0509 - val_acc: 0.5416\n",
      "Epoch 737/1000\n",
      "3784/3784 [==============================] - 1s 230us/step - loss: 0.4912 - acc: 0.6855 - val_loss: 1.0651 - val_acc: 0.5439\n",
      "Epoch 738/1000\n",
      "3784/3784 [==============================] - 1s 233us/step - loss: 0.4905 - acc: 0.6839 - val_loss: 1.0518 - val_acc: 0.5344\n",
      "Epoch 739/1000\n",
      "3784/3784 [==============================] - 1s 226us/step - loss: 0.4971 - acc: 0.6829 - val_loss: 1.0563 - val_acc: 0.5416\n",
      "Epoch 740/1000\n",
      "3784/3784 [==============================] - 1s 230us/step - loss: 0.4887 - acc: 0.6948 - val_loss: 1.0788 - val_acc: 0.5439\n",
      "Epoch 741/1000\n",
      "3784/3784 [==============================] - 1s 231us/step - loss: 0.4826 - acc: 0.6919 - val_loss: 1.0800 - val_acc: 0.5392\n",
      "Epoch 742/1000\n",
      "3784/3784 [==============================] - 1s 232us/step - loss: 0.4890 - acc: 0.6816 - val_loss: 1.0725 - val_acc: 0.5439\n",
      "Epoch 743/1000\n",
      "3784/3784 [==============================] - 1s 232us/step - loss: 0.4846 - acc: 0.6908 - val_loss: 1.0577 - val_acc: 0.5297\n",
      "Epoch 744/1000\n",
      "3784/3784 [==============================] - 1s 233us/step - loss: 0.4885 - acc: 0.6916 - val_loss: 1.0683 - val_acc: 0.5297\n",
      "Epoch 745/1000\n",
      "3784/3784 [==============================] - 1s 232us/step - loss: 0.4896 - acc: 0.6890 - val_loss: 1.0680 - val_acc: 0.5297\n",
      "Epoch 746/1000\n",
      "3784/3784 [==============================] - 1s 229us/step - loss: 0.4832 - acc: 0.6892 - val_loss: 1.0790 - val_acc: 0.5368\n",
      "Epoch 747/1000\n",
      "3784/3784 [==============================] - 1s 228us/step - loss: 0.4911 - acc: 0.6876 - val_loss: 1.0617 - val_acc: 0.5416\n",
      "Epoch 748/1000\n",
      "3784/3784 [==============================] - 1s 229us/step - loss: 0.4978 - acc: 0.6829 - val_loss: 1.0564 - val_acc: 0.5582\n",
      "Epoch 749/1000\n",
      "3784/3784 [==============================] - 1s 230us/step - loss: 0.4824 - acc: 0.6858 - val_loss: 1.0656 - val_acc: 0.5416\n",
      "Epoch 750/1000\n",
      "3784/3784 [==============================] - ETA: 0s - loss: 0.4760 - acc: 0.691 - 1s 234us/step - loss: 0.4782 - acc: 0.6908 - val_loss: 1.0846 - val_acc: 0.5439\n",
      "Epoch 751/1000\n",
      "3784/3784 [==============================] - 1s 231us/step - loss: 0.4942 - acc: 0.6800 - val_loss: 1.0727 - val_acc: 0.5392\n",
      "Epoch 752/1000\n",
      "3784/3784 [==============================] - 1s 235us/step - loss: 0.4931 - acc: 0.6829 - val_loss: 1.0482 - val_acc: 0.5321\n",
      "Epoch 753/1000\n",
      "3784/3784 [==============================] - 1s 233us/step - loss: 0.4912 - acc: 0.6868 - val_loss: 1.0703 - val_acc: 0.5416\n",
      "Epoch 754/1000\n",
      "3784/3784 [==============================] - 1s 226us/step - loss: 0.4946 - acc: 0.6890 - val_loss: 1.0565 - val_acc: 0.5321\n",
      "Epoch 755/1000\n",
      "3784/3784 [==============================] - 1s 232us/step - loss: 0.4888 - acc: 0.6911 - val_loss: 1.0777 - val_acc: 0.5463\n",
      "Epoch 756/1000\n",
      "3784/3784 [==============================] - 1s 234us/step - loss: 0.4875 - acc: 0.6842 - val_loss: 1.0854 - val_acc: 0.5368\n",
      "Epoch 757/1000\n",
      "3784/3784 [==============================] - 1s 231us/step - loss: 0.4817 - acc: 0.6942 - val_loss: 1.0949 - val_acc: 0.5344\n",
      "Epoch 758/1000\n",
      "3784/3784 [==============================] - 1s 230us/step - loss: 0.4803 - acc: 0.6987 - val_loss: 1.1131 - val_acc: 0.5392\n",
      "Epoch 759/1000\n",
      "3784/3784 [==============================] - 1s 232us/step - loss: 0.4823 - acc: 0.6908 - val_loss: 1.0858 - val_acc: 0.5392\n",
      "Epoch 760/1000\n",
      "3784/3784 [==============================] - 1s 232us/step - loss: 0.4812 - acc: 0.6834 - val_loss: 1.0795 - val_acc: 0.5321\n",
      "Epoch 761/1000\n",
      "3784/3784 [==============================] - 1s 230us/step - loss: 0.4941 - acc: 0.6932 - val_loss: 1.0452 - val_acc: 0.5321\n",
      "Epoch 762/1000\n",
      "3784/3784 [==============================] - 1s 234us/step - loss: 0.4856 - acc: 0.6895 - val_loss: 1.0645 - val_acc: 0.5368\n",
      "Epoch 763/1000\n",
      "3784/3784 [==============================] - 1s 231us/step - loss: 0.4752 - acc: 0.6921 - val_loss: 1.0767 - val_acc: 0.5416\n",
      "Epoch 764/1000\n",
      "3784/3784 [==============================] - 1s 233us/step - loss: 0.4808 - acc: 0.6911 - val_loss: 1.0784 - val_acc: 0.5321\n",
      "Epoch 765/1000\n",
      "3784/3784 [==============================] - 1s 232us/step - loss: 0.4787 - acc: 0.6945 - val_loss: 1.0909 - val_acc: 0.5368\n",
      "Epoch 766/1000\n",
      "3784/3784 [==============================] - 1s 228us/step - loss: 0.4932 - acc: 0.6876 - val_loss: 1.0778 - val_acc: 0.5321\n",
      "Epoch 767/1000\n",
      "3784/3784 [==============================] - 1s 233us/step - loss: 0.4706 - acc: 0.6956 - val_loss: 1.0974 - val_acc: 0.5439\n",
      "Epoch 768/1000\n",
      "3784/3784 [==============================] - 1s 236us/step - loss: 0.4968 - acc: 0.6813 - val_loss: 1.0845 - val_acc: 0.5321\n",
      "Epoch 769/1000\n",
      "3784/3784 [==============================] - 1s 227us/step - loss: 0.4900 - acc: 0.6937 - val_loss: 1.0984 - val_acc: 0.5439\n",
      "Epoch 770/1000\n",
      "3784/3784 [==============================] - 1s 231us/step - loss: 0.4890 - acc: 0.6831 - val_loss: 1.0738 - val_acc: 0.5416\n",
      "Epoch 771/1000\n",
      "3784/3784 [==============================] - 1s 236us/step - loss: 0.4933 - acc: 0.6855 - val_loss: 1.0561 - val_acc: 0.5463\n",
      "Epoch 772/1000\n",
      "3784/3784 [==============================] - 1s 236us/step - loss: 0.4852 - acc: 0.6897 - val_loss: 1.0660 - val_acc: 0.5344\n",
      "Epoch 773/1000\n",
      "3784/3784 [==============================] - 1s 237us/step - loss: 0.4815 - acc: 0.6837 - val_loss: 1.0628 - val_acc: 0.5439\n",
      "Epoch 774/1000\n",
      "3784/3784 [==============================] - 1s 234us/step - loss: 0.4812 - acc: 0.6956 - val_loss: 1.0746 - val_acc: 0.53440s - loss: 0.4804 - acc\n",
      "Epoch 775/1000\n",
      "3784/3784 [==============================] - 1s 235us/step - loss: 0.4900 - acc: 0.6882 - val_loss: 1.0767 - val_acc: 0.5368\n",
      "Epoch 776/1000\n",
      "3784/3784 [==============================] - 1s 231us/step - loss: 0.4813 - acc: 0.6919 - val_loss: 1.0784 - val_acc: 0.5439\n",
      "Epoch 777/1000\n",
      "3784/3784 [==============================] - 1s 232us/step - loss: 0.4844 - acc: 0.6921 - val_loss: 1.0728 - val_acc: 0.5439\n",
      "Epoch 778/1000\n",
      "3784/3784 [==============================] - 1s 240us/step - loss: 0.4782 - acc: 0.6892 - val_loss: 1.0889 - val_acc: 0.5416\n",
      "Epoch 779/1000\n",
      "3784/3784 [==============================] - 1s 235us/step - loss: 0.4768 - acc: 0.7011 - val_loss: 1.1039 - val_acc: 0.5511\n",
      "Epoch 780/1000\n",
      "3784/3784 [==============================] - 1s 230us/step - loss: 0.4900 - acc: 0.7016 - val_loss: 1.1014 - val_acc: 0.5511\n",
      "Epoch 781/1000\n",
      "3784/3784 [==============================] - 1s 232us/step - loss: 0.4804 - acc: 0.6969 - val_loss: 1.1047 - val_acc: 0.5273\n",
      "Epoch 782/1000\n",
      "3784/3784 [==============================] - 1s 232us/step - loss: 0.4762 - acc: 0.6921 - val_loss: 1.1007 - val_acc: 0.5392\n",
      "Epoch 783/1000\n",
      "3784/3784 [==============================] - 1s 230us/step - loss: 0.4966 - acc: 0.6921 - val_loss: 1.0636 - val_acc: 0.5487\n",
      "Epoch 784/1000\n",
      "3784/3784 [==============================] - 1s 233us/step - loss: 0.4783 - acc: 0.6927 - val_loss: 1.0843 - val_acc: 0.5392\n",
      "Epoch 785/1000\n",
      "3784/3784 [==============================] - 1s 234us/step - loss: 0.4802 - acc: 0.6919 - val_loss: 1.0844 - val_acc: 0.5487\n",
      "Epoch 786/1000\n",
      "3784/3784 [==============================] - 1s 231us/step - loss: 0.4758 - acc: 0.6974 - val_loss: 1.0844 - val_acc: 0.5582\n",
      "Epoch 787/1000\n",
      "3784/3784 [==============================] - 1s 234us/step - loss: 0.4741 - acc: 0.6995 - val_loss: 1.0817 - val_acc: 0.5416\n",
      "Epoch 788/1000\n",
      "3784/3784 [==============================] - 1s 233us/step - loss: 0.4800 - acc: 0.6948 - val_loss: 1.0886 - val_acc: 0.5416\n",
      "Epoch 789/1000\n",
      "3784/3784 [==============================] - 1s 233us/step - loss: 0.4822 - acc: 0.6934 - val_loss: 1.0800 - val_acc: 0.5534\n",
      "Epoch 790/1000\n",
      "3784/3784 [==============================] - 1s 230us/step - loss: 0.4836 - acc: 0.6903 - val_loss: 1.0877 - val_acc: 0.5439\n",
      "Epoch 791/1000\n",
      "3784/3784 [==============================] - 1s 230us/step - loss: 0.4846 - acc: 0.6969 - val_loss: 1.0912 - val_acc: 0.5368\n",
      "Epoch 792/1000\n",
      "3784/3784 [==============================] - 1s 229us/step - loss: 0.4748 - acc: 0.6961 - val_loss: 1.0935 - val_acc: 0.5463\n",
      "Epoch 793/1000\n",
      "3784/3784 [==============================] - 1s 232us/step - loss: 0.4749 - acc: 0.6982 - val_loss: 1.1085 - val_acc: 0.5463\n",
      "Epoch 794/1000\n",
      "3784/3784 [==============================] - 1s 230us/step - loss: 0.4783 - acc: 0.7001 - val_loss: 1.0961 - val_acc: 0.5416\n",
      "Epoch 795/1000\n",
      "3784/3784 [==============================] - 1s 231us/step - loss: 0.4820 - acc: 0.6845 - val_loss: 1.0895 - val_acc: 0.5416\n",
      "Epoch 796/1000\n",
      "3784/3784 [==============================] - 1s 234us/step - loss: 0.4781 - acc: 0.6940 - val_loss: 1.1057 - val_acc: 0.5392\n",
      "Epoch 797/1000\n",
      "3784/3784 [==============================] - 1s 229us/step - loss: 0.4739 - acc: 0.6961 - val_loss: 1.1183 - val_acc: 0.5344\n",
      "Epoch 798/1000\n",
      "3784/3784 [==============================] - 1s 236us/step - loss: 0.4859 - acc: 0.6934 - val_loss: 1.1151 - val_acc: 0.5416\n",
      "Epoch 799/1000\n",
      "3784/3784 [==============================] - 1s 227us/step - loss: 0.4773 - acc: 0.7014 - val_loss: 1.1376 - val_acc: 0.5392\n",
      "Epoch 800/1000\n",
      "3784/3784 [==============================] - 1s 230us/step - loss: 0.4843 - acc: 0.6937 - val_loss: 1.1206 - val_acc: 0.5487\n",
      "Epoch 801/1000\n",
      "3784/3784 [==============================] - 1s 231us/step - loss: 0.4882 - acc: 0.6921 - val_loss: 1.1064 - val_acc: 0.5416\n",
      "Epoch 802/1000\n",
      "3784/3784 [==============================] - 1s 228us/step - loss: 0.4809 - acc: 0.6919 - val_loss: 1.1050 - val_acc: 0.5439\n",
      "Epoch 803/1000\n",
      "3784/3784 [==============================] - 1s 232us/step - loss: 0.4766 - acc: 0.6945 - val_loss: 1.0984 - val_acc: 0.5558\n",
      "Epoch 804/1000\n",
      "3784/3784 [==============================] - 1s 237us/step - loss: 0.4798 - acc: 0.6956 - val_loss: 1.0761 - val_acc: 0.5368\n",
      "Epoch 805/1000\n",
      "3784/3784 [==============================] - 1s 230us/step - loss: 0.4735 - acc: 0.6956 - val_loss: 1.0934 - val_acc: 0.5463\n",
      "Epoch 806/1000\n",
      "3784/3784 [==============================] - 1s 232us/step - loss: 0.4719 - acc: 0.6940 - val_loss: 1.1046 - val_acc: 0.5487\n",
      "Epoch 807/1000\n",
      "3784/3784 [==============================] - 1s 234us/step - loss: 0.4785 - acc: 0.6956 - val_loss: 1.1083 - val_acc: 0.5534\n",
      "Epoch 808/1000\n",
      "3784/3784 [==============================] - 1s 233us/step - loss: 0.4729 - acc: 0.6897 - val_loss: 1.0966 - val_acc: 0.5511\n",
      "Epoch 809/1000\n",
      "3784/3784 [==============================] - 1s 229us/step - loss: 0.4734 - acc: 0.6937 - val_loss: 1.0870 - val_acc: 0.5439\n",
      "Epoch 810/1000\n",
      "3784/3784 [==============================] - 1s 231us/step - loss: 0.4682 - acc: 0.7045 - val_loss: 1.1026 - val_acc: 0.5439\n",
      "Epoch 811/1000\n",
      "3784/3784 [==============================] - 1s 233us/step - loss: 0.4748 - acc: 0.6995 - val_loss: 1.1137 - val_acc: 0.5487\n",
      "Epoch 812/1000\n",
      "3784/3784 [==============================] - 1s 232us/step - loss: 0.4794 - acc: 0.7035 - val_loss: 1.1023 - val_acc: 0.5392\n",
      "Epoch 813/1000\n",
      "3784/3784 [==============================] - 1s 232us/step - loss: 0.4786 - acc: 0.6979 - val_loss: 1.0925 - val_acc: 0.5416\n",
      "Epoch 814/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3784/3784 [==============================] - 1s 233us/step - loss: 0.4646 - acc: 0.7064 - val_loss: 1.0953 - val_acc: 0.5416\n",
      "Epoch 815/1000\n",
      "3784/3784 [==============================] - 1s 231us/step - loss: 0.4655 - acc: 0.7014 - val_loss: 1.1162 - val_acc: 0.5321\n",
      "Epoch 816/1000\n",
      "3784/3784 [==============================] - 1s 232us/step - loss: 0.4869 - acc: 0.6908 - val_loss: 1.1152 - val_acc: 0.5392\n",
      "Epoch 817/1000\n",
      "3784/3784 [==============================] - 1s 232us/step - loss: 0.4651 - acc: 0.6995 - val_loss: 1.1216 - val_acc: 0.5321\n",
      "Epoch 818/1000\n",
      "3784/3784 [==============================] - 1s 231us/step - loss: 0.4750 - acc: 0.6927 - val_loss: 1.1217 - val_acc: 0.5487\n",
      "Epoch 819/1000\n",
      "3784/3784 [==============================] - 1s 230us/step - loss: 0.4702 - acc: 0.6903 - val_loss: 1.0929 - val_acc: 0.5321\n",
      "Epoch 820/1000\n",
      "3784/3784 [==============================] - 1s 233us/step - loss: 0.4690 - acc: 0.7027 - val_loss: 1.0938 - val_acc: 0.5392\n",
      "Epoch 821/1000\n",
      "3784/3784 [==============================] - 1s 229us/step - loss: 0.4735 - acc: 0.6948 - val_loss: 1.1101 - val_acc: 0.5344\n",
      "Epoch 822/1000\n",
      "3784/3784 [==============================] - 1s 232us/step - loss: 0.4627 - acc: 0.7032 - val_loss: 1.1362 - val_acc: 0.5534\n",
      "Epoch 823/1000\n",
      "3784/3784 [==============================] - 1s 232us/step - loss: 0.4647 - acc: 0.7038 - val_loss: 1.1341 - val_acc: 0.5463\n",
      "Epoch 824/1000\n",
      "3784/3784 [==============================] - 1s 229us/step - loss: 0.4743 - acc: 0.6897 - val_loss: 1.1268 - val_acc: 0.5368\n",
      "Epoch 825/1000\n",
      "3784/3784 [==============================] - 1s 224us/step - loss: 0.4736 - acc: 0.6964 - val_loss: 1.1250 - val_acc: 0.5463\n",
      "Epoch 826/1000\n",
      "3784/3784 [==============================] - 1s 226us/step - loss: 0.4758 - acc: 0.7038 - val_loss: 1.1227 - val_acc: 0.5534\n",
      "Epoch 827/1000\n",
      "3784/3784 [==============================] - 1s 225us/step - loss: 0.4710 - acc: 0.6945 - val_loss: 1.0844 - val_acc: 0.5487\n",
      "Epoch 828/1000\n",
      "3784/3784 [==============================] - 1s 225us/step - loss: 0.4780 - acc: 0.6945 - val_loss: 1.1029 - val_acc: 0.5463\n",
      "Epoch 829/1000\n",
      "3784/3784 [==============================] - 1s 226us/step - loss: 0.4749 - acc: 0.6948 - val_loss: 1.1039 - val_acc: 0.5487\n",
      "Epoch 830/1000\n",
      "3784/3784 [==============================] - 1s 226us/step - loss: 0.4744 - acc: 0.6948 - val_loss: 1.0985 - val_acc: 0.5511\n",
      "Epoch 831/1000\n",
      "3784/3784 [==============================] - 1s 232us/step - loss: 0.4737 - acc: 0.6995 - val_loss: 1.0965 - val_acc: 0.5463\n",
      "Epoch 832/1000\n",
      "3784/3784 [==============================] - 1s 225us/step - loss: 0.4676 - acc: 0.7053 - val_loss: 1.1008 - val_acc: 0.5463\n",
      "Epoch 833/1000\n",
      "3784/3784 [==============================] - 1s 224us/step - loss: 0.4743 - acc: 0.6995 - val_loss: 1.1019 - val_acc: 0.5534\n",
      "Epoch 834/1000\n",
      "3784/3784 [==============================] - 1s 224us/step - loss: 0.4805 - acc: 0.6927 - val_loss: 1.1025 - val_acc: 0.5439\n",
      "Epoch 835/1000\n",
      "3784/3784 [==============================] - 1s 226us/step - loss: 0.4819 - acc: 0.6921 - val_loss: 1.1008 - val_acc: 0.5368\n",
      "Epoch 836/1000\n",
      "3784/3784 [==============================] - 1s 226us/step - loss: 0.4757 - acc: 0.6985 - val_loss: 1.1071 - val_acc: 0.5344\n",
      "Epoch 837/1000\n",
      "3784/3784 [==============================] - 1s 226us/step - loss: 0.4790 - acc: 0.6956 - val_loss: 1.1057 - val_acc: 0.5368\n",
      "Epoch 838/1000\n",
      "3784/3784 [==============================] - 1s 225us/step - loss: 0.4658 - acc: 0.6971 - val_loss: 1.1006 - val_acc: 0.5558\n",
      "Epoch 839/1000\n",
      "3784/3784 [==============================] - 1s 226us/step - loss: 0.4640 - acc: 0.7032 - val_loss: 1.1027 - val_acc: 0.5416\n",
      "Epoch 840/1000\n",
      "3784/3784 [==============================] - 1s 225us/step - loss: 0.4628 - acc: 0.7106 - val_loss: 1.1141 - val_acc: 0.5439\n",
      "Epoch 841/1000\n",
      "3784/3784 [==============================] - 1s 224us/step - loss: 0.4732 - acc: 0.7019 - val_loss: 1.1267 - val_acc: 0.5463\n",
      "Epoch 842/1000\n",
      "3784/3784 [==============================] - 1s 228us/step - loss: 0.4853 - acc: 0.6940 - val_loss: 1.1173 - val_acc: 0.5368\n",
      "Epoch 843/1000\n",
      "3784/3784 [==============================] - 1s 226us/step - loss: 0.4716 - acc: 0.6974 - val_loss: 1.1240 - val_acc: 0.5511\n",
      "Epoch 844/1000\n",
      "3784/3784 [==============================] - 1s 225us/step - loss: 0.4756 - acc: 0.6966 - val_loss: 1.1346 - val_acc: 0.5606\n",
      "Epoch 845/1000\n",
      "3784/3784 [==============================] - 1s 224us/step - loss: 0.4764 - acc: 0.6971 - val_loss: 1.1272 - val_acc: 0.5344\n",
      "Epoch 846/1000\n",
      "3784/3784 [==============================] - 1s 225us/step - loss: 0.4729 - acc: 0.6995 - val_loss: 1.1320 - val_acc: 0.5511\n",
      "Epoch 847/1000\n",
      "3784/3784 [==============================] - 1s 225us/step - loss: 0.4635 - acc: 0.7019 - val_loss: 1.1185 - val_acc: 0.5368\n",
      "Epoch 848/1000\n",
      "3784/3784 [==============================] - 1s 226us/step - loss: 0.4710 - acc: 0.6987 - val_loss: 1.1437 - val_acc: 0.5392\n",
      "Epoch 849/1000\n",
      "3784/3784 [==============================] - 1s 224us/step - loss: 0.4672 - acc: 0.7024 - val_loss: 1.1325 - val_acc: 0.5463\n",
      "Epoch 850/1000\n",
      "3784/3784 [==============================] - 1s 225us/step - loss: 0.4690 - acc: 0.6993 - val_loss: 1.1213 - val_acc: 0.5511\n",
      "Epoch 851/1000\n",
      "3784/3784 [==============================] - 1s 226us/step - loss: 0.4709 - acc: 0.6998 - val_loss: 1.1342 - val_acc: 0.5487\n",
      "Epoch 852/1000\n",
      "3784/3784 [==============================] - 1s 225us/step - loss: 0.4657 - acc: 0.6969 - val_loss: 1.1371 - val_acc: 0.5463\n",
      "Epoch 853/1000\n",
      "3784/3784 [==============================] - 1s 224us/step - loss: 0.4707 - acc: 0.7006 - val_loss: 1.1208 - val_acc: 0.5439\n",
      "Epoch 854/1000\n",
      "3784/3784 [==============================] - 1s 225us/step - loss: 0.4655 - acc: 0.7048 - val_loss: 1.1304 - val_acc: 0.5487\n",
      "Epoch 855/1000\n",
      "3784/3784 [==============================] - 1s 224us/step - loss: 0.4803 - acc: 0.6987 - val_loss: 1.1072 - val_acc: 0.5606\n",
      "Epoch 856/1000\n",
      "3784/3784 [==============================] - 1s 224us/step - loss: 0.4755 - acc: 0.6929 - val_loss: 1.1140 - val_acc: 0.5392\n",
      "Epoch 857/1000\n",
      "3784/3784 [==============================] - 1s 227us/step - loss: 0.4739 - acc: 0.6908 - val_loss: 1.1142 - val_acc: 0.5321\n",
      "Epoch 858/1000\n",
      "3784/3784 [==============================] - 1s 232us/step - loss: 0.4584 - acc: 0.7093 - val_loss: 1.1324 - val_acc: 0.5416\n",
      "Epoch 859/1000\n",
      "3784/3784 [==============================] - 1s 225us/step - loss: 0.4674 - acc: 0.7022 - val_loss: 1.1313 - val_acc: 0.5416\n",
      "Epoch 860/1000\n",
      "3784/3784 [==============================] - 1s 224us/step - loss: 0.4615 - acc: 0.7119 - val_loss: 1.1325 - val_acc: 0.5487\n",
      "Epoch 861/1000\n",
      "3784/3784 [==============================] - 1s 224us/step - loss: 0.4613 - acc: 0.7051 - val_loss: 1.1193 - val_acc: 0.5487\n",
      "Epoch 862/1000\n",
      "3784/3784 [==============================] - 1s 224us/step - loss: 0.4625 - acc: 0.7038 - val_loss: 1.1392 - val_acc: 0.5534\n",
      "Epoch 863/1000\n",
      "3784/3784 [==============================] - 1s 225us/step - loss: 0.4603 - acc: 0.7056 - val_loss: 1.1443 - val_acc: 0.5321\n",
      "Epoch 864/1000\n",
      "3784/3784 [==============================] - 1s 225us/step - loss: 0.4776 - acc: 0.6924 - val_loss: 1.1174 - val_acc: 0.5368\n",
      "Epoch 865/1000\n",
      "3784/3784 [==============================] - 1s 227us/step - loss: 0.4760 - acc: 0.6987 - val_loss: 1.1272 - val_acc: 0.5344\n",
      "Epoch 866/1000\n",
      "3784/3784 [==============================] - 1s 223us/step - loss: 0.4639 - acc: 0.7030 - val_loss: 1.1312 - val_acc: 0.5439\n",
      "Epoch 867/1000\n",
      "3784/3784 [==============================] - 1s 226us/step - loss: 0.4617 - acc: 0.7014 - val_loss: 1.0995 - val_acc: 0.5439\n",
      "Epoch 868/1000\n",
      "3784/3784 [==============================] - 1s 226us/step - loss: 0.4657 - acc: 0.7030 - val_loss: 1.1178 - val_acc: 0.5487\n",
      "Epoch 869/1000\n",
      "3784/3784 [==============================] - 1s 224us/step - loss: 0.4599 - acc: 0.7048 - val_loss: 1.1231 - val_acc: 0.5582\n",
      "Epoch 870/1000\n",
      "3784/3784 [==============================] - 1s 226us/step - loss: 0.4719 - acc: 0.6982 - val_loss: 1.1161 - val_acc: 0.5487\n",
      "Epoch 871/1000\n",
      "3784/3784 [==============================] - 1s 224us/step - loss: 0.4590 - acc: 0.7114 - val_loss: 1.1229 - val_acc: 0.5511\n",
      "Epoch 872/1000\n",
      "3784/3784 [==============================] - 1s 225us/step - loss: 0.4577 - acc: 0.7170 - val_loss: 1.1320 - val_acc: 0.5511\n",
      "Epoch 873/1000\n",
      "3784/3784 [==============================] - 1s 225us/step - loss: 0.4643 - acc: 0.7045 - val_loss: 1.1289 - val_acc: 0.5582\n",
      "Epoch 874/1000\n",
      "3784/3784 [==============================] - 1s 224us/step - loss: 0.4634 - acc: 0.7096 - val_loss: 1.1342 - val_acc: 0.5534\n",
      "Epoch 875/1000\n",
      "3784/3784 [==============================] - 1s 225us/step - loss: 0.4637 - acc: 0.7040 - val_loss: 1.1411 - val_acc: 0.5534\n",
      "Epoch 876/1000\n",
      "3784/3784 [==============================] - 1s 228us/step - loss: 0.4621 - acc: 0.7024 - val_loss: 1.1565 - val_acc: 0.5534\n",
      "Epoch 877/1000\n",
      "3784/3784 [==============================] - 1s 227us/step - loss: 0.4599 - acc: 0.7112 - val_loss: 1.1541 - val_acc: 0.5487\n",
      "Epoch 878/1000\n",
      "3784/3784 [==============================] - 1s 226us/step - loss: 0.4555 - acc: 0.7119 - val_loss: 1.1309 - val_acc: 0.5439\n",
      "Epoch 879/1000\n",
      "3784/3784 [==============================] - 1s 225us/step - loss: 0.4571 - acc: 0.7059 - val_loss: 1.1396 - val_acc: 0.5487\n",
      "Epoch 880/1000\n",
      "3784/3784 [==============================] - 1s 224us/step - loss: 0.4764 - acc: 0.6990 - val_loss: 1.1145 - val_acc: 0.5558\n",
      "Epoch 881/1000\n",
      "3784/3784 [==============================] - 1s 223us/step - loss: 0.4744 - acc: 0.7008 - val_loss: 1.1422 - val_acc: 0.5463\n",
      "Epoch 882/1000\n",
      "3784/3784 [==============================] - 1s 225us/step - loss: 0.4609 - acc: 0.7022 - val_loss: 1.1439 - val_acc: 0.5511\n",
      "Epoch 883/1000\n",
      "3784/3784 [==============================] - 1s 226us/step - loss: 0.4623 - acc: 0.7090 - val_loss: 1.1494 - val_acc: 0.5368\n",
      "Epoch 884/1000\n",
      "3784/3784 [==============================] - 1s 226us/step - loss: 0.4718 - acc: 0.6998 - val_loss: 1.1208 - val_acc: 0.5629\n",
      "Epoch 885/1000\n",
      "3784/3784 [==============================] - 1s 231us/step - loss: 0.4602 - acc: 0.7098 - val_loss: 1.1394 - val_acc: 0.5629\n",
      "Epoch 886/1000\n",
      "3784/3784 [==============================] - 1s 227us/step - loss: 0.4633 - acc: 0.7085 - val_loss: 1.1244 - val_acc: 0.5534\n",
      "Epoch 887/1000\n",
      "3784/3784 [==============================] - 1s 223us/step - loss: 0.4713 - acc: 0.7032 - val_loss: 1.1150 - val_acc: 0.5534\n",
      "Epoch 888/1000\n",
      "3784/3784 [==============================] - 1s 227us/step - loss: 0.4680 - acc: 0.6995 - val_loss: 1.1250 - val_acc: 0.5511\n",
      "Epoch 889/1000\n",
      "3784/3784 [==============================] - 1s 225us/step - loss: 0.4551 - acc: 0.7117 - val_loss: 1.1575 - val_acc: 0.5558\n",
      "Epoch 890/1000\n",
      "3784/3784 [==============================] - 1s 224us/step - loss: 0.4711 - acc: 0.7090 - val_loss: 1.1466 - val_acc: 0.5487\n",
      "Epoch 891/1000\n",
      "3784/3784 [==============================] - 1s 227us/step - loss: 0.4580 - acc: 0.7082 - val_loss: 1.1405 - val_acc: 0.5534\n",
      "Epoch 892/1000\n",
      "3784/3784 [==============================] - 1s 224us/step - loss: 0.4654 - acc: 0.7016 - val_loss: 1.1273 - val_acc: 0.5511\n",
      "Epoch 893/1000\n",
      "3784/3784 [==============================] - 1s 224us/step - loss: 0.4609 - acc: 0.7048 - val_loss: 1.1298 - val_acc: 0.5463\n",
      "Epoch 894/1000\n",
      "3784/3784 [==============================] - 1s 225us/step - loss: 0.4532 - acc: 0.7090 - val_loss: 1.1298 - val_acc: 0.5416\n",
      "Epoch 895/1000\n",
      "3784/3784 [==============================] - 1s 230us/step - loss: 0.4634 - acc: 0.7016 - val_loss: 1.1347 - val_acc: 0.5511\n",
      "Epoch 896/1000\n",
      "3784/3784 [==============================] - 1s 228us/step - loss: 0.4591 - acc: 0.7154 - val_loss: 1.1552 - val_acc: 0.5439\n",
      "Epoch 897/1000\n",
      "3784/3784 [==============================] - 1s 225us/step - loss: 0.4546 - acc: 0.7093 - val_loss: 1.1718 - val_acc: 0.5511\n",
      "Epoch 898/1000\n",
      "3784/3784 [==============================] - 1s 224us/step - loss: 0.4630 - acc: 0.7135 - val_loss: 1.1527 - val_acc: 0.5463\n",
      "Epoch 899/1000\n",
      "3784/3784 [==============================] - 1s 223us/step - loss: 0.4763 - acc: 0.6993 - val_loss: 1.1220 - val_acc: 0.5439\n",
      "Epoch 900/1000\n",
      "3784/3784 [==============================] - 1s 226us/step - loss: 0.4697 - acc: 0.7008 - val_loss: 1.1221 - val_acc: 0.5558\n",
      "Epoch 901/1000\n",
      "3784/3784 [==============================] - 1s 225us/step - loss: 0.4626 - acc: 0.7130 - val_loss: 1.1334 - val_acc: 0.5534\n",
      "Epoch 902/1000\n",
      "3784/3784 [==============================] - 1s 224us/step - loss: 0.4652 - acc: 0.7067 - val_loss: 1.1408 - val_acc: 0.5487\n",
      "Epoch 903/1000\n",
      "3784/3784 [==============================] - 1s 225us/step - loss: 0.4629 - acc: 0.7067 - val_loss: 1.1401 - val_acc: 0.5416\n",
      "Epoch 904/1000\n",
      "3784/3784 [==============================] - 1s 225us/step - loss: 0.4683 - acc: 0.7027 - val_loss: 1.1416 - val_acc: 0.5534\n",
      "Epoch 905/1000\n",
      "3784/3784 [==============================] - 1s 227us/step - loss: 0.4697 - acc: 0.7003 - val_loss: 1.1334 - val_acc: 0.5511\n",
      "Epoch 906/1000\n",
      "3784/3784 [==============================] - 1s 224us/step - loss: 0.4631 - acc: 0.6987 - val_loss: 1.1525 - val_acc: 0.5511\n",
      "Epoch 907/1000\n",
      "3784/3784 [==============================] - 1s 225us/step - loss: 0.4621 - acc: 0.7149 - val_loss: 1.1490 - val_acc: 0.5487\n",
      "Epoch 908/1000\n",
      "3784/3784 [==============================] - 1s 223us/step - loss: 0.4580 - acc: 0.7080 - val_loss: 1.1614 - val_acc: 0.5534\n",
      "Epoch 909/1000\n",
      "3784/3784 [==============================] - 1s 224us/step - loss: 0.4594 - acc: 0.7038 - val_loss: 1.1405 - val_acc: 0.5534\n",
      "Epoch 910/1000\n",
      "3784/3784 [==============================] - 1s 224us/step - loss: 0.4622 - acc: 0.7030 - val_loss: 1.1415 - val_acc: 0.5439\n",
      "Epoch 911/1000\n",
      "3784/3784 [==============================] - 1s 223us/step - loss: 0.4571 - acc: 0.7032 - val_loss: 1.1504 - val_acc: 0.5487\n",
      "Epoch 912/1000\n",
      "3784/3784 [==============================] - 1s 230us/step - loss: 0.4467 - acc: 0.7164 - val_loss: 1.1494 - val_acc: 0.5463\n",
      "Epoch 913/1000\n",
      "3784/3784 [==============================] - 1s 223us/step - loss: 0.4647 - acc: 0.7038 - val_loss: 1.1391 - val_acc: 0.5463\n",
      "Epoch 914/1000\n",
      "3784/3784 [==============================] - 1s 228us/step - loss: 0.4640 - acc: 0.7016 - val_loss: 1.1508 - val_acc: 0.5558\n",
      "Epoch 915/1000\n",
      "3784/3784 [==============================] - 1s 225us/step - loss: 0.4551 - acc: 0.7125 - val_loss: 1.1731 - val_acc: 0.5463\n",
      "Epoch 916/1000\n",
      "3784/3784 [==============================] - 1s 224us/step - loss: 0.4584 - acc: 0.7119 - val_loss: 1.1267 - val_acc: 0.5416\n",
      "Epoch 917/1000\n",
      "3784/3784 [==============================] - 1s 223us/step - loss: 0.4627 - acc: 0.7032 - val_loss: 1.1331 - val_acc: 0.5582\n",
      "Epoch 918/1000\n",
      "3784/3784 [==============================] - 1s 226us/step - loss: 0.4557 - acc: 0.7080 - val_loss: 1.1453 - val_acc: 0.5439\n",
      "Epoch 919/1000\n",
      "3784/3784 [==============================] - 1s 227us/step - loss: 0.4524 - acc: 0.7106 - val_loss: 1.1728 - val_acc: 0.5487\n",
      "Epoch 920/1000\n",
      "3784/3784 [==============================] - 1s 225us/step - loss: 0.4538 - acc: 0.7122 - val_loss: 1.1510 - val_acc: 0.5439\n",
      "Epoch 921/1000\n",
      "3784/3784 [==============================] - 1s 225us/step - loss: 0.4548 - acc: 0.7098 - val_loss: 1.1496 - val_acc: 0.5416\n",
      "Epoch 922/1000\n",
      "3784/3784 [==============================] - 1s 225us/step - loss: 0.4593 - acc: 0.7090 - val_loss: 1.1545 - val_acc: 0.5487\n",
      "Epoch 923/1000\n",
      "3784/3784 [==============================] - 1s 225us/step - loss: 0.4613 - acc: 0.7112 - val_loss: 1.1365 - val_acc: 0.5582\n",
      "Epoch 924/1000\n",
      "3784/3784 [==============================] - 1s 226us/step - loss: 0.4644 - acc: 0.7011 - val_loss: 1.1589 - val_acc: 0.5273\n",
      "Epoch 925/1000\n",
      "3784/3784 [==============================] - 1s 225us/step - loss: 0.4601 - acc: 0.7075 - val_loss: 1.1579 - val_acc: 0.5416\n",
      "Epoch 926/1000\n",
      "3784/3784 [==============================] - 1s 224us/step - loss: 0.4487 - acc: 0.7170 - val_loss: 1.1705 - val_acc: 0.5344\n",
      "Epoch 927/1000\n",
      "3784/3784 [==============================] - 1s 225us/step - loss: 0.4587 - acc: 0.7146 - val_loss: 1.1767 - val_acc: 0.5511\n",
      "Epoch 928/1000\n",
      "3784/3784 [==============================] - 1s 226us/step - loss: 0.4556 - acc: 0.7109 - val_loss: 1.1813 - val_acc: 0.5534\n",
      "Epoch 929/1000\n",
      "3784/3784 [==============================] - 1s 225us/step - loss: 0.4621 - acc: 0.7090 - val_loss: 1.1784 - val_acc: 0.5439\n",
      "Epoch 930/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3784/3784 [==============================] - 1s 224us/step - loss: 0.4552 - acc: 0.7101 - val_loss: 1.1691 - val_acc: 0.5463\n",
      "Epoch 931/1000\n",
      "3784/3784 [==============================] - 1s 225us/step - loss: 0.4562 - acc: 0.7151 - val_loss: 1.1745 - val_acc: 0.5463\n",
      "Epoch 932/1000\n",
      "3784/3784 [==============================] - 1s 224us/step - loss: 0.4548 - acc: 0.7090 - val_loss: 1.1293 - val_acc: 0.5487\n",
      "Epoch 933/1000\n",
      "3784/3784 [==============================] - 1s 224us/step - loss: 0.4643 - acc: 0.7035 - val_loss: 1.1450 - val_acc: 0.5606\n",
      "Epoch 934/1000\n",
      "3784/3784 [==============================] - 1s 225us/step - loss: 0.4517 - acc: 0.7067 - val_loss: 1.1397 - val_acc: 0.5558\n",
      "Epoch 935/1000\n",
      "3784/3784 [==============================] - 1s 227us/step - loss: 0.4628 - acc: 0.7093 - val_loss: 1.1412 - val_acc: 0.5534\n",
      "Epoch 936/1000\n",
      "3784/3784 [==============================] - 1s 227us/step - loss: 0.4524 - acc: 0.7125 - val_loss: 1.1540 - val_acc: 0.5439\n",
      "Epoch 937/1000\n",
      "3784/3784 [==============================] - 1s 225us/step - loss: 0.4548 - acc: 0.7125 - val_loss: 1.1621 - val_acc: 0.5487\n",
      "Epoch 938/1000\n",
      "3784/3784 [==============================] - 1s 226us/step - loss: 0.4548 - acc: 0.7119 - val_loss: 1.1506 - val_acc: 0.5416\n",
      "Epoch 939/1000\n",
      "3784/3784 [==============================] - 1s 234us/step - loss: 0.4635 - acc: 0.7024 - val_loss: 1.1357 - val_acc: 0.5463\n",
      "Epoch 940/1000\n",
      "3784/3784 [==============================] - 1s 225us/step - loss: 0.4463 - acc: 0.7093 - val_loss: 1.1544 - val_acc: 0.5463\n",
      "Epoch 941/1000\n",
      "3784/3784 [==============================] - 1s 226us/step - loss: 0.4531 - acc: 0.7101 - val_loss: 1.1516 - val_acc: 0.5534\n",
      "Epoch 942/1000\n",
      "3784/3784 [==============================] - 1s 226us/step - loss: 0.4520 - acc: 0.7172 - val_loss: 1.1653 - val_acc: 0.5463\n",
      "Epoch 943/1000\n",
      "3784/3784 [==============================] - 1s 224us/step - loss: 0.4402 - acc: 0.7178 - val_loss: 1.1513 - val_acc: 0.5463\n",
      "Epoch 944/1000\n",
      "3784/3784 [==============================] - 1s 225us/step - loss: 0.4491 - acc: 0.7077 - val_loss: 1.1691 - val_acc: 0.5511\n",
      "Epoch 945/1000\n",
      "3784/3784 [==============================] - 1s 225us/step - loss: 0.4459 - acc: 0.7112 - val_loss: 1.1652 - val_acc: 0.5368\n",
      "Epoch 946/1000\n",
      "3784/3784 [==============================] - 1s 225us/step - loss: 0.4561 - acc: 0.7119 - val_loss: 1.1940 - val_acc: 0.5416\n",
      "Epoch 947/1000\n",
      "3784/3784 [==============================] - 1s 223us/step - loss: 0.4551 - acc: 0.7082 - val_loss: 1.1632 - val_acc: 0.5392\n",
      "Epoch 948/1000\n",
      "3784/3784 [==============================] - 1s 227us/step - loss: 0.4536 - acc: 0.7043 - val_loss: 1.1765 - val_acc: 0.5582\n",
      "Epoch 949/1000\n",
      "3784/3784 [==============================] - 1s 225us/step - loss: 0.4566 - acc: 0.7167 - val_loss: 1.1931 - val_acc: 0.5487\n",
      "Epoch 950/1000\n",
      "3784/3784 [==============================] - 1s 223us/step - loss: 0.4560 - acc: 0.7130 - val_loss: 1.1682 - val_acc: 0.5439\n",
      "Epoch 951/1000\n",
      "3784/3784 [==============================] - 1s 223us/step - loss: 0.4590 - acc: 0.7146 - val_loss: 1.1505 - val_acc: 0.5511\n",
      "Epoch 952/1000\n",
      "3784/3784 [==============================] - 1s 225us/step - loss: 0.4536 - acc: 0.7061 - val_loss: 1.1502 - val_acc: 0.5487\n",
      "Epoch 953/1000\n",
      "3784/3784 [==============================] - 1s 225us/step - loss: 0.4599 - acc: 0.7048 - val_loss: 1.1465 - val_acc: 0.5629\n",
      "Epoch 954/1000\n",
      "3784/3784 [==============================] - 1s 224us/step - loss: 0.4541 - acc: 0.7106 - val_loss: 1.1659 - val_acc: 0.5582\n",
      "Epoch 955/1000\n",
      "3784/3784 [==============================] - 1s 226us/step - loss: 0.4563 - acc: 0.7130 - val_loss: 1.1428 - val_acc: 0.5463\n",
      "Epoch 956/1000\n",
      "3784/3784 [==============================] - 1s 225us/step - loss: 0.4451 - acc: 0.7117 - val_loss: 1.1861 - val_acc: 0.5368\n",
      "Epoch 957/1000\n",
      "3784/3784 [==============================] - 1s 226us/step - loss: 0.4586 - acc: 0.7043 - val_loss: 1.1740 - val_acc: 0.5368\n",
      "Epoch 958/1000\n",
      "3784/3784 [==============================] - 1s 224us/step - loss: 0.4545 - acc: 0.7112 - val_loss: 1.1608 - val_acc: 0.5439\n",
      "Epoch 959/1000\n",
      "3784/3784 [==============================] - 1s 224us/step - loss: 0.4503 - acc: 0.7154 - val_loss: 1.1847 - val_acc: 0.5463\n",
      "Epoch 960/1000\n",
      "3784/3784 [==============================] - 1s 224us/step - loss: 0.4567 - acc: 0.7019 - val_loss: 1.1584 - val_acc: 0.5368\n",
      "Epoch 961/1000\n",
      "3784/3784 [==============================] - 1s 223us/step - loss: 0.4631 - acc: 0.7011 - val_loss: 1.1449 - val_acc: 0.5273\n",
      "Epoch 962/1000\n",
      "3784/3784 [==============================] - 1s 224us/step - loss: 0.4629 - acc: 0.7075 - val_loss: 1.1544 - val_acc: 0.5344\n",
      "Epoch 963/1000\n",
      "3784/3784 [==============================] - 1s 225us/step - loss: 0.4624 - acc: 0.7056 - val_loss: 1.1684 - val_acc: 0.5368\n",
      "Epoch 964/1000\n",
      "3784/3784 [==============================] - 1s 225us/step - loss: 0.4562 - acc: 0.7090 - val_loss: 1.1807 - val_acc: 0.5297\n",
      "Epoch 965/1000\n",
      "3784/3784 [==============================] - 1s 225us/step - loss: 0.4469 - acc: 0.7207 - val_loss: 1.2061 - val_acc: 0.5344\n",
      "Epoch 966/1000\n",
      "3784/3784 [==============================] - 1s 234us/step - loss: 0.4504 - acc: 0.7143 - val_loss: 1.1968 - val_acc: 0.5511\n",
      "Epoch 967/1000\n",
      "3784/3784 [==============================] - 1s 224us/step - loss: 0.4527 - acc: 0.7138 - val_loss: 1.1998 - val_acc: 0.5439\n",
      "Epoch 968/1000\n",
      "3784/3784 [==============================] - 1s 226us/step - loss: 0.4454 - acc: 0.7154 - val_loss: 1.2185 - val_acc: 0.5368\n",
      "Epoch 969/1000\n",
      "3784/3784 [==============================] - 1s 226us/step - loss: 0.4523 - acc: 0.7109 - val_loss: 1.1758 - val_acc: 0.5321\n",
      "Epoch 970/1000\n",
      "3784/3784 [==============================] - 1s 227us/step - loss: 0.4513 - acc: 0.7151 - val_loss: 1.1643 - val_acc: 0.5344\n",
      "Epoch 971/1000\n",
      "3784/3784 [==============================] - 1s 224us/step - loss: 0.4524 - acc: 0.7178 - val_loss: 1.1882 - val_acc: 0.5392\n",
      "Epoch 972/1000\n",
      "3784/3784 [==============================] - 1s 228us/step - loss: 0.4452 - acc: 0.7199 - val_loss: 1.1883 - val_acc: 0.5392\n",
      "Epoch 973/1000\n",
      "3784/3784 [==============================] - 1s 224us/step - loss: 0.4583 - acc: 0.7080 - val_loss: 1.1704 - val_acc: 0.5368\n",
      "Epoch 974/1000\n",
      "3784/3784 [==============================] - 1s 226us/step - loss: 0.4495 - acc: 0.7143 - val_loss: 1.1773 - val_acc: 0.5392\n",
      "Epoch 975/1000\n",
      "3784/3784 [==============================] - 1s 225us/step - loss: 0.4639 - acc: 0.7082 - val_loss: 1.1729 - val_acc: 0.5392\n",
      "Epoch 976/1000\n",
      "3784/3784 [==============================] - 1s 225us/step - loss: 0.4583 - acc: 0.7080 - val_loss: 1.1956 - val_acc: 0.5439\n",
      "Epoch 977/1000\n",
      "3784/3784 [==============================] - 1s 226us/step - loss: 0.4529 - acc: 0.7048 - val_loss: 1.1699 - val_acc: 0.5297\n",
      "Epoch 978/1000\n",
      "3784/3784 [==============================] - 1s 226us/step - loss: 0.4438 - acc: 0.7207 - val_loss: 1.1926 - val_acc: 0.5439\n",
      "Epoch 979/1000\n",
      "3784/3784 [==============================] - 1s 227us/step - loss: 0.4445 - acc: 0.7133 - val_loss: 1.1875 - val_acc: 0.5249\n",
      "Epoch 980/1000\n",
      "3784/3784 [==============================] - 1s 223us/step - loss: 0.4644 - acc: 0.7059 - val_loss: 1.1434 - val_acc: 0.5463\n",
      "Epoch 981/1000\n",
      "3784/3784 [==============================] - 1s 225us/step - loss: 0.4432 - acc: 0.7191 - val_loss: 1.1793 - val_acc: 0.5321\n",
      "Epoch 982/1000\n",
      "3784/3784 [==============================] - 1s 226us/step - loss: 0.4534 - acc: 0.7119 - val_loss: 1.1671 - val_acc: 0.5344\n",
      "Epoch 983/1000\n",
      "3784/3784 [==============================] - 1s 224us/step - loss: 0.4491 - acc: 0.7141 - val_loss: 1.1718 - val_acc: 0.5487\n",
      "Epoch 984/1000\n",
      "3784/3784 [==============================] - 1s 225us/step - loss: 0.4563 - acc: 0.7143 - val_loss: 1.1628 - val_acc: 0.5321\n",
      "Epoch 985/1000\n",
      "3784/3784 [==============================] - 1s 224us/step - loss: 0.4434 - acc: 0.7164 - val_loss: 1.1728 - val_acc: 0.5344\n",
      "Epoch 986/1000\n",
      "3784/3784 [==============================] - 1s 224us/step - loss: 0.4424 - acc: 0.7204 - val_loss: 1.1978 - val_acc: 0.5463\n",
      "Epoch 987/1000\n",
      "3784/3784 [==============================] - 1s 224us/step - loss: 0.4502 - acc: 0.7175 - val_loss: 1.1684 - val_acc: 0.5463\n",
      "Epoch 988/1000\n",
      "3784/3784 [==============================] - 1s 225us/step - loss: 0.4483 - acc: 0.7093 - val_loss: 1.1821 - val_acc: 0.5487\n",
      "Epoch 989/1000\n",
      "3784/3784 [==============================] - 1s 224us/step - loss: 0.4537 - acc: 0.7125 - val_loss: 1.1588 - val_acc: 0.5344\n",
      "Epoch 990/1000\n",
      "3784/3784 [==============================] - 1s 225us/step - loss: 0.4356 - acc: 0.7228 - val_loss: 1.1750 - val_acc: 0.5463\n",
      "Epoch 991/1000\n",
      "3784/3784 [==============================] - 1s 222us/step - loss: 0.4402 - acc: 0.7196 - val_loss: 1.1885 - val_acc: 0.5558\n",
      "Epoch 992/1000\n",
      "3784/3784 [==============================] - 1s 225us/step - loss: 0.4499 - acc: 0.7088 - val_loss: 1.1552 - val_acc: 0.5558\n",
      "Epoch 993/1000\n",
      "3784/3784 [==============================] - 1s 232us/step - loss: 0.4533 - acc: 0.7151 - val_loss: 1.1571 - val_acc: 0.5463\n",
      "Epoch 994/1000\n",
      "3784/3784 [==============================] - 1s 224us/step - loss: 0.4430 - acc: 0.7270 - val_loss: 1.1804 - val_acc: 0.5439\n",
      "Epoch 995/1000\n",
      "3784/3784 [==============================] - 1s 226us/step - loss: 0.4579 - acc: 0.7109 - val_loss: 1.1530 - val_acc: 0.5368\n",
      "Epoch 996/1000\n",
      "3784/3784 [==============================] - 1s 224us/step - loss: 0.4498 - acc: 0.7040 - val_loss: 1.1599 - val_acc: 0.5511\n",
      "Epoch 997/1000\n",
      "3784/3784 [==============================] - 1s 227us/step - loss: 0.4540 - acc: 0.7059 - val_loss: 1.1737 - val_acc: 0.5487\n",
      "Epoch 998/1000\n",
      "3784/3784 [==============================] - 1s 226us/step - loss: 0.4528 - acc: 0.7093 - val_loss: 1.1569 - val_acc: 0.5368\n",
      "Epoch 999/1000\n",
      "3784/3784 [==============================] - 1s 225us/step - loss: 0.4513 - acc: 0.7188 - val_loss: 1.1565 - val_acc: 0.5463\n",
      "Epoch 1000/1000\n",
      "3784/3784 [==============================] - 1s 225us/step - loss: 0.4526 - acc: 0.7119 - val_loss: 1.1618 - val_acc: 0.5463\n"
     ]
    }
   ],
   "source": [
    "model.optimizer.lr = 0.00002\n",
    "history = model.fit(train_X, train_y, epochs=1000, batch_size=128, validation_split=0.1, verbose=1, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pyplot.plot(history.history['loss'], label='multi_train')\n",
    "pyplot.plot(history.history['val_loss'], label='multi_test')\n",
    "pyplot.legend()\n",
    "pyplot.yscale('log')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "regession accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "20/20 [==============================] - 0s 900us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.97523295879364014, 0.34999999403953552]"
      ]
     },
     "execution_count": 344,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_X,test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.44908598,  0.55091399],\n",
       "       [ 0.31791157,  0.68208849],\n",
       "       [ 0.9359858 ,  0.0640142 ],\n",
       "       [ 0.50761443,  0.49238551],\n",
       "       [ 0.93432957,  0.06567046],\n",
       "       [ 0.20137239,  0.79862761],\n",
       "       [ 0.80506045,  0.19493958],\n",
       "       [ 0.30424833,  0.69575167],\n",
       "       [ 0.34166884,  0.65833116],\n",
       "       [ 0.42683479,  0.57316518],\n",
       "       [ 0.2518304 ,  0.7481696 ],\n",
       "       [ 0.61474901,  0.38525099],\n",
       "       [ 0.37301657,  0.6269834 ],\n",
       "       [ 0.16675115,  0.83324885],\n",
       "       [ 0.46943262,  0.53056735],\n",
       "       [ 0.3657099 ,  0.6342901 ],\n",
       "       [ 0.49318859,  0.50681144],\n",
       "       [ 0.64690256,  0.35309747],\n",
       "       [ 0.37790051,  0.62209946],\n",
       "       [ 0.65741855,  0.34258151]], dtype=float32)"
      ]
     },
     "execution_count": 345,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.]])"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pred = model.predict(test_X)\n",
    "\n",
    "plt.plot(pred)\n",
    "plt.plot(test_y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
