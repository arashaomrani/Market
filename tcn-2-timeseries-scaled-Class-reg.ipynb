{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from math import sqrt\n",
    "from numpy import concatenate\n",
    "from matplotlib import pyplot\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, Normalizer\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras import optimizers\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.layers import Add, LSTM, Activation, Flatten, Dropout, SimpleRNN, Bidirectional, Conv1D, GRU, Input, BatchNormalization\n",
    "from keras.models import Model\n",
    "from keras.initializers import he_normal\n",
    "import plotly.offline as py\n",
    "import plotly.graph_objs as go\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "py.init_notebook_mode(connected=True)\n",
    "from googlefinance.client import get_price_data, get_prices_data, get_prices_time_data\n",
    "import fix_yahoo_finance as yf\n",
    "from stockstats import StockDataFrame\n",
    "from keras.preprocessing.sequence import TimeseriesGenerator\n",
    "from sklearn.utils import shuffle\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OBV(data):\n",
    "    last_obv = 0\n",
    "    obv = [last_obv]\n",
    "    for i in range(1,len(data)):\n",
    "        if data['Close'][i] >= data['Close'][i-1]:\n",
    "            obv.append(last_obv + data['Volume'][i])\n",
    "        else:\n",
    "            obv.append(last_obv - data['Volume'][i])\n",
    "        last_obv = obv[-1]\n",
    "    return pd.DataFrame(obv, columns=['OBV'])\n",
    "def Bias(data, period=6):\n",
    "    MA = data['Close'].rolling(window=period).mean()\n",
    "    bias=[]\n",
    "    for i in range(len(data)):\n",
    "        bias.append(((data['Close'][i]-MA[i])/MA[i])*100)\n",
    "    return pd.DataFrame(bias, columns=['Bias'])\n",
    "def PSY(data, period=12):\n",
    "    psy = [np.nan]*(period-1)\n",
    "    for i in range(len(data)-period+1):\n",
    "        diff = np.ediff1d(data['Close'][i:i+period])\n",
    "        psy.append((len(diff[diff>=0])/len(diff))*100)\n",
    "    return pd.DataFrame(psy, columns=['PSY'])\n",
    "def SY(data, i, p):\n",
    "    return ((data['Close'][i-p]-data['Close'][i-p-1])/data['Close'][i-p-1])*100\n",
    "\n",
    "def ASY(data, period):\n",
    "    if period == 1:\n",
    "        asy = [np.nan]*2\n",
    "        for i in range(2,len(data)):\n",
    "            asy.append(((data['Close'][i-1]-data['Close'][i-2])/data['Close'][i-2])*100)\n",
    "    else:\n",
    "        asy = [np.nan]*period\n",
    "        for i in range(period,len(data)):\n",
    "            A=0\n",
    "            for j in range(period):\n",
    "                A = A + SY(data, i,j)\n",
    "            A = A/(j+1)\n",
    "            asy.append(A)\n",
    "            \n",
    "    return pd.DataFrame(asy, columns=['ASY'+str(period)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_prepare(Stock):\n",
    "    df=pd.read_csv(Stock+'.csv')\n",
    "    data=df\n",
    "    data['Close'].replace(0, np.nan, inplace=True)\n",
    "    data['Close'].fillna(method='ffill', inplace=True)\n",
    "#     indicators = ['close_50_sma','close_150_sma','close_20_ema','close_40_ema','boll','boll_ub','boll_lb',\\\n",
    "#              'macd','kdjk','kdjd','kdjj','atr','adx','vr','rsi_14']\n",
    "#     for i in indicators:\n",
    "#         df = StockDataFrame.retype(data)\n",
    "#         df = df.get(i)\n",
    "    data.replace([np.inf, -np.inf], np.nan)\n",
    "    data = data.dropna()\n",
    "    #values = data[2:]\n",
    "    values = data[['Open'] + ['High'] + ['Low'] + ['Close'] + ['Volume']]\n",
    "    values.insert(5,'OBV',OBV(data))\n",
    "    values.insert(6,'MA5',data['Close'].rolling(window=5).mean())\n",
    "    values.insert(7,'Bias',Bias(data,6))\n",
    "    values.insert(8,'PSY12',PSY(data,12))\n",
    "    values.insert(9,'ASY1',ASY(data,1))\n",
    "    values.insert(10,'ASY2',ASY(data,2))\n",
    "    values.insert(11,'ASY3',ASY(data,3))\n",
    "    values.insert(12,'ASY4',ASY(data,4))\n",
    "    values.insert(13,'ASY5',ASY(data,5))\n",
    "    diff = np.ediff1d(data['Close'])\n",
    "    diff=np.append(np.array([0]),diff)\n",
    "    values.insert(14,'diff',diff)\n",
    "    values.dropna(inplace=True)\n",
    "    values = values.astype('float32')\n",
    "    values = values.values \n",
    "    #values = values[~np.isnan(values).any(axis=1)]\n",
    "    #scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "    scaler = StandardScaler()\n",
    "    #scaler = Normalizer()\n",
    "    scaler.fit(values)\n",
    "    scaled = scaler.transform(values)\n",
    "    #scaled = normalize(values)\n",
    "    return scaled\n",
    "\n",
    "def data_gen(scaled, seq_len, status1, status2, end):\n",
    "    scaled2=scaled\n",
    "    scaled=scaled[:-end]\n",
    "    if status2 == 'timeseriesgen':\n",
    "        data_gen = TimeseriesGenerator(scaled[:,:-1], scaled[:,-1],\n",
    "                                       length=seq_len, sampling_rate=1,\n",
    "                                       stride=1, batch_size=len(scaled))\n",
    "        X, y = data_gen[0]\n",
    "        print(X.shape, y.shape)\n",
    "#         y = np.ediff1d(scaled[:,3])\n",
    "#         print(y.shape)\n",
    "#         data_gen = TimeseriesGenerator(y, y,\n",
    "#                                        length=seq_len, sampling_rate=1,\n",
    "#                                        stride=1, batch_size=len(y))\n",
    "#         y_seq,_ = data_gen[0]\n",
    "#         print(y_seq.shape)\n",
    "#         y=y_seq\n",
    "#         X = X[:-1]\n",
    "#         print(X.shape, y.shape)\n",
    "#         if status1 == 'classification':\n",
    "#             y1 = np.empty([len(y),y.shape[1]], dtype=np.float32)\n",
    "#             for i in range(len(y)):\n",
    "#                 for j in range(y.shape[1]):\n",
    "#                     if y[i,j] >= 0:\n",
    "#                         y1[i,j] = 1.0\n",
    "#                     else:\n",
    "#                         y1[i,j] = 0.0\n",
    "#             y1=y1[:,-1]\n",
    "#             print(X.shape, y1.shape)\n",
    "#         else:\n",
    "#             y1 = np.empty([len(y),y.shape[1]], dtype=np.float32)\n",
    "#             for i in range(len(y)):\n",
    "#                 for j in range(y.shape[1]):\n",
    "#                     if y[i,j] >= 0:\n",
    "#                         y1[i,j] = 1.0\n",
    "#                     else:\n",
    "#                         y1[i,j] = -1.0\n",
    "#            #y1 = y\n",
    "#             y1=y1[:,-1]\n",
    "#             print(X.shape, y1.shape)\n",
    "#         X, y1 = shuffle(X, y1, random_state = 0)\n",
    "#         train_X = X[:-seq_len]\n",
    "#         train_y = y1[:-seq_len]\n",
    "#         test_X = X[-seq_len:]\n",
    "#         test_y = y1[-seq_len:]\n",
    "        y1=y\n",
    "        scaler = StandardScaler()\n",
    "        train_X = X[:-seq_len]\n",
    "        train_y = y1[:-seq_len]\n",
    "        train_X, train_y = shuffle(train_X, train_y, random_state = 0)\n",
    "        test_X = X[-seq_len:]\n",
    "        test_y = y1[-seq_len:]\n",
    "    else:\n",
    "        X = scaled[-(seq_len+1):-1,:].reshape(1,seq_len,56)\n",
    "        y = scaled[-1,-1]-scaled[-2,-1]\n",
    "        for i in range(1,4127):\n",
    "            X = np.append(X, scaled[-(seq_len+1)-i:-1-i,:].reshape(1,seq_len,56),axis=0)\n",
    "            y = np.append(y, scaled[-1-i,3]-scaled[-2-i,3])\n",
    "        if status1 == 'classification':\n",
    "            y1 = np.empty([len(y)], dtype=np.float32)\n",
    "            for i in range(len(y)):\n",
    "                if y[i] >= 0.0:\n",
    "                    y1[i] = 1.0\n",
    "                else:\n",
    "                    y1[i] = 0.0\n",
    "            print(X.shape,y.shape)\n",
    "            scaler = StandardScaler()\n",
    "        else:\n",
    "            y1 = np.empty([len(y)], dtype=np.float32)\n",
    "            for i in range(len(y)):\n",
    "                if y[i] >= 0.0:\n",
    "                    y1[i] = 1.0\n",
    "                else:\n",
    "                    y1[i] = -1.0\n",
    "            #y1 = y\n",
    "            #print(X.shape, y1.shape)\n",
    "            #scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "            #y1=y\n",
    "            scaler = StandardScaler()\n",
    "            #scaler = Normalizer()\n",
    "            #scaler.fit(y1.reshape(-1,1))\n",
    "            #y1 = scaler.transform(y1.reshape(-1,1))\n",
    "            #y1 = normalize(y1)\n",
    "        X, y1 = shuffle(X, y1, random_state = 0)\n",
    "        train_X = X\n",
    "        train_y = y1\n",
    "        test_X = X[-seq_len:]\n",
    "        test_y = y1[-seq_len:]\n",
    "    return scaler, scaled2, scaled, train_X, train_y, test_X, test_y\n",
    "\n",
    "def tcn(h,filters=400, kernel_size=3, padding='causal', activation='relu', dilation_rate=1):\n",
    "    main = BatchNormalization()(h)\n",
    "    main = h\n",
    "    init = he_normal(seed=1)\n",
    "    for i in range(2):\n",
    "        main = Conv1D(filters=filters,kernel_size=kernel_size,padding=padding, dilation_rate=dilation_rate, kernel_initializer=init)(main)\n",
    "        main = Activation('relu')(main)\n",
    "        main = BatchNormalization()(main)\n",
    "        main = Dropout(0.15)(main)\n",
    "    side_path = Conv1D(filters=filters,kernel_size=1, padding='same', kernel_initializer=init)(h)\n",
    "    side_path = BatchNormalization()(side_path)\n",
    "    return Add()([main,side_path])\n",
    "\n",
    "def MODEL(status):\n",
    "    # status: either \"classification\" or \"regression\"\n",
    "    Inp = Input(shape=(15,14))\n",
    "    inp = Inp\n",
    "    D = [1,1,1,1,1]\n",
    "    for i in range(3):\n",
    "        inp=tcn(inp,dilation_rate=D[i])\n",
    "        inp = BatchNormalization()(inp)\n",
    "        #inp = Activation('relu')(inp)\n",
    "    inp=Flatten()(inp)\n",
    "    init = he_normal(seed=1)\n",
    "    inp=Dense(100, activation='selu',kernel_initializer=init)(inp)\n",
    "    inp = BatchNormalization()(inp)\n",
    "    #inp = Activation('selu')(inp)\n",
    "    inp = Dropout(0.45)(inp)\n",
    "    inp=Dense(20, activation='selu', kernel_initializer=init)(inp)\n",
    "    inp = BatchNormalization()(inp)\n",
    "    #inp = Activation('selu')(inp)\n",
    "    inp = Dropout(0.25)(inp)\n",
    "    if status == 'classification':\n",
    "        out=Dense(1,activation='sigmoid', kernel_initializer=init)(inp)\n",
    "    else:\n",
    "        out=Dense(1)(inp)\n",
    "    model = Model(Inp,out)\n",
    "    return model\n",
    "\n",
    "def train_model(model, train_X, train_y, status):\n",
    "    ad = optimizers.Adam(lr=0.0008)\n",
    "    if status == 'classification':\n",
    "        model.compile(loss='binary_crossentropy', optimizer=ad)\n",
    "    else: \n",
    "        model.compile(loss='mse', optimizer=ad)\n",
    "    history = model.fit(train_X, train_y, epochs=10, batch_size=128, verbose=1, validation_split=0.1, shuffle=False)\n",
    "    return model, history\n",
    "\n",
    "def plot_loss(history):\n",
    "    pyplot.plot(history.history['loss'], label='multi_train')\n",
    "    pyplot.plot(history.history['val_loss'], label='multi_test')\n",
    "    pyplot.legend()\n",
    "    pyplot.yscale('log')\n",
    "    pyplot.show()\n",
    "\n",
    "def acc(model, scaled2, status, seq_len, end, scaler):\n",
    "    pred = model.predict(scaled2[-(seq_len+end):-end,:-1].reshape(1,seq_len,14))[0][-1]\n",
    "    #y_test = scaled2[-end,3]-scaled2[-end-1,3]\n",
    "    y_test = scaled2[-end,-1]\n",
    "    #pred = scaler.inverse_transform(pred)\n",
    "    #y_test = scaler.transform(y_test)\n",
    "    print('prediction', pred, 'y_test', y_test)\n",
    "#     for i in range(1,seq_len):\n",
    "#         pred = np.append(pred, np.array([model.predict(scaled2[-(seq_len+1)-i:-1-i,:].reshape(1,seq_len,56))[0][-1]]))\n",
    "#         y_test = np.append(y_test, scaled2[-1-i,3]-scaled2[-2-i,3])\n",
    "    p=np.empty(1) \n",
    "    if status == \"classification\":\n",
    "        T = 0.5\n",
    "    else:\n",
    "        T = 0.0\n",
    "    for i in range(1):\n",
    "        if pred >= T:\n",
    "            p = +1.0\n",
    "        else:\n",
    "            p = -1.0\n",
    "    mul = np.multiply(p, y_test)\n",
    "    return mul #len(mul[mul>=0])/len(mul)\n",
    "\n",
    "def test_acc(model, test_X, test_y, status):\n",
    "    pred = model.predict(test_X)\n",
    "    p=np.empty([len(pred)])\n",
    "    y_test=np.empty([len(pred)])\n",
    "    if status == \"classification\":\n",
    "        T = 0.5\n",
    "    else:\n",
    "        T = 0.0\n",
    "    for i in range(len(pred)):\n",
    "        if pred[i,-1] >= T:\n",
    "            p[i] = +1.0\n",
    "        else:\n",
    "            p[i] = -1.0\n",
    "        if test_y[i] >= T:\n",
    "            y_test[i] = 1.0\n",
    "        else:\n",
    "            y_test[i] = -1.0\n",
    "    mul = np.multiply(p, y_test)\n",
    "    return len(mul[mul>=0])/len(mul)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aalahgholipour160413\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:27: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4251, 15, 14) (4251,)\n",
      "Train on 3812 samples, validate on 424 samples\n",
      "Epoch 1/10\n",
      "3812/3812 [==============================] - 20s 5ms/step - loss: 2.4169 - val_loss: 1.2166\n",
      "Epoch 2/10\n",
      "3812/3812 [==============================] - 4s 964us/step - loss: 1.7248 - val_loss: 1.0079\n",
      "Epoch 3/10\n",
      "3812/3812 [==============================] - 4s 975us/step - loss: 1.6437 - val_loss: 1.0189\n",
      "Epoch 4/10\n",
      "3812/3812 [==============================] - 4s 978us/step - loss: 1.5864 - val_loss: 0.9730\n",
      "Epoch 5/10\n",
      "3812/3812 [==============================] - 4s 962us/step - loss: 1.5262 - val_loss: 0.9948\n",
      "Epoch 6/10\n",
      "3812/3812 [==============================] - 4s 982us/step - loss: 1.4409 - val_loss: 0.9487\n",
      "Epoch 7/10\n",
      "3812/3812 [==============================] - 4s 974us/step - loss: 1.3636 - val_loss: 0.9684\n",
      "Epoch 8/10\n",
      "3812/3812 [==============================] - 4s 964us/step - loss: 1.3298 - val_loss: 0.9450\n",
      "Epoch 9/10\n",
      "3812/3812 [==============================] - 4s 981us/step - loss: 1.2796 - val_loss: 0.9136\n",
      "Epoch 10/10\n",
      "3812/3812 [==============================] - 4s 966us/step - loss: 1.2244 - val_loss: 0.9272\n",
      "prediction -0.181673 y_test 0.173054\n",
      "prediction -0.170976 y_test 0.817457\n",
      "prediction -0.116895 y_test -0.745228\n",
      "prediction -0.0857316 y_test 2.52514\n",
      "prediction -0.126071 y_test 0.398595\n",
      "prediction -0.115948 y_test -0.390805\n",
      "prediction -0.0342702 y_test -0.00416002\n",
      "prediction -0.0355422 y_test 0.140832\n",
      "prediction -0.0895276 y_test 0.318043\n",
      "prediction -0.104528 y_test -0.455244\n",
      "prediction -0.0756249 y_test -0.503575\n",
      "prediction -0.0227058 y_test 0.173051\n",
      "prediction -0.0190972 y_test -0.439137\n",
      "prediction -0.1011 y_test 0.301936\n",
      "prediction -0.0245304 y_test 0.366373\n",
      "prediction 0.00836777 y_test 0.0119502\n",
      "prediction -0.0228647 y_test 0.269714\n",
      "prediction -0.0333787 y_test 0.124722\n",
      "prediction -0.0192822 y_test -0.326361\n",
      "(4246, 15, 14) (4246,)\n",
      "Train on 3807 samples, validate on 424 samples\n",
      "Epoch 1/10\n",
      "3807/3807 [==============================] - 22s 6ms/step - loss: 2.4527 - val_loss: 1.2927\n",
      "Epoch 2/10\n",
      "3807/3807 [==============================] - 4s 974us/step - loss: 1.7188 - val_loss: 1.1220\n",
      "Epoch 3/10\n",
      "3807/3807 [==============================] - 4s 992us/step - loss: 1.6597 - val_loss: 1.0817\n",
      "Epoch 4/10\n",
      "3807/3807 [==============================] - 4s 990us/step - loss: 1.4846 - val_loss: 1.1241\n",
      "Epoch 5/10\n",
      "3807/3807 [==============================] - 4s 983us/step - loss: 1.4856 - val_loss: 1.0756\n",
      "Epoch 6/10\n",
      "3807/3807 [==============================] - 4s 986us/step - loss: 1.4067 - val_loss: 1.0867\n",
      "Epoch 7/10\n",
      "3807/3807 [==============================] - 4s 982us/step - loss: 1.3592 - val_loss: 1.0792\n",
      "Epoch 8/10\n",
      "3807/3807 [==============================] - 4s 984us/step - loss: 1.3135 - val_loss: 1.0822\n",
      "Epoch 9/10\n",
      "3807/3807 [==============================] - 4s 986us/step - loss: 1.2765 - val_loss: 1.0746\n",
      "Epoch 10/10\n",
      "3807/3807 [==============================] - 4s 988us/step - loss: 1.2373 - val_loss: 1.0814\n",
      "prediction 0.0580082 y_test -1.29085\n",
      "prediction 0.0487478 y_test 1.56138\n",
      "prediction -0.0435797 y_test -1.41763\n",
      "prediction 0.0221863 y_test -0.910554\n",
      "prediction -0.0566454 y_test -0.276723\n",
      "prediction 0.00958822 y_test -0.0231966\n",
      "prediction 0.0159994 y_test -0.530262\n",
      "prediction 0.0150421 y_test 0.864161\n",
      "prediction -0.10148 y_test 1.11771\n",
      "prediction -0.0962683 y_test -5.22061\n",
      "prediction -0.126963 y_test 0.483869\n",
      "prediction -0.0910649 y_test 0.420479\n",
      "prediction -0.0712691 y_test 1.30785\n",
      "prediction -0.257973 y_test 1.49799\n",
      "prediction -0.215099 y_test 0.990934\n",
      "prediction -0.337031 y_test 1.81492\n",
      "prediction -0.458901 y_test 1.62476\n",
      "prediction -0.249867 y_test -0.657028\n",
      "prediction -0.13011 y_test 3.84318\n",
      "(4189, 15, 14) (4189,)\n",
      "Train on 3756 samples, validate on 418 samples\n",
      "Epoch 1/10\n",
      "3756/3756 [==============================] - 22s 6ms/step - loss: 2.4864 - val_loss: 1.0719\n",
      "Epoch 2/10\n",
      "3756/3756 [==============================] - 4s 984us/step - loss: 1.7816 - val_loss: 0.8556\n",
      "Epoch 3/10\n",
      "3756/3756 [==============================] - 4s 995us/step - loss: 1.7247 - val_loss: 0.8657\n",
      "Epoch 4/10\n",
      "3756/3756 [==============================] - 4s 995us/step - loss: 1.6019 - val_loss: 0.8271\n",
      "Epoch 5/10\n",
      "3756/3756 [==============================] - 4s 987us/step - loss: 1.6227 - val_loss: 0.8299\n",
      "Epoch 6/10\n",
      "3756/3756 [==============================] - 4s 993us/step - loss: 1.5054 - val_loss: 0.8260\n",
      "Epoch 7/10\n",
      "3756/3756 [==============================] - 4s 991us/step - loss: 1.4527 - val_loss: 0.8227\n",
      "Epoch 8/10\n",
      "3756/3756 [==============================] - 4s 1ms/step - loss: 1.3624 - val_loss: 0.8180\n",
      "Epoch 9/10\n",
      "3756/3756 [==============================] - 4s 999us/step - loss: 1.3106 - val_loss: 0.8153\n",
      "Epoch 10/10\n",
      "3756/3756 [==============================] - 4s 995us/step - loss: 1.2914 - val_loss: 0.8162\n",
      "prediction 0.170427 y_test 0.555273\n",
      "prediction 0.282031 y_test -0.807345\n",
      "prediction 0.0843902 y_test -0.568103\n",
      "prediction 0.0686436 y_test 1.21057\n",
      "prediction 0.255715 y_test 0.035183\n",
      "prediction 0.0877526 y_test -0.235252\n",
      "prediction 0.256064 y_test -1.40023\n",
      "prediction 0.381698 y_test 8.05482\n",
      "prediction 0.297023 y_test -0.29766\n",
      "prediction 0.0846528 y_test 0.648882\n",
      "prediction 0.126747 y_test -0.817739\n",
      "prediction 0.108026 y_test 0.690481\n",
      "prediction 0.20329 y_test 1.11696\n",
      "prediction 0.15954 y_test 0.420045\n",
      "prediction -0.153194 y_test 1.73065\n",
      "prediction 0.071476 y_test 0.440853\n",
      "prediction 0.107068 y_test 1.23137\n",
      "prediction 0.0178619 y_test -1.24421\n",
      "prediction 0.0789351 y_test -0.100026\n",
      "(4246, 15, 14) (4246,)\n",
      "Train on 3807 samples, validate on 424 samples\n",
      "Epoch 1/10\n",
      "3807/3807 [==============================] - 22s 6ms/step - loss: 1.8901 - val_loss: 1.2638\n",
      "Epoch 2/10\n",
      "3807/3807 [==============================] - 4s 986us/step - loss: 1.4932 - val_loss: 1.0150\n",
      "Epoch 3/10\n",
      "3807/3807 [==============================] - 4s 996us/step - loss: 1.4299 - val_loss: 1.0530\n",
      "Epoch 4/10\n",
      "3807/3807 [==============================] - 4s 990us/step - loss: 1.3447 - val_loss: 1.0591\n",
      "Epoch 5/10\n",
      "3807/3807 [==============================] - 4s 1ms/step - loss: 1.2909 - val_loss: 1.0178\n",
      "Epoch 6/10\n",
      "3807/3807 [==============================] - 4s 982us/step - loss: 1.2479 - val_loss: 1.0201\n",
      "Epoch 7/10\n",
      "3807/3807 [==============================] - 4s 1ms/step - loss: 1.2053 - val_loss: 1.0432\n",
      "Epoch 8/10\n",
      "3807/3807 [==============================] - 4s 982us/step - loss: 1.1823 - val_loss: 1.0415\n",
      "Epoch 9/10\n",
      "3807/3807 [==============================] - 4s 996us/step - loss: 1.1113 - val_loss: 1.0578\n",
      "Epoch 10/10\n",
      "3807/3807 [==============================] - 4s 986us/step - loss: 1.1248 - val_loss: 1.0413\n",
      "prediction -0.00858578 y_test 0.863849\n",
      "prediction -0.350429 y_test 1.72029\n",
      "prediction -0.178884 y_test -1.58792\n",
      "prediction -0.244563 y_test 0.964601\n",
      "prediction -0.242016 y_test -1.33602\n",
      "prediction 0.00640996 y_test 0.108171\n",
      "prediction 0.0649225 y_test 0.292895\n",
      "prediction 0.221278 y_test -2.15889\n",
      "prediction -0.364457 y_test -0.781853\n",
      "prediction -0.310487 y_test 0.830256\n",
      "prediction -0.243321 y_test 1.33406\n",
      "prediction -0.0549569 y_test 1.60274\n",
      "prediction -0.256065 y_test 1.11575\n",
      "prediction -0.112688 y_test 1.14933\n",
      "prediction -0.286149 y_test -0.05977\n",
      "prediction -0.108473 y_test -1.94057\n",
      "prediction -0.13909 y_test -1.80622\n",
      "prediction -0.23489 y_test 0.511199\n",
      "prediction -0.211817 y_test -0.278072\n",
      "(4251, 15, 14) (4251,)\n",
      "Train on 3812 samples, validate on 424 samples\n",
      "Epoch 1/10\n",
      "3812/3812 [==============================] - 23s 6ms/step - loss: 2.2762 - val_loss: 0.8934\n",
      "Epoch 2/10\n",
      "3812/3812 [==============================] - 4s 995us/step - loss: 1.7267 - val_loss: 0.6806\n",
      "Epoch 3/10\n",
      "3812/3812 [==============================] - 4s 999us/step - loss: 1.7141 - val_loss: 0.6399\n",
      "Epoch 4/10\n",
      "3812/3812 [==============================] - 4s 995us/step - loss: 1.5680 - val_loss: 0.6471\n",
      "Epoch 5/10\n",
      "3812/3812 [==============================] - 4s 986us/step - loss: 1.4638 - val_loss: 0.6143\n",
      "Epoch 6/10\n",
      "3812/3812 [==============================] - 4s 1ms/step - loss: 1.4524 - val_loss: 0.6025\n",
      "Epoch 7/10\n",
      "3812/3812 [==============================] - 4s 994us/step - loss: 1.4089 - val_loss: 0.5953\n",
      "Epoch 8/10\n",
      "3812/3812 [==============================] - 4s 992us/step - loss: 1.3497 - val_loss: 0.5843\n",
      "Epoch 9/10\n",
      "3812/3812 [==============================] - 4s 994us/step - loss: 1.2898 - val_loss: 0.5948\n",
      "Epoch 10/10\n",
      "3812/3812 [==============================] - 4s 1ms/step - loss: 1.2903 - val_loss: 0.5957\n",
      "prediction 0.154427 y_test -1.00273\n",
      "prediction 0.186165 y_test 0.345989\n",
      "prediction 0.30126 y_test 0.524498\n",
      "prediction 0.281763 y_test -0.0903635\n",
      "prediction 0.248391 y_test -0.645716\n",
      "prediction 0.20013 y_test 0.583996\n",
      "prediction 0.150311 y_test -0.249032\n",
      "prediction 0.111487 y_test -0.24904\n",
      "prediction 0.107607 y_test 0.921182\n",
      "prediction 0.132957 y_test -0.48704\n",
      "prediction -0.0398307 y_test -1.06224\n",
      "prediction -0.0110808 y_test 0.484828\n",
      "prediction -0.0205184 y_test 0.445159\n",
      "prediction 0.0845436 y_test 1.75421\n",
      "prediction 0.168011 y_test 0.0286451\n",
      "prediction 0.149847 y_test 0.20715\n",
      "prediction 0.2088 y_test 0.187322\n",
      "prediction 0.0215229 y_test 0.564158\n",
      "prediction 0.0261478 y_test -0.665547\n",
      "(4246, 15, 14) (4246,)\n",
      "Train on 3807 samples, validate on 424 samples\n",
      "Epoch 1/10\n",
      "3807/3807 [==============================] - 24s 6ms/step - loss: 2.4951 - val_loss: 1.2334\n",
      "Epoch 2/10\n",
      "3807/3807 [==============================] - 4s 1ms/step - loss: 1.9717 - val_loss: 0.9999\n",
      "Epoch 3/10\n",
      "3807/3807 [==============================] - 4s 1ms/step - loss: 1.8381 - val_loss: 1.0118\n",
      "Epoch 4/10\n",
      "3807/3807 [==============================] - 4s 1ms/step - loss: 1.7869 - val_loss: 0.9071\n",
      "Epoch 5/10\n",
      "3807/3807 [==============================] - 4s 1ms/step - loss: 1.6488 - val_loss: 0.9261\n",
      "Epoch 6/10\n",
      "3807/3807 [==============================] - 4s 992us/step - loss: 1.6761 - val_loss: 0.9121\n",
      "Epoch 7/10\n",
      "3807/3807 [==============================] - 4s 1ms/step - loss: 1.4905 - val_loss: 0.8909\n",
      "Epoch 8/10\n",
      "3807/3807 [==============================] - 4s 994us/step - loss: 1.4429 - val_loss: 0.8969\n",
      "Epoch 9/10\n",
      "3807/3807 [==============================] - 4s 1ms/step - loss: 1.3798 - val_loss: 0.9200\n",
      "Epoch 10/10\n",
      "3807/3807 [==============================] - 4s 996us/step - loss: 1.4024 - val_loss: 0.9293\n",
      "prediction -0.195564 y_test -1.63869\n",
      "prediction -0.122324 y_test 2.66275\n",
      "prediction -0.118021 y_test 0.577031\n",
      "prediction -0.0773095 y_test -1.11868\n",
      "prediction -0.30093 y_test -0.253858\n",
      "prediction -0.29121 y_test -0.316043\n",
      "prediction -0.226857 y_test 0.848342\n",
      "prediction -0.243941 y_test 0.12484\n",
      "prediction -0.290532 y_test 0.175719\n",
      "prediction -0.247779 y_test 1.07444\n",
      "prediction -0.239662 y_test 0.588338\n",
      "prediction -0.252781 y_test 1.04617\n",
      "prediction -0.188839 y_test -0.169076\n",
      "prediction -0.0609007 y_test 0.0852787\n",
      "prediction -0.154634 y_test -0.197339\n",
      "prediction 0.0240745 y_test 1.44184\n",
      "prediction 0.0647322 y_test -0.909542\n",
      "prediction 0.0380471 y_test 0.158762\n",
      "prediction 0.0605156 y_test -1.4126\n",
      "(4246, 15, 14) (4246,)\n",
      "Train on 3807 samples, validate on 424 samples\n",
      "Epoch 1/10\n",
      "3807/3807 [==============================] - 28s 7ms/step - loss: 1.8118 - val_loss: 1.1302\n",
      "Epoch 2/10\n",
      "3807/3807 [==============================] - 4s 1ms/step - loss: 1.4655 - val_loss: 1.0616\n",
      "Epoch 3/10\n",
      "3807/3807 [==============================] - 4s 1ms/step - loss: 1.4571 - val_loss: 1.0695\n",
      "Epoch 4/10\n",
      "3807/3807 [==============================] - 4s 1ms/step - loss: 1.3208 - val_loss: 1.0652\n",
      "Epoch 5/10\n",
      "3807/3807 [==============================] - 4s 1ms/step - loss: 1.2626 - val_loss: 1.0597\n",
      "Epoch 6/10\n",
      "3807/3807 [==============================] - 4s 1ms/step - loss: 1.2932 - val_loss: 1.0587\n",
      "Epoch 7/10\n",
      "3807/3807 [==============================] - 4s 1ms/step - loss: 1.2233 - val_loss: 1.0623\n",
      "Epoch 8/10\n",
      "3807/3807 [==============================] - 4s 1ms/step - loss: 1.1602 - val_loss: 1.0549\n",
      "Epoch 9/10\n",
      "3807/3807 [==============================] - 4s 1ms/step - loss: 1.1468 - val_loss: 1.0688\n",
      "Epoch 10/10\n",
      "3807/3807 [==============================] - 4s 1ms/step - loss: 1.1314 - val_loss: 1.0707\n",
      "prediction -0.146198 y_test 0.0150723\n",
      "prediction -0.137203 y_test 1.26874\n",
      "prediction -0.153693 y_test 0.383793\n",
      "prediction -0.187856 y_test 1.48997\n",
      "prediction -0.191436 y_test -0.796127\n",
      "prediction -0.158363 y_test 0.752512\n",
      "prediction -0.197674 y_test -0.0340912\n",
      "prediction -0.141895 y_test -2.14813\n",
      "prediction -0.110029 y_test -0.378235\n",
      "prediction -0.128087 y_test 0.752524\n",
      "prediction -0.126689 y_test 0.97375\n",
      "prediction -0.0786548 y_test 2.00619\n",
      "prediction -0.0878459 y_test -0.23075\n",
      "prediction -0.111537 y_test -0.329069\n",
      "prediction -0.0981298 y_test 0.678772\n",
      "prediction -0.0420927 y_test -0.624058\n",
      "prediction -0.0773727 y_test -1.70566\n",
      "prediction -0.122972 y_test 0.580452\n",
      "prediction -0.113174 y_test -0.501151\n",
      "(4246, 15, 14) (4246,)\n",
      "Train on 3807 samples, validate on 424 samples\n",
      "Epoch 1/10\n",
      "3807/3807 [==============================] - 31s 8ms/step - loss: 2.5483 - val_loss: 0.7395\n",
      "Epoch 2/10\n",
      "3807/3807 [==============================] - 4s 1ms/step - loss: 1.9470 - val_loss: 0.6769\n",
      "Epoch 3/10\n",
      "3807/3807 [==============================] - 4s 1ms/step - loss: 1.8819 - val_loss: 0.6237\n",
      "Epoch 4/10\n",
      "3807/3807 [==============================] - 4s 1ms/step - loss: 1.7028 - val_loss: 0.6540\n",
      "Epoch 5/10\n",
      "3807/3807 [==============================] - 4s 1ms/step - loss: 1.5401 - val_loss: 0.6021\n",
      "Epoch 6/10\n",
      "3807/3807 [==============================] - 4s 1ms/step - loss: 1.5520 - val_loss: 0.6312\n",
      "Epoch 7/10\n",
      "3807/3807 [==============================] - 4s 1ms/step - loss: 1.4981 - val_loss: 0.6271\n",
      "Epoch 8/10\n",
      "3807/3807 [==============================] - 4s 1ms/step - loss: 1.4841 - val_loss: 0.5882\n",
      "Epoch 9/10\n",
      "3807/3807 [==============================] - 4s 1ms/step - loss: 1.3487 - val_loss: 0.5760\n",
      "Epoch 10/10\n",
      "3807/3807 [==============================] - 4s 1ms/step - loss: 1.4033 - val_loss: 0.5889\n",
      "prediction -0.129619 y_test 0.67767\n",
      "prediction -0.152597 y_test -0.394633\n",
      "prediction -0.138662 y_test 0.320234\n",
      "prediction -0.140124 y_test -0.849557\n",
      "prediction -0.116722 y_test -0.405467\n",
      "prediction -0.118618 y_test -1.71607\n",
      "prediction -0.165154 y_test -0.145516\n",
      "prediction -0.113989 y_test 0.00612287\n",
      "prediction -0.0741723 y_test 0.0819501\n",
      "prediction -0.00886677 y_test 1.37088\n",
      "prediction -0.0118034 y_test 1.13259\n",
      "prediction -0.0336715 y_test 2.18323\n",
      "prediction -0.0716846 y_test -0.13469\n",
      "prediction -0.0261136 y_test -0.979529\n",
      "prediction -0.0852442 y_test -1.16367\n",
      "prediction -0.231824 y_test 3.40718\n",
      "prediction -0.452978 y_test -1.61858\n",
      "prediction -0.429572 y_test -0.15635\n",
      "prediction -0.18604 y_test -2.2143\n",
      "(4255, 15, 14) (4255,)\n",
      "Train on 3816 samples, validate on 424 samples\n",
      "Epoch 1/10\n",
      "3816/3816 [==============================] - 30s 8ms/step - loss: 1.4986 - val_loss: 1.3581\n",
      "Epoch 2/10\n",
      "3816/3816 [==============================] - 4s 1ms/step - loss: 1.2649 - val_loss: 1.1038\n",
      "Epoch 3/10\n",
      "3816/3816 [==============================] - 4s 1ms/step - loss: 1.2225 - val_loss: 1.0660\n",
      "Epoch 4/10\n",
      "3816/3816 [==============================] - 4s 1ms/step - loss: 1.1990 - val_loss: 1.0558\n",
      "Epoch 5/10\n",
      "3816/3816 [==============================] - 4s 1ms/step - loss: 1.1500 - val_loss: 1.0340\n",
      "Epoch 6/10\n",
      "3816/3816 [==============================] - 4s 1ms/step - loss: 1.0935 - val_loss: 1.0338\n",
      "Epoch 7/10\n",
      "3816/3816 [==============================] - 4s 1ms/step - loss: 1.0977 - val_loss: 1.0527\n",
      "Epoch 8/10\n",
      "3816/3816 [==============================] - 4s 1ms/step - loss: 1.0574 - val_loss: 1.0407\n",
      "Epoch 9/10\n",
      "3816/3816 [==============================] - 4s 1ms/step - loss: 1.0409 - val_loss: 1.0553\n",
      "Epoch 10/10\n",
      "3816/3816 [==============================] - 4s 1ms/step - loss: 1.0310 - val_loss: 1.0543\n",
      "prediction 0.137103 y_test 5.69313\n",
      "prediction 0.120554 y_test 10.1642\n",
      "prediction 0.0897919 y_test 0.302293\n",
      "prediction 0.258697 y_test -1.01808\n",
      "prediction 0.147626 y_test -2.98501\n",
      "prediction 0.118208 y_test -0.599202\n",
      "prediction 0.012291 y_test 1.61359\n",
      "prediction -0.175275 y_test 1.22202\n",
      "prediction 0.0976738 y_test 0.111074\n",
      "prediction -0.0417824 y_test -0.444401\n",
      "prediction 0.0312503 y_test 1.30399\n",
      "prediction -0.0412027 y_test -0.999874\n",
      "prediction -0.10655 y_test 0.447995\n",
      "prediction -0.0199993 y_test -0.426184\n",
      "prediction 0.0266908 y_test 0.229457\n",
      "prediction 0.147187 y_test 2.82469\n",
      "prediction 0.14207 y_test -2.29294\n",
      "prediction 0.136285 y_test -0.253166\n",
      "prediction 0.183453 y_test 2.33297\n",
      "(4246, 15, 14) (4246,)\n",
      "Train on 3807 samples, validate on 424 samples\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3807/3807 [==============================] - 28s 7ms/step - loss: 1.9620 - val_loss: 1.0091\n",
      "Epoch 2/10\n",
      "3807/3807 [==============================] - 4s 1ms/step - loss: 1.5325 - val_loss: 0.9835\n",
      "Epoch 3/10\n",
      "3807/3807 [==============================] - 4s 1ms/step - loss: 1.5043 - val_loss: 0.9034\n",
      "Epoch 4/10\n",
      "3807/3807 [==============================] - 4s 1ms/step - loss: 1.3898 - val_loss: 0.8902\n",
      "Epoch 5/10\n",
      "3807/3807 [==============================] - 4s 1ms/step - loss: 1.3764 - val_loss: 0.8993\n",
      "Epoch 6/10\n",
      "3807/3807 [==============================] - 4s 1ms/step - loss: 1.3239 - val_loss: 0.9011\n",
      "Epoch 7/10\n",
      "3807/3807 [==============================] - 4s 1ms/step - loss: 1.2658 - val_loss: 0.9068\n",
      "Epoch 8/10\n",
      "3807/3807 [==============================] - 4s 1ms/step - loss: 1.2169 - val_loss: 0.8921\n",
      "Epoch 9/10\n",
      "3807/3807 [==============================] - 4s 1ms/step - loss: 1.1806 - val_loss: 0.8927\n",
      "Epoch 10/10\n",
      "3807/3807 [==============================] - 4s 1ms/step - loss: 1.1591 - val_loss: 0.8815\n",
      "prediction 0.00303477 y_test -0.808344\n",
      "prediction 0.0370817 y_test -1.29114\n",
      "prediction 0.0385888 y_test -0.219335\n",
      "prediction -0.026118 y_test 0.524164\n",
      "prediction 0.0467333 y_test -1.06905\n",
      "prediction 0.122513 y_test 0.630379\n",
      "prediction 0.134096 y_test 0.62072\n",
      "prediction 0.164405 y_test -1.84152\n",
      "prediction 0.130349 y_test 0.369674\n",
      "prediction 0.0778833 y_test 1.67321\n",
      "prediction 0.0742503 y_test 0.437264\n",
      "prediction 0.132543 y_test 0.553129\n",
      "prediction 0.148317 y_test -1.1463\n",
      "prediction 0.236207 y_test 0.591754\n",
      "prediction 0.212073 y_test 0.446922\n",
      "prediction 0.106091 y_test -0.373833\n",
      "prediction 0.116783 y_test -1.18492\n",
      "prediction 0.169522 y_test 0.350365\n",
      "prediction 0.328974 y_test -1.51322\n"
     ]
    }
   ],
   "source": [
    "stocks=['OI','NI','UPS','HSIC','CCE','IBM','MAS','EFX','AAPL','WDC']\n",
    "status1='classification'\n",
    "status2='timeseriesgen'\n",
    "accuracy=np.empty((19,0))\n",
    "\n",
    "\n",
    "test_accuracy=pd.DataFrame()\n",
    "for Stock in stocks:\n",
    "    Mul=np.empty((0,1))\n",
    "    scaled = data_prepare(Stock)\n",
    "    scaler, scaled2, scaled, train_X, train_y, test_X, test_y = data_gen(scaled, 15, status1, status2, 20)\n",
    "    model = MODEL(status1)\n",
    "    #model.load_weights(Stock+status2+'_'+status1+'.h5')\n",
    "    model, history = train_model(model, train_X, train_y,status1)\n",
    "    #plot_loss(history)\n",
    "    model.save_weights(Stock+status2+'_'+status1+'.h5')\n",
    "    for end in range(1,20):\n",
    "        mul = acc(model, scaled2,status1, 15, end, scaler)\n",
    "        Mul = np.append(Mul,[[mul]],axis=0)\n",
    "    accuracy = np.append(accuracy, Mul, axis=1)\n",
    "    #model_test_accuracy = test_acc(model, test_X, test_y,status1)\n",
    "    #print(Stock+' accuracy', mul)\n",
    "    #print(Stock +' test_accuracy', model_test_accuracy) \n",
    "    #test_accuracy = test_accuracy.append(pd.DataFrame([[Stock, model_test_accuracy]]),ignore_index=True)\n",
    "#Mul=np.array(Mul)\n",
    "#accuracy = accuracy.append(pd.DataFrame([[str(end), len(Mul[Mul>=0])/len(Mul)]]),ignore_index=True)\n",
    "#accuracy.columns=['Stock','accuracy']\n",
    "#test_accuracy.columns=['Stock','test_accuracy']\n",
    "Accuracy = pd.DataFrame(accuracy)\n",
    "Accuracy.columns = stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OI</th>\n",
       "      <th>NI</th>\n",
       "      <th>UPS</th>\n",
       "      <th>HSIC</th>\n",
       "      <th>CCE</th>\n",
       "      <th>IBM</th>\n",
       "      <th>MAS</th>\n",
       "      <th>EFX</th>\n",
       "      <th>AAPL</th>\n",
       "      <th>WDC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.173054</td>\n",
       "      <td>-1.290853</td>\n",
       "      <td>0.555273</td>\n",
       "      <td>-0.863849</td>\n",
       "      <td>-1.002730</td>\n",
       "      <td>1.638692</td>\n",
       "      <td>-0.015072</td>\n",
       "      <td>-0.677670</td>\n",
       "      <td>5.693126</td>\n",
       "      <td>-0.808344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.817457</td>\n",
       "      <td>1.561382</td>\n",
       "      <td>-0.807345</td>\n",
       "      <td>-1.720293</td>\n",
       "      <td>0.345989</td>\n",
       "      <td>-2.662752</td>\n",
       "      <td>-1.268738</td>\n",
       "      <td>0.394633</td>\n",
       "      <td>10.164231</td>\n",
       "      <td>-1.291137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.745228</td>\n",
       "      <td>1.417632</td>\n",
       "      <td>-0.568103</td>\n",
       "      <td>1.587920</td>\n",
       "      <td>0.524498</td>\n",
       "      <td>-0.577031</td>\n",
       "      <td>-0.383793</td>\n",
       "      <td>-0.320234</td>\n",
       "      <td>0.302293</td>\n",
       "      <td>-0.219335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-2.525143</td>\n",
       "      <td>-0.910554</td>\n",
       "      <td>1.210569</td>\n",
       "      <td>-0.964601</td>\n",
       "      <td>-0.090364</td>\n",
       "      <td>1.118677</td>\n",
       "      <td>-1.489971</td>\n",
       "      <td>0.849557</td>\n",
       "      <td>-1.018076</td>\n",
       "      <td>-0.524164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.398595</td>\n",
       "      <td>0.276723</td>\n",
       "      <td>0.035183</td>\n",
       "      <td>1.336024</td>\n",
       "      <td>-0.645716</td>\n",
       "      <td>0.253858</td>\n",
       "      <td>0.796127</td>\n",
       "      <td>0.405467</td>\n",
       "      <td>-2.985014</td>\n",
       "      <td>-1.069049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.390805</td>\n",
       "      <td>-0.023197</td>\n",
       "      <td>-0.235252</td>\n",
       "      <td>0.108171</td>\n",
       "      <td>0.583996</td>\n",
       "      <td>0.316043</td>\n",
       "      <td>-0.752512</td>\n",
       "      <td>1.716071</td>\n",
       "      <td>-0.599202</td>\n",
       "      <td>0.630379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.004160</td>\n",
       "      <td>-0.530262</td>\n",
       "      <td>-1.400227</td>\n",
       "      <td>0.292895</td>\n",
       "      <td>-0.249032</td>\n",
       "      <td>-0.848342</td>\n",
       "      <td>0.034091</td>\n",
       "      <td>0.145516</td>\n",
       "      <td>1.613590</td>\n",
       "      <td>0.620720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.140832</td>\n",
       "      <td>0.864161</td>\n",
       "      <td>8.054819</td>\n",
       "      <td>-2.158888</td>\n",
       "      <td>-0.249040</td>\n",
       "      <td>-0.124840</td>\n",
       "      <td>2.148132</td>\n",
       "      <td>-0.006123</td>\n",
       "      <td>-1.222020</td>\n",
       "      <td>-1.841521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.318043</td>\n",
       "      <td>-1.117706</td>\n",
       "      <td>-0.297660</td>\n",
       "      <td>0.781853</td>\n",
       "      <td>0.921182</td>\n",
       "      <td>-0.175719</td>\n",
       "      <td>0.378235</td>\n",
       "      <td>-0.081950</td>\n",
       "      <td>0.111074</td>\n",
       "      <td>0.369674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.455244</td>\n",
       "      <td>5.220614</td>\n",
       "      <td>0.648882</td>\n",
       "      <td>-0.830256</td>\n",
       "      <td>-0.487040</td>\n",
       "      <td>-1.074441</td>\n",
       "      <td>-0.752524</td>\n",
       "      <td>-1.370879</td>\n",
       "      <td>0.444401</td>\n",
       "      <td>1.673213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.503575</td>\n",
       "      <td>-0.483869</td>\n",
       "      <td>-0.817739</td>\n",
       "      <td>-1.334062</td>\n",
       "      <td>1.062239</td>\n",
       "      <td>-0.588338</td>\n",
       "      <td>-0.973750</td>\n",
       "      <td>-1.132587</td>\n",
       "      <td>1.303986</td>\n",
       "      <td>0.437264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-0.173051</td>\n",
       "      <td>-0.420479</td>\n",
       "      <td>0.690481</td>\n",
       "      <td>-1.602742</td>\n",
       "      <td>-0.484828</td>\n",
       "      <td>-1.046169</td>\n",
       "      <td>-2.006185</td>\n",
       "      <td>-2.183232</td>\n",
       "      <td>0.999874</td>\n",
       "      <td>0.553129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.439137</td>\n",
       "      <td>-1.307849</td>\n",
       "      <td>1.116959</td>\n",
       "      <td>-1.115746</td>\n",
       "      <td>-0.445159</td>\n",
       "      <td>0.169076</td>\n",
       "      <td>0.230750</td>\n",
       "      <td>0.134690</td>\n",
       "      <td>-0.447995</td>\n",
       "      <td>-1.146297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-0.301936</td>\n",
       "      <td>-1.497992</td>\n",
       "      <td>0.420045</td>\n",
       "      <td>-1.149326</td>\n",
       "      <td>1.754208</td>\n",
       "      <td>-0.085279</td>\n",
       "      <td>0.329069</td>\n",
       "      <td>0.979529</td>\n",
       "      <td>0.426184</td>\n",
       "      <td>0.591754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-0.366373</td>\n",
       "      <td>-0.990934</td>\n",
       "      <td>-1.730651</td>\n",
       "      <td>0.059770</td>\n",
       "      <td>0.028645</td>\n",
       "      <td>0.197339</td>\n",
       "      <td>-0.678772</td>\n",
       "      <td>1.163668</td>\n",
       "      <td>0.229457</td>\n",
       "      <td>0.446922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.011950</td>\n",
       "      <td>-1.814921</td>\n",
       "      <td>0.440853</td>\n",
       "      <td>1.940570</td>\n",
       "      <td>0.207150</td>\n",
       "      <td>1.441841</td>\n",
       "      <td>0.624058</td>\n",
       "      <td>-3.407183</td>\n",
       "      <td>2.824692</td>\n",
       "      <td>-0.373833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-0.269714</td>\n",
       "      <td>-1.624765</td>\n",
       "      <td>1.231369</td>\n",
       "      <td>1.806225</td>\n",
       "      <td>0.187322</td>\n",
       "      <td>-0.909542</td>\n",
       "      <td>1.705661</td>\n",
       "      <td>1.618584</td>\n",
       "      <td>-2.292940</td>\n",
       "      <td>-1.184921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-0.124722</td>\n",
       "      <td>0.657028</td>\n",
       "      <td>-1.244209</td>\n",
       "      <td>-0.511199</td>\n",
       "      <td>0.564158</td>\n",
       "      <td>0.158762</td>\n",
       "      <td>-0.580452</td>\n",
       "      <td>0.156350</td>\n",
       "      <td>-0.253166</td>\n",
       "      <td>0.350365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.326361</td>\n",
       "      <td>-3.843182</td>\n",
       "      <td>-0.100026</td>\n",
       "      <td>0.278072</td>\n",
       "      <td>-0.665547</td>\n",
       "      <td>-1.412602</td>\n",
       "      <td>0.501151</td>\n",
       "      <td>2.214304</td>\n",
       "      <td>2.332968</td>\n",
       "      <td>-1.513225</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          OI        NI       UPS      HSIC       CCE       IBM       MAS  \\\n",
       "0  -0.173054 -1.290853  0.555273 -0.863849 -1.002730  1.638692 -0.015072   \n",
       "1  -0.817457  1.561382 -0.807345 -1.720293  0.345989 -2.662752 -1.268738   \n",
       "2   0.745228  1.417632 -0.568103  1.587920  0.524498 -0.577031 -0.383793   \n",
       "3  -2.525143 -0.910554  1.210569 -0.964601 -0.090364  1.118677 -1.489971   \n",
       "4  -0.398595  0.276723  0.035183  1.336024 -0.645716  0.253858  0.796127   \n",
       "5   0.390805 -0.023197 -0.235252  0.108171  0.583996  0.316043 -0.752512   \n",
       "6   0.004160 -0.530262 -1.400227  0.292895 -0.249032 -0.848342  0.034091   \n",
       "7  -0.140832  0.864161  8.054819 -2.158888 -0.249040 -0.124840  2.148132   \n",
       "8  -0.318043 -1.117706 -0.297660  0.781853  0.921182 -0.175719  0.378235   \n",
       "9   0.455244  5.220614  0.648882 -0.830256 -0.487040 -1.074441 -0.752524   \n",
       "10  0.503575 -0.483869 -0.817739 -1.334062  1.062239 -0.588338 -0.973750   \n",
       "11 -0.173051 -0.420479  0.690481 -1.602742 -0.484828 -1.046169 -2.006185   \n",
       "12  0.439137 -1.307849  1.116959 -1.115746 -0.445159  0.169076  0.230750   \n",
       "13 -0.301936 -1.497992  0.420045 -1.149326  1.754208 -0.085279  0.329069   \n",
       "14 -0.366373 -0.990934 -1.730651  0.059770  0.028645  0.197339 -0.678772   \n",
       "15  0.011950 -1.814921  0.440853  1.940570  0.207150  1.441841  0.624058   \n",
       "16 -0.269714 -1.624765  1.231369  1.806225  0.187322 -0.909542  1.705661   \n",
       "17 -0.124722  0.657028 -1.244209 -0.511199  0.564158  0.158762 -0.580452   \n",
       "18  0.326361 -3.843182 -0.100026  0.278072 -0.665547 -1.412602  0.501151   \n",
       "\n",
       "         EFX       AAPL       WDC  \n",
       "0  -0.677670   5.693126 -0.808344  \n",
       "1   0.394633  10.164231 -1.291137  \n",
       "2  -0.320234   0.302293 -0.219335  \n",
       "3   0.849557  -1.018076 -0.524164  \n",
       "4   0.405467  -2.985014 -1.069049  \n",
       "5   1.716071  -0.599202  0.630379  \n",
       "6   0.145516   1.613590  0.620720  \n",
       "7  -0.006123  -1.222020 -1.841521  \n",
       "8  -0.081950   0.111074  0.369674  \n",
       "9  -1.370879   0.444401  1.673213  \n",
       "10 -1.132587   1.303986  0.437264  \n",
       "11 -2.183232   0.999874  0.553129  \n",
       "12  0.134690  -0.447995 -1.146297  \n",
       "13  0.979529   0.426184  0.591754  \n",
       "14  1.163668   0.229457  0.446922  \n",
       "15 -3.407183   2.824692 -0.373833  \n",
       "16  1.618584  -2.292940 -1.184921  \n",
       "17  0.156350  -0.253166  0.350365  \n",
       "18  2.214304   2.332968 -1.513225  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OI</th>\n",
       "      <th>NI</th>\n",
       "      <th>UPS</th>\n",
       "      <th>HSIC</th>\n",
       "      <th>CCE</th>\n",
       "      <th>IBM</th>\n",
       "      <th>MAS</th>\n",
       "      <th>EFX</th>\n",
       "      <th>AAPL</th>\n",
       "      <th>WDC</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.173054</td>\n",
       "      <td>-1.290853</td>\n",
       "      <td>0.555273</td>\n",
       "      <td>-0.863849</td>\n",
       "      <td>-1.002730</td>\n",
       "      <td>1.638692</td>\n",
       "      <td>-0.015072</td>\n",
       "      <td>-0.677670</td>\n",
       "      <td>5.693126</td>\n",
       "      <td>-0.808344</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.817457</td>\n",
       "      <td>1.561382</td>\n",
       "      <td>-0.807345</td>\n",
       "      <td>-1.720293</td>\n",
       "      <td>0.345989</td>\n",
       "      <td>-2.662752</td>\n",
       "      <td>-1.268738</td>\n",
       "      <td>0.394633</td>\n",
       "      <td>10.164231</td>\n",
       "      <td>-1.291137</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.745228</td>\n",
       "      <td>1.417632</td>\n",
       "      <td>-0.568103</td>\n",
       "      <td>1.587920</td>\n",
       "      <td>0.524498</td>\n",
       "      <td>-0.577031</td>\n",
       "      <td>-0.383793</td>\n",
       "      <td>-0.320234</td>\n",
       "      <td>0.302293</td>\n",
       "      <td>-0.219335</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-2.525143</td>\n",
       "      <td>-0.910554</td>\n",
       "      <td>1.210569</td>\n",
       "      <td>-0.964601</td>\n",
       "      <td>-0.090364</td>\n",
       "      <td>1.118677</td>\n",
       "      <td>-1.489971</td>\n",
       "      <td>0.849557</td>\n",
       "      <td>-1.018076</td>\n",
       "      <td>-0.524164</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.398595</td>\n",
       "      <td>0.276723</td>\n",
       "      <td>0.035183</td>\n",
       "      <td>1.336024</td>\n",
       "      <td>-0.645716</td>\n",
       "      <td>0.253858</td>\n",
       "      <td>0.796127</td>\n",
       "      <td>0.405467</td>\n",
       "      <td>-2.985014</td>\n",
       "      <td>-1.069049</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.390805</td>\n",
       "      <td>-0.023197</td>\n",
       "      <td>-0.235252</td>\n",
       "      <td>0.108171</td>\n",
       "      <td>0.583996</td>\n",
       "      <td>0.316043</td>\n",
       "      <td>-0.752512</td>\n",
       "      <td>1.716071</td>\n",
       "      <td>-0.599202</td>\n",
       "      <td>0.630379</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.004160</td>\n",
       "      <td>-0.530262</td>\n",
       "      <td>-1.400227</td>\n",
       "      <td>0.292895</td>\n",
       "      <td>-0.249032</td>\n",
       "      <td>-0.848342</td>\n",
       "      <td>0.034091</td>\n",
       "      <td>0.145516</td>\n",
       "      <td>1.613590</td>\n",
       "      <td>0.620720</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.140832</td>\n",
       "      <td>0.864161</td>\n",
       "      <td>8.054819</td>\n",
       "      <td>-2.158888</td>\n",
       "      <td>-0.249040</td>\n",
       "      <td>-0.124840</td>\n",
       "      <td>2.148132</td>\n",
       "      <td>-0.006123</td>\n",
       "      <td>-1.222020</td>\n",
       "      <td>-1.841521</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.318043</td>\n",
       "      <td>-1.117706</td>\n",
       "      <td>-0.297660</td>\n",
       "      <td>0.781853</td>\n",
       "      <td>0.921182</td>\n",
       "      <td>-0.175719</td>\n",
       "      <td>0.378235</td>\n",
       "      <td>-0.081950</td>\n",
       "      <td>0.111074</td>\n",
       "      <td>0.369674</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.455244</td>\n",
       "      <td>5.220614</td>\n",
       "      <td>0.648882</td>\n",
       "      <td>-0.830256</td>\n",
       "      <td>-0.487040</td>\n",
       "      <td>-1.074441</td>\n",
       "      <td>-0.752524</td>\n",
       "      <td>-1.370879</td>\n",
       "      <td>0.444401</td>\n",
       "      <td>1.673213</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.503575</td>\n",
       "      <td>-0.483869</td>\n",
       "      <td>-0.817739</td>\n",
       "      <td>-1.334062</td>\n",
       "      <td>1.062239</td>\n",
       "      <td>-0.588338</td>\n",
       "      <td>-0.973750</td>\n",
       "      <td>-1.132587</td>\n",
       "      <td>1.303986</td>\n",
       "      <td>0.437264</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-0.173051</td>\n",
       "      <td>-0.420479</td>\n",
       "      <td>0.690481</td>\n",
       "      <td>-1.602742</td>\n",
       "      <td>-0.484828</td>\n",
       "      <td>-1.046169</td>\n",
       "      <td>-2.006185</td>\n",
       "      <td>-2.183232</td>\n",
       "      <td>0.999874</td>\n",
       "      <td>0.553129</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.439137</td>\n",
       "      <td>-1.307849</td>\n",
       "      <td>1.116959</td>\n",
       "      <td>-1.115746</td>\n",
       "      <td>-0.445159</td>\n",
       "      <td>0.169076</td>\n",
       "      <td>0.230750</td>\n",
       "      <td>0.134690</td>\n",
       "      <td>-0.447995</td>\n",
       "      <td>-1.146297</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-0.301936</td>\n",
       "      <td>-1.497992</td>\n",
       "      <td>0.420045</td>\n",
       "      <td>-1.149326</td>\n",
       "      <td>1.754208</td>\n",
       "      <td>-0.085279</td>\n",
       "      <td>0.329069</td>\n",
       "      <td>0.979529</td>\n",
       "      <td>0.426184</td>\n",
       "      <td>0.591754</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-0.366373</td>\n",
       "      <td>-0.990934</td>\n",
       "      <td>-1.730651</td>\n",
       "      <td>0.059770</td>\n",
       "      <td>0.028645</td>\n",
       "      <td>0.197339</td>\n",
       "      <td>-0.678772</td>\n",
       "      <td>1.163668</td>\n",
       "      <td>0.229457</td>\n",
       "      <td>0.446922</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.011950</td>\n",
       "      <td>-1.814921</td>\n",
       "      <td>0.440853</td>\n",
       "      <td>1.940570</td>\n",
       "      <td>0.207150</td>\n",
       "      <td>1.441841</td>\n",
       "      <td>0.624058</td>\n",
       "      <td>-3.407183</td>\n",
       "      <td>2.824692</td>\n",
       "      <td>-0.373833</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-0.269714</td>\n",
       "      <td>-1.624765</td>\n",
       "      <td>1.231369</td>\n",
       "      <td>1.806225</td>\n",
       "      <td>0.187322</td>\n",
       "      <td>-0.909542</td>\n",
       "      <td>1.705661</td>\n",
       "      <td>1.618584</td>\n",
       "      <td>-2.292940</td>\n",
       "      <td>-1.184921</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-0.124722</td>\n",
       "      <td>0.657028</td>\n",
       "      <td>-1.244209</td>\n",
       "      <td>-0.511199</td>\n",
       "      <td>0.564158</td>\n",
       "      <td>0.158762</td>\n",
       "      <td>-0.580452</td>\n",
       "      <td>0.156350</td>\n",
       "      <td>-0.253166</td>\n",
       "      <td>0.350365</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.326361</td>\n",
       "      <td>-3.843182</td>\n",
       "      <td>-0.100026</td>\n",
       "      <td>0.278072</td>\n",
       "      <td>-0.665547</td>\n",
       "      <td>-1.412602</td>\n",
       "      <td>0.501151</td>\n",
       "      <td>2.214304</td>\n",
       "      <td>2.332968</td>\n",
       "      <td>-1.513225</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.421053</td>\n",
       "      <td>0.315789</td>\n",
       "      <td>0.526316</td>\n",
       "      <td>0.473684</td>\n",
       "      <td>0.526316</td>\n",
       "      <td>0.421053</td>\n",
       "      <td>0.473684</td>\n",
       "      <td>0.578947</td>\n",
       "      <td>0.631579</td>\n",
       "      <td>0.473684</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          OI        NI       UPS      HSIC       CCE       IBM       MAS  \\\n",
       "0  -0.173054 -1.290853  0.555273 -0.863849 -1.002730  1.638692 -0.015072   \n",
       "1  -0.817457  1.561382 -0.807345 -1.720293  0.345989 -2.662752 -1.268738   \n",
       "2   0.745228  1.417632 -0.568103  1.587920  0.524498 -0.577031 -0.383793   \n",
       "3  -2.525143 -0.910554  1.210569 -0.964601 -0.090364  1.118677 -1.489971   \n",
       "4  -0.398595  0.276723  0.035183  1.336024 -0.645716  0.253858  0.796127   \n",
       "5   0.390805 -0.023197 -0.235252  0.108171  0.583996  0.316043 -0.752512   \n",
       "6   0.004160 -0.530262 -1.400227  0.292895 -0.249032 -0.848342  0.034091   \n",
       "7  -0.140832  0.864161  8.054819 -2.158888 -0.249040 -0.124840  2.148132   \n",
       "8  -0.318043 -1.117706 -0.297660  0.781853  0.921182 -0.175719  0.378235   \n",
       "9   0.455244  5.220614  0.648882 -0.830256 -0.487040 -1.074441 -0.752524   \n",
       "10  0.503575 -0.483869 -0.817739 -1.334062  1.062239 -0.588338 -0.973750   \n",
       "11 -0.173051 -0.420479  0.690481 -1.602742 -0.484828 -1.046169 -2.006185   \n",
       "12  0.439137 -1.307849  1.116959 -1.115746 -0.445159  0.169076  0.230750   \n",
       "13 -0.301936 -1.497992  0.420045 -1.149326  1.754208 -0.085279  0.329069   \n",
       "14 -0.366373 -0.990934 -1.730651  0.059770  0.028645  0.197339 -0.678772   \n",
       "15  0.011950 -1.814921  0.440853  1.940570  0.207150  1.441841  0.624058   \n",
       "16 -0.269714 -1.624765  1.231369  1.806225  0.187322 -0.909542  1.705661   \n",
       "17 -0.124722  0.657028 -1.244209 -0.511199  0.564158  0.158762 -0.580452   \n",
       "18  0.326361 -3.843182 -0.100026  0.278072 -0.665547 -1.412602  0.501151   \n",
       "0   0.421053  0.315789  0.526316  0.473684  0.526316  0.421053  0.473684   \n",
       "\n",
       "         EFX       AAPL       WDC  Accuracy  \n",
       "0  -0.677670   5.693126 -0.808344       0.3  \n",
       "1   0.394633  10.164231 -1.291137       0.4  \n",
       "2  -0.320234   0.302293 -0.219335       0.5  \n",
       "3   0.849557  -1.018076 -0.524164       0.3  \n",
       "4   0.405467  -2.985014 -1.069049       0.6  \n",
       "5   1.716071  -0.599202  0.630379       0.6  \n",
       "6   0.145516   1.613590  0.620720       0.6  \n",
       "7  -0.006123  -1.222020 -1.841521       0.3  \n",
       "8  -0.081950   0.111074  0.369674       0.5  \n",
       "9  -1.370879   0.444401  1.673213       0.5  \n",
       "10 -1.132587   1.303986  0.437264       0.4  \n",
       "11 -2.183232   0.999874  0.553129       0.3  \n",
       "12  0.134690  -0.447995 -1.146297       0.5  \n",
       "13  0.979529   0.426184  0.591754       0.6  \n",
       "14  1.163668   0.229457  0.446922       0.6  \n",
       "15 -3.407183   2.824692 -0.373833       0.7  \n",
       "16  1.618584  -2.292940 -1.184921       0.5  \n",
       "17  0.156350  -0.253166  0.350365       0.5  \n",
       "18  2.214304   2.332968 -1.513225       0.5  \n",
       "0   0.578947   0.631579  0.473684       1.0  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r=[]\n",
    "A = Accuracy\n",
    "for i in range(A.shape[1]):\n",
    "    m = A.iloc[:,i]\n",
    "    r.append(len(m[m>-0.0])/len(m))\n",
    "A=A.append(pd.DataFrame([r], columns=stocks))\n",
    "r=[]\n",
    "for i in range(A.shape[0]):\n",
    "    m = A.iloc[i,:]\n",
    "    r.append(len(m[m>-0.0])/len(m))\n",
    "A['Accuracy'] = r\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OI</th>\n",
       "      <th>NI</th>\n",
       "      <th>UPS</th>\n",
       "      <th>HSIC</th>\n",
       "      <th>CCE</th>\n",
       "      <th>IBM</th>\n",
       "      <th>MAS</th>\n",
       "      <th>EFX</th>\n",
       "      <th>AAPL</th>\n",
       "      <th>WDC</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.173054</td>\n",
       "      <td>1.290853</td>\n",
       "      <td>0.555273</td>\n",
       "      <td>0.863849</td>\n",
       "      <td>1.002730</td>\n",
       "      <td>-1.638692</td>\n",
       "      <td>0.015072</td>\n",
       "      <td>-0.677670</td>\n",
       "      <td>-5.693126</td>\n",
       "      <td>-0.808344</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.817457</td>\n",
       "      <td>-1.561382</td>\n",
       "      <td>-0.807345</td>\n",
       "      <td>-1.720293</td>\n",
       "      <td>0.345989</td>\n",
       "      <td>2.662752</td>\n",
       "      <td>1.268738</td>\n",
       "      <td>0.394633</td>\n",
       "      <td>-10.164231</td>\n",
       "      <td>-1.291137</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.745228</td>\n",
       "      <td>1.417632</td>\n",
       "      <td>-0.568103</td>\n",
       "      <td>1.587920</td>\n",
       "      <td>0.524498</td>\n",
       "      <td>0.577031</td>\n",
       "      <td>0.383793</td>\n",
       "      <td>-0.320234</td>\n",
       "      <td>-0.302293</td>\n",
       "      <td>-0.219335</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.525143</td>\n",
       "      <td>0.910554</td>\n",
       "      <td>-1.210569</td>\n",
       "      <td>0.964601</td>\n",
       "      <td>-0.090364</td>\n",
       "      <td>-1.118677</td>\n",
       "      <td>1.489971</td>\n",
       "      <td>0.849557</td>\n",
       "      <td>1.018076</td>\n",
       "      <td>-0.524164</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.398595</td>\n",
       "      <td>0.276723</td>\n",
       "      <td>0.035183</td>\n",
       "      <td>-1.336024</td>\n",
       "      <td>0.645716</td>\n",
       "      <td>-0.253858</td>\n",
       "      <td>-0.796127</td>\n",
       "      <td>-0.405467</td>\n",
       "      <td>2.985014</td>\n",
       "      <td>1.069049</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.390805</td>\n",
       "      <td>0.023197</td>\n",
       "      <td>-0.235252</td>\n",
       "      <td>0.108171</td>\n",
       "      <td>-0.583996</td>\n",
       "      <td>-0.316043</td>\n",
       "      <td>0.752512</td>\n",
       "      <td>-1.716071</td>\n",
       "      <td>0.599202</td>\n",
       "      <td>-0.630379</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.004160</td>\n",
       "      <td>0.530262</td>\n",
       "      <td>-1.400227</td>\n",
       "      <td>0.292895</td>\n",
       "      <td>0.249032</td>\n",
       "      <td>0.848342</td>\n",
       "      <td>-0.034091</td>\n",
       "      <td>-0.145516</td>\n",
       "      <td>-1.613590</td>\n",
       "      <td>-0.620720</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.140832</td>\n",
       "      <td>-0.864161</td>\n",
       "      <td>8.054819</td>\n",
       "      <td>-2.158888</td>\n",
       "      <td>0.249040</td>\n",
       "      <td>0.124840</td>\n",
       "      <td>-2.148132</td>\n",
       "      <td>-0.006123</td>\n",
       "      <td>-1.222020</td>\n",
       "      <td>1.841521</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.318043</td>\n",
       "      <td>-1.117706</td>\n",
       "      <td>-0.297660</td>\n",
       "      <td>-0.781853</td>\n",
       "      <td>-0.921182</td>\n",
       "      <td>-0.175719</td>\n",
       "      <td>-0.378235</td>\n",
       "      <td>-0.081950</td>\n",
       "      <td>-0.111074</td>\n",
       "      <td>-0.369674</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.455244</td>\n",
       "      <td>5.220614</td>\n",
       "      <td>0.648882</td>\n",
       "      <td>0.830256</td>\n",
       "      <td>0.487040</td>\n",
       "      <td>-1.074441</td>\n",
       "      <td>0.752524</td>\n",
       "      <td>-1.370879</td>\n",
       "      <td>0.444401</td>\n",
       "      <td>-1.673213</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-0.503575</td>\n",
       "      <td>-0.483869</td>\n",
       "      <td>-0.817739</td>\n",
       "      <td>-1.334062</td>\n",
       "      <td>1.062239</td>\n",
       "      <td>-0.588338</td>\n",
       "      <td>0.973750</td>\n",
       "      <td>1.132587</td>\n",
       "      <td>-1.303986</td>\n",
       "      <td>-0.437264</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.173051</td>\n",
       "      <td>-0.420479</td>\n",
       "      <td>0.690481</td>\n",
       "      <td>1.602742</td>\n",
       "      <td>-0.484828</td>\n",
       "      <td>-1.046169</td>\n",
       "      <td>2.006185</td>\n",
       "      <td>2.183232</td>\n",
       "      <td>0.999874</td>\n",
       "      <td>-0.553129</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-0.439137</td>\n",
       "      <td>-1.307849</td>\n",
       "      <td>1.116959</td>\n",
       "      <td>1.115746</td>\n",
       "      <td>-0.445159</td>\n",
       "      <td>0.169076</td>\n",
       "      <td>-0.230750</td>\n",
       "      <td>-0.134690</td>\n",
       "      <td>-0.447995</td>\n",
       "      <td>1.146297</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.301936</td>\n",
       "      <td>-1.497992</td>\n",
       "      <td>0.420045</td>\n",
       "      <td>1.149326</td>\n",
       "      <td>-1.754208</td>\n",
       "      <td>-0.085279</td>\n",
       "      <td>-0.329069</td>\n",
       "      <td>-0.979529</td>\n",
       "      <td>0.426184</td>\n",
       "      <td>-0.591754</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.366373</td>\n",
       "      <td>-0.990934</td>\n",
       "      <td>1.730651</td>\n",
       "      <td>-0.059770</td>\n",
       "      <td>-0.028645</td>\n",
       "      <td>-0.197339</td>\n",
       "      <td>0.678772</td>\n",
       "      <td>-1.163668</td>\n",
       "      <td>-0.229457</td>\n",
       "      <td>0.446922</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.011950</td>\n",
       "      <td>-1.814921</td>\n",
       "      <td>0.440853</td>\n",
       "      <td>-1.940570</td>\n",
       "      <td>-0.207150</td>\n",
       "      <td>1.441841</td>\n",
       "      <td>-0.624058</td>\n",
       "      <td>3.407183</td>\n",
       "      <td>-2.824692</td>\n",
       "      <td>0.373833</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.269714</td>\n",
       "      <td>-1.624765</td>\n",
       "      <td>1.231369</td>\n",
       "      <td>-1.806225</td>\n",
       "      <td>-0.187322</td>\n",
       "      <td>-0.909542</td>\n",
       "      <td>-1.705661</td>\n",
       "      <td>1.618584</td>\n",
       "      <td>2.292940</td>\n",
       "      <td>1.184921</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.124722</td>\n",
       "      <td>0.657028</td>\n",
       "      <td>-1.244209</td>\n",
       "      <td>0.511199</td>\n",
       "      <td>-0.564158</td>\n",
       "      <td>0.158762</td>\n",
       "      <td>0.580452</td>\n",
       "      <td>0.156350</td>\n",
       "      <td>0.253166</td>\n",
       "      <td>-0.350365</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-0.326361</td>\n",
       "      <td>-3.843182</td>\n",
       "      <td>-0.100026</td>\n",
       "      <td>-0.278072</td>\n",
       "      <td>0.665547</td>\n",
       "      <td>-1.412602</td>\n",
       "      <td>-0.501151</td>\n",
       "      <td>2.214304</td>\n",
       "      <td>-2.332968</td>\n",
       "      <td>-1.513225</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.398595</td>\n",
       "      <td>-1.751519</td>\n",
       "      <td>2.645988</td>\n",
       "      <td>4.390376</td>\n",
       "      <td>-0.722835</td>\n",
       "      <td>0.011795</td>\n",
       "      <td>0.703358</td>\n",
       "      <td>0.697919</td>\n",
       "      <td>-2.296549</td>\n",
       "      <td>-1.184921</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>-0.068602</td>\n",
       "      <td>0.149950</td>\n",
       "      <td>-0.287264</td>\n",
       "      <td>-0.225708</td>\n",
       "      <td>-0.881510</td>\n",
       "      <td>-0.785190</td>\n",
       "      <td>-1.705651</td>\n",
       "      <td>-0.903710</td>\n",
       "      <td>-1.303972</td>\n",
       "      <td>0.630378</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>-0.519687</td>\n",
       "      <td>-0.800778</td>\n",
       "      <td>0.534464</td>\n",
       "      <td>0.849027</td>\n",
       "      <td>1.062229</td>\n",
       "      <td>-0.723005</td>\n",
       "      <td>1.662046</td>\n",
       "      <td>-2.150739</td>\n",
       "      <td>3.012318</td>\n",
       "      <td>0.296584</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>-0.181374</td>\n",
       "      <td>1.751538</td>\n",
       "      <td>-0.141634</td>\n",
       "      <td>-0.865823</td>\n",
       "      <td>-0.385649</td>\n",
       "      <td>-0.332999</td>\n",
       "      <td>-0.058680</td>\n",
       "      <td>-1.305885</td>\n",
       "      <td>-1.841229</td>\n",
       "      <td>-0.215183</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.269714</td>\n",
       "      <td>0.800778</td>\n",
       "      <td>-0.380882</td>\n",
       "      <td>-0.681099</td>\n",
       "      <td>-0.464997</td>\n",
       "      <td>-0.519528</td>\n",
       "      <td>-1.140281</td>\n",
       "      <td>0.818476</td>\n",
       "      <td>0.398866</td>\n",
       "      <td>-0.335207</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>-0.020270</td>\n",
       "      <td>1.688148</td>\n",
       "      <td>0.347243</td>\n",
       "      <td>0.662342</td>\n",
       "      <td>-1.615372</td>\n",
       "      <td>0.102235</td>\n",
       "      <td>-0.451976</td>\n",
       "      <td>1.466937</td>\n",
       "      <td>-1.176487</td>\n",
       "      <td>-1.899461</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.406915</td>\n",
       "      <td>1.181089</td>\n",
       "      <td>-2.617221</td>\n",
       "      <td>0.897428</td>\n",
       "      <td>0.249032</td>\n",
       "      <td>-0.655188</td>\n",
       "      <td>-1.096656</td>\n",
       "      <td>0.255239</td>\n",
       "      <td>0.289584</td>\n",
       "      <td>-0.315893</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>-0.278034</td>\n",
       "      <td>0.420479</td>\n",
       "      <td>-1.254614</td>\n",
       "      <td>-1.100923</td>\n",
       "      <td>0.130032</td>\n",
       "      <td>-0.299078</td>\n",
       "      <td>-2.221865</td>\n",
       "      <td>0.439379</td>\n",
       "      <td>-2.014248</td>\n",
       "      <td>0.697969</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>-0.648568</td>\n",
       "      <td>2.892428</td>\n",
       "      <td>-0.672118</td>\n",
       "      <td>0.259301</td>\n",
       "      <td>0.526710</td>\n",
       "      <td>0.158753</td>\n",
       "      <td>0.383793</td>\n",
       "      <td>0.201089</td>\n",
       "      <td>2.547910</td>\n",
       "      <td>0.866276</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>-0.124722</td>\n",
       "      <td>1.607775</td>\n",
       "      <td>-3.657381</td>\n",
       "      <td>1.115745</td>\n",
       "      <td>1.320076</td>\n",
       "      <td>-0.243544</td>\n",
       "      <td>-0.378245</td>\n",
       "      <td>-3.687374</td>\n",
       "      <td>0.535467</td>\n",
       "      <td>-0.350357</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.809670</td>\n",
       "      <td>0.276723</td>\n",
       "      <td>0.274424</td>\n",
       "      <td>1.535570</td>\n",
       "      <td>-1.357526</td>\n",
       "      <td>0.435721</td>\n",
       "      <td>0.850854</td>\n",
       "      <td>-1.414205</td>\n",
       "      <td>0.990758</td>\n",
       "      <td>-0.002754</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>-0.100820</td>\n",
       "      <td>-0.103557</td>\n",
       "      <td>-0.256061</td>\n",
       "      <td>-0.009379</td>\n",
       "      <td>-0.107982</td>\n",
       "      <td>-0.362239</td>\n",
       "      <td>0.310041</td>\n",
       "      <td>-1.890781</td>\n",
       "      <td>-0.693865</td>\n",
       "      <td>2.063609</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>-0.551907</td>\n",
       "      <td>4.523393</td>\n",
       "      <td>-1.233805</td>\n",
       "      <td>1.401222</td>\n",
       "      <td>-1.278188</td>\n",
       "      <td>-0.565733</td>\n",
       "      <td>-0.624058</td>\n",
       "      <td>7.772227</td>\n",
       "      <td>2.821097</td>\n",
       "      <td>1.155956</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>-0.189164</td>\n",
       "      <td>1.544392</td>\n",
       "      <td>-2.045129</td>\n",
       "      <td>0.124967</td>\n",
       "      <td>0.663334</td>\n",
       "      <td>0.367897</td>\n",
       "      <td>0.580452</td>\n",
       "      <td>0.764316</td>\n",
       "      <td>0.134782</td>\n",
       "      <td>-0.997307</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.358588</td>\n",
       "      <td>1.607775</td>\n",
       "      <td>-0.016820</td>\n",
       "      <td>1.166122</td>\n",
       "      <td>-0.048483</td>\n",
       "      <td>0.610943</td>\n",
       "      <td>1.858692</td>\n",
       "      <td>2.681474</td>\n",
       "      <td>1.828534</td>\n",
       "      <td>1.001466</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.221386</td>\n",
       "      <td>3.572646</td>\n",
       "      <td>-0.453692</td>\n",
       "      <td>0.527995</td>\n",
       "      <td>-0.603826</td>\n",
       "      <td>0.356589</td>\n",
       "      <td>0.875430</td>\n",
       "      <td>0.331067</td>\n",
       "      <td>-0.047338</td>\n",
       "      <td>-2.262222</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>-0.672469</td>\n",
       "      <td>0.086580</td>\n",
       "      <td>-0.162442</td>\n",
       "      <td>-4.056485</td>\n",
       "      <td>0.229200</td>\n",
       "      <td>-0.745618</td>\n",
       "      <td>-2.197288</td>\n",
       "      <td>-0.632925</td>\n",
       "      <td>1.482497</td>\n",
       "      <td>2.932631</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>-0.535797</td>\n",
       "      <td>-1.497992</td>\n",
       "      <td>-1.067376</td>\n",
       "      <td>2.660692</td>\n",
       "      <td>-0.544334</td>\n",
       "      <td>0.786165</td>\n",
       "      <td>1.489974</td>\n",
       "      <td>0.634336</td>\n",
       "      <td>-0.912416</td>\n",
       "      <td>-0.022062</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>-0.793563</td>\n",
       "      <td>0.213346</td>\n",
       "      <td>1.533017</td>\n",
       "      <td>0.748272</td>\n",
       "      <td>0.328369</td>\n",
       "      <td>-1.373031</td>\n",
       "      <td>-0.869882</td>\n",
       "      <td>-1.434444</td>\n",
       "      <td>0.471717</td>\n",
       "      <td>-0.118619</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>-0.140832</td>\n",
       "      <td>-1.561382</td>\n",
       "      <td>0.243220</td>\n",
       "      <td>-0.160523</td>\n",
       "      <td>-0.405489</td>\n",
       "      <td>-0.248217</td>\n",
       "      <td>0.924596</td>\n",
       "      <td>0.569350</td>\n",
       "      <td>1.646414</td>\n",
       "      <td>-1.088365</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>-0.285825</td>\n",
       "      <td>-1.117700</td>\n",
       "      <td>0.409642</td>\n",
       "      <td>-0.210900</td>\n",
       "      <td>0.169702</td>\n",
       "      <td>-0.344298</td>\n",
       "      <td>-0.058680</td>\n",
       "      <td>-1.206986</td>\n",
       "      <td>0.517237</td>\n",
       "      <td>-1.020774</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.068599</td>\n",
       "      <td>-1.688148</td>\n",
       "      <td>-0.880148</td>\n",
       "      <td>1.168096</td>\n",
       "      <td>0.288703</td>\n",
       "      <td>-0.242559</td>\n",
       "      <td>1.317901</td>\n",
       "      <td>0.092776</td>\n",
       "      <td>-0.566379</td>\n",
       "      <td>-0.591754</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>-0.092500</td>\n",
       "      <td>0.023197</td>\n",
       "      <td>0.201613</td>\n",
       "      <td>0.294856</td>\n",
       "      <td>-0.266652</td>\n",
       "      <td>-0.231261</td>\n",
       "      <td>-1.803981</td>\n",
       "      <td>0.947036</td>\n",
       "      <td>-1.303972</td>\n",
       "      <td>-1.277327</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>-0.874113</td>\n",
       "      <td>-0.213346</td>\n",
       "      <td>0.183249</td>\n",
       "      <td>-0.832242</td>\n",
       "      <td>-0.464995</td>\n",
       "      <td>0.791823</td>\n",
       "      <td>-0.304490</td>\n",
       "      <td>1.901614</td>\n",
       "      <td>-1.404141</td>\n",
       "      <td>-1.702180</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.028062</td>\n",
       "      <td>-0.420479</td>\n",
       "      <td>0.308064</td>\n",
       "      <td>-0.597131</td>\n",
       "      <td>-0.229200</td>\n",
       "      <td>-0.242559</td>\n",
       "      <td>0.850841</td>\n",
       "      <td>0.016956</td>\n",
       "      <td>-3.025041</td>\n",
       "      <td>1.725648</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.382483</td>\n",
       "      <td>0.276723</td>\n",
       "      <td>-1.158558</td>\n",
       "      <td>0.227696</td>\n",
       "      <td>-0.725055</td>\n",
       "      <td>-0.078645</td>\n",
       "      <td>-1.017365</td>\n",
       "      <td>0.190254</td>\n",
       "      <td>0.617419</td>\n",
       "      <td>0.499356</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>-0.149152</td>\n",
       "      <td>1.861314</td>\n",
       "      <td>-0.609710</td>\n",
       "      <td>0.110132</td>\n",
       "      <td>-0.784553</td>\n",
       "      <td>0.497899</td>\n",
       "      <td>1.367067</td>\n",
       "      <td>0.461046</td>\n",
       "      <td>0.407968</td>\n",
       "      <td>3.884400</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.495260</td>\n",
       "      <td>1.037320</td>\n",
       "      <td>2.958039</td>\n",
       "      <td>-1.487167</td>\n",
       "      <td>0.742673</td>\n",
       "      <td>-0.321692</td>\n",
       "      <td>-0.722387</td>\n",
       "      <td>0.547692</td>\n",
       "      <td>0.662952</td>\n",
       "      <td>1.779428</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.011950</td>\n",
       "      <td>0.530262</td>\n",
       "      <td>-1.743482</td>\n",
       "      <td>-0.611953</td>\n",
       "      <td>0.048475</td>\n",
       "      <td>-0.085279</td>\n",
       "      <td>-1.066531</td>\n",
       "      <td>1.358633</td>\n",
       "      <td>-0.347841</td>\n",
       "      <td>2.223597</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.124722</td>\n",
       "      <td>-0.103570</td>\n",
       "      <td>-0.162441</td>\n",
       "      <td>0.511197</td>\n",
       "      <td>-0.288703</td>\n",
       "      <td>0.061688</td>\n",
       "      <td>0.506700</td>\n",
       "      <td>0.134683</td>\n",
       "      <td>0.234964</td>\n",
       "      <td>-0.248302</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.489796</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.367347</td>\n",
       "      <td>0.489796</td>\n",
       "      <td>0.591837</td>\n",
       "      <td>0.510204</td>\n",
       "      <td>0.387755</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          OI        NI       UPS      HSIC       CCE       IBM       MAS  \\\n",
       "0  -0.173054  1.290853  0.555273  0.863849  1.002730 -1.638692  0.015072   \n",
       "1  -0.817457 -1.561382 -0.807345 -1.720293  0.345989  2.662752  1.268738   \n",
       "2   0.745228  1.417632 -0.568103  1.587920  0.524498  0.577031  0.383793   \n",
       "3   2.525143  0.910554 -1.210569  0.964601 -0.090364 -1.118677  1.489971   \n",
       "4   0.398595  0.276723  0.035183 -1.336024  0.645716 -0.253858 -0.796127   \n",
       "5   0.390805  0.023197 -0.235252  0.108171 -0.583996 -0.316043  0.752512   \n",
       "6  -0.004160  0.530262 -1.400227  0.292895  0.249032  0.848342 -0.034091   \n",
       "7   0.140832 -0.864161  8.054819 -2.158888  0.249040  0.124840 -2.148132   \n",
       "8  -0.318043 -1.117706 -0.297660 -0.781853 -0.921182 -0.175719 -0.378235   \n",
       "9   0.455244  5.220614  0.648882  0.830256  0.487040 -1.074441  0.752524   \n",
       "10 -0.503575 -0.483869 -0.817739 -1.334062  1.062239 -0.588338  0.973750   \n",
       "11  0.173051 -0.420479  0.690481  1.602742 -0.484828 -1.046169  2.006185   \n",
       "12 -0.439137 -1.307849  1.116959  1.115746 -0.445159  0.169076 -0.230750   \n",
       "13  0.301936 -1.497992  0.420045  1.149326 -1.754208 -0.085279 -0.329069   \n",
       "14  0.366373 -0.990934  1.730651 -0.059770 -0.028645 -0.197339  0.678772   \n",
       "15  0.011950 -1.814921  0.440853 -1.940570 -0.207150  1.441841 -0.624058   \n",
       "16  0.269714 -1.624765  1.231369 -1.806225 -0.187322 -0.909542 -1.705661   \n",
       "17  0.124722  0.657028 -1.244209  0.511199 -0.564158  0.158762  0.580452   \n",
       "18 -0.326361 -3.843182 -0.100026 -0.278072  0.665547 -1.412602 -0.501151   \n",
       "19  0.398595 -1.751519  2.645988  4.390376 -0.722835  0.011795  0.703358   \n",
       "20 -0.068602  0.149950 -0.287264 -0.225708 -0.881510 -0.785190 -1.705651   \n",
       "21 -0.519687 -0.800778  0.534464  0.849027  1.062229 -0.723005  1.662046   \n",
       "22 -0.181374  1.751538 -0.141634 -0.865823 -0.385649 -0.332999 -0.058680   \n",
       "23  0.269714  0.800778 -0.380882 -0.681099 -0.464997 -0.519528 -1.140281   \n",
       "24 -0.020270  1.688148  0.347243  0.662342 -1.615372  0.102235 -0.451976   \n",
       "25  0.406915  1.181089 -2.617221  0.897428  0.249032 -0.655188 -1.096656   \n",
       "26 -0.278034  0.420479 -1.254614 -1.100923  0.130032 -0.299078 -2.221865   \n",
       "27 -0.648568  2.892428 -0.672118  0.259301  0.526710  0.158753  0.383793   \n",
       "28 -0.124722  1.607775 -3.657381  1.115745  1.320076 -0.243544 -0.378245   \n",
       "29  0.809670  0.276723  0.274424  1.535570 -1.357526  0.435721  0.850854   \n",
       "30 -0.100820 -0.103557 -0.256061 -0.009379 -0.107982 -0.362239  0.310041   \n",
       "31 -0.551907  4.523393 -1.233805  1.401222 -1.278188 -0.565733 -0.624058   \n",
       "32 -0.189164  1.544392 -2.045129  0.124967  0.663334  0.367897  0.580452   \n",
       "33  0.358588  1.607775 -0.016820  1.166122 -0.048483  0.610943  1.858692   \n",
       "34  0.221386  3.572646 -0.453692  0.527995 -0.603826  0.356589  0.875430   \n",
       "35 -0.672469  0.086580 -0.162442 -4.056485  0.229200 -0.745618 -2.197288   \n",
       "36 -0.535797 -1.497992 -1.067376  2.660692 -0.544334  0.786165  1.489974   \n",
       "37 -0.793563  0.213346  1.533017  0.748272  0.328369 -1.373031 -0.869882   \n",
       "38 -0.140832 -1.561382  0.243220 -0.160523 -0.405489 -0.248217  0.924596   \n",
       "39 -0.285825 -1.117700  0.409642 -0.210900  0.169702 -0.344298 -0.058680   \n",
       "40  0.068599 -1.688148 -0.880148  1.168096  0.288703 -0.242559  1.317901   \n",
       "41 -0.092500  0.023197  0.201613  0.294856 -0.266652 -0.231261 -1.803981   \n",
       "42 -0.874113 -0.213346  0.183249 -0.832242 -0.464995  0.791823 -0.304490   \n",
       "43  0.028062 -0.420479  0.308064 -0.597131 -0.229200 -0.242559  0.850841   \n",
       "44  0.382483  0.276723 -1.158558  0.227696 -0.725055 -0.078645 -1.017365   \n",
       "45 -0.149152  1.861314 -0.609710  0.110132 -0.784553  0.497899  1.367067   \n",
       "46  0.495260  1.037320  2.958039 -1.487167  0.742673 -0.321692 -0.722387   \n",
       "47  0.011950  0.530262 -1.743482 -0.611953  0.048475 -0.085279 -1.066531   \n",
       "48  0.124722 -0.103570 -0.162441  0.511197 -0.288703  0.061688  0.506700   \n",
       "0   0.489796  0.571429  0.428571  0.571429  0.428571  0.367347  0.489796   \n",
       "\n",
       "         EFX       AAPL       WDC  Accuracy  \n",
       "0  -0.677670  -5.693126 -0.808344       0.5  \n",
       "1   0.394633 -10.164231 -1.291137       0.4  \n",
       "2  -0.320234  -0.302293 -0.219335       0.6  \n",
       "3   0.849557   1.018076 -0.524164       0.6  \n",
       "4  -0.405467   2.985014  1.069049       0.6  \n",
       "5  -1.716071   0.599202 -0.630379       0.5  \n",
       "6  -0.145516  -1.613590 -0.620720       0.4  \n",
       "7  -0.006123  -1.222020  1.841521       0.5  \n",
       "8  -0.081950  -0.111074 -0.369674       0.0  \n",
       "9  -1.370879   0.444401 -1.673213       0.7  \n",
       "10  1.132587  -1.303986 -0.437264       0.3  \n",
       "11  2.183232   0.999874 -0.553129       0.6  \n",
       "12 -0.134690  -0.447995  1.146297       0.4  \n",
       "13 -0.979529   0.426184 -0.591754       0.4  \n",
       "14 -1.163668  -0.229457  0.446922       0.4  \n",
       "15  3.407183  -2.824692  0.373833       0.5  \n",
       "16  1.618584   2.292940  1.184921       0.5  \n",
       "17  0.156350   0.253166 -0.350365       0.7  \n",
       "18  2.214304  -2.332968 -1.513225       0.2  \n",
       "19  0.697919  -2.296549 -1.184921       0.6  \n",
       "20 -0.903710  -1.303972  0.630378       0.2  \n",
       "21 -2.150739   3.012318  0.296584       0.6  \n",
       "22 -1.305885  -1.841229 -0.215183       0.1  \n",
       "23  0.818476   0.398866 -0.335207       0.4  \n",
       "24  1.466937  -1.176487 -1.899461       0.5  \n",
       "25  0.255239   0.289584 -0.315893       0.6  \n",
       "26  0.439379  -2.014248  0.697969       0.4  \n",
       "27  0.201089   2.547910  0.866276       0.8  \n",
       "28 -3.687374   0.535467 -0.350357       0.4  \n",
       "29 -1.414205   0.990758 -0.002754       0.7  \n",
       "30 -1.890781  -0.693865  2.063609       0.2  \n",
       "31  7.772227   2.821097  1.155956       0.5  \n",
       "32  0.764316   0.134782 -0.997307       0.7  \n",
       "33  2.681474   1.828534  1.001466       0.8  \n",
       "34  0.331067  -0.047338 -2.262222       0.6  \n",
       "35 -0.632925   1.482497  2.932631       0.4  \n",
       "36  0.634336  -0.912416 -0.022062       0.4  \n",
       "37 -1.434444   0.471717 -0.118619       0.5  \n",
       "38  0.569350   1.646414 -1.088365       0.4  \n",
       "39 -1.206986   0.517237 -1.020774       0.3  \n",
       "40  0.092776  -0.566379 -0.591754       0.5  \n",
       "41  0.947036  -1.303972 -1.277327       0.4  \n",
       "42  1.901614  -1.404141 -1.702180       0.3  \n",
       "43  0.016956  -3.025041  1.725648       0.5  \n",
       "44  0.190254   0.617419  0.499356       0.6  \n",
       "45  0.461046   0.407968  3.884400       0.7  \n",
       "46  0.547692   0.662952  1.779428       0.7  \n",
       "47  1.358633  -0.347841  2.223597       0.5  \n",
       "48  0.134683   0.234964 -0.248302       0.6  \n",
       "0   0.591837   0.510204  0.387755       1.0  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r=[]\n",
    "A = Accuracy\n",
    "for i in range(A.shape[1]):\n",
    "    m = A.iloc[:,i]\n",
    "    r.append(len(m[m>-0.0])/len(m))\n",
    "A=A.append(pd.DataFrame([r], columns=stocks))\n",
    "r=[]\n",
    "for i in range(A.shape[0]):\n",
    "    m = A.iloc[i,:]\n",
    "    r.append(len(m[m>-0.0])/len(m))\n",
    "A['Accuracy'] = r\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  5.],\n",
       "       [ 2.,  6.],\n",
       "       [ 3.,  7.],\n",
       "       [ 4.,  8.]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test=np.empty((4,0))\n",
    "test = np.append(test,np.array([[1],[2],[3],[4]]),axis=1)\n",
    "test = np.append(test,np.array([[5],[6],[7],[8]]),axis=1)\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aalahgholipour160413\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:27: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4198, 44, 14) (4198,)\n",
      "(4193, 44, 14) (4193,)\n",
      "(4136, 44, 14) (4136,)\n",
      "(4193, 44, 14) (4193,)\n",
      "(4198, 44, 14) (4198,)\n",
      "(4193, 44, 14) (4193,)\n",
      "(4193, 44, 14) (4193,)\n",
      "(4193, 44, 14) (4193,)\n",
      "(4202, 44, 14) (4202,)\n",
      "(4193, 44, 14) (4193,)\n",
      "Train on 37306 samples, validate on 4146 samples\n",
      "Epoch 1/50\n",
      "37306/37306 [==============================] - 80s 2ms/step - loss: 1.3774 - val_loss: 0.9745\n",
      "Epoch 2/50\n",
      "37306/37306 [==============================] - 74s 2ms/step - loss: 1.0848 - val_loss: 0.9785\n",
      "Epoch 3/50\n",
      "37306/37306 [==============================] - 75s 2ms/step - loss: 1.0220 - val_loss: 0.9648\n",
      "Epoch 4/50\n",
      "37306/37306 [==============================] - 74s 2ms/step - loss: 1.0000 - val_loss: 0.9608\n",
      "Epoch 5/50\n",
      "37306/37306 [==============================] - 74s 2ms/step - loss: 0.9866 - val_loss: 0.9623\n",
      "Epoch 6/50\n",
      "37306/37306 [==============================] - 72s 2ms/step - loss: 0.9842 - val_loss: 0.9607\n",
      "Epoch 7/50\n",
      "37306/37306 [==============================] - 72s 2ms/step - loss: 0.9785 - val_loss: 0.9615\n",
      "Epoch 8/50\n",
      "37306/37306 [==============================] - 72s 2ms/step - loss: 0.9754 - val_loss: 0.9628\n",
      "Epoch 9/50\n",
      "37306/37306 [==============================] - 72s 2ms/step - loss: 0.9722 - val_loss: 0.9604\n",
      "Epoch 10/50\n",
      "37306/37306 [==============================] - 72s 2ms/step - loss: 0.9712 - val_loss: 0.9594\n",
      "Epoch 11/50\n",
      "37306/37306 [==============================] - 72s 2ms/step - loss: 0.9671 - val_loss: 0.9641\n",
      "Epoch 12/50\n",
      "37306/37306 [==============================] - 71s 2ms/step - loss: 0.9668 - val_loss: 0.9652\n",
      "Epoch 13/50\n",
      "37306/37306 [==============================] - 71s 2ms/step - loss: 0.9662 - val_loss: 0.9644\n",
      "Epoch 14/50\n",
      "37306/37306 [==============================] - 72s 2ms/step - loss: 0.9622 - val_loss: 0.9621\n",
      "Epoch 15/50\n",
      "37306/37306 [==============================] - 72s 2ms/step - loss: 0.9603 - val_loss: 0.9635\n",
      "Epoch 16/50\n",
      "37306/37306 [==============================] - 72s 2ms/step - loss: 0.9572 - val_loss: 0.9647\n",
      "Epoch 17/50\n",
      "37306/37306 [==============================] - 72s 2ms/step - loss: 0.9564 - val_loss: 0.9647\n",
      "Epoch 18/50\n",
      "37306/37306 [==============================] - 72s 2ms/step - loss: 0.9514 - val_loss: 0.9619\n",
      "Epoch 19/50\n",
      "37306/37306 [==============================] - 72s 2ms/step - loss: 0.9480 - val_loss: 0.9651\n",
      "Epoch 20/50\n",
      "37306/37306 [==============================] - 72s 2ms/step - loss: 0.9424 - val_loss: 0.9690\n",
      "Epoch 21/50\n",
      "37306/37306 [==============================] - 72s 2ms/step - loss: 0.9382 - val_loss: 0.9727\n",
      "Epoch 22/50\n",
      "37306/37306 [==============================] - 73s 2ms/step - loss: 0.9324 - val_loss: 0.9738\n",
      "Epoch 23/50\n",
      "37306/37306 [==============================] - 72s 2ms/step - loss: 0.9220 - val_loss: 0.9740\n",
      "Epoch 24/50\n",
      "37306/37306 [==============================] - 72s 2ms/step - loss: 0.9135 - val_loss: 0.9911\n",
      "Epoch 25/50\n",
      "37306/37306 [==============================] - 72s 2ms/step - loss: 0.9013 - val_loss: 0.9786\n",
      "Epoch 26/50\n",
      "37306/37306 [==============================] - 72s 2ms/step - loss: 0.8869 - val_loss: 0.9906\n",
      "Epoch 27/50\n",
      "37306/37306 [==============================] - 72s 2ms/step - loss: 0.8704 - val_loss: 0.9927\n",
      "Epoch 28/50\n",
      "37306/37306 [==============================] - 72s 2ms/step - loss: 0.8478 - val_loss: 0.9982\n",
      "Epoch 29/50\n",
      "37306/37306 [==============================] - 72s 2ms/step - loss: 0.8231 - val_loss: 1.0141\n",
      "Epoch 30/50\n",
      "37306/37306 [==============================] - 72s 2ms/step - loss: 0.7937 - val_loss: 1.0240\n",
      "Epoch 31/50\n",
      "37306/37306 [==============================] - 72s 2ms/step - loss: 0.7747 - val_loss: 1.0410\n",
      "Epoch 32/50\n",
      "37306/37306 [==============================] - 72s 2ms/step - loss: 0.7347 - val_loss: 1.0579\n",
      "Epoch 33/50\n",
      "37306/37306 [==============================] - 72s 2ms/step - loss: 0.7094 - val_loss: 1.0724\n",
      "Epoch 34/50\n",
      "37306/37306 [==============================] - 72s 2ms/step - loss: 0.6724 - val_loss: 1.0678\n",
      "Epoch 35/50\n",
      "37306/37306 [==============================] - 72s 2ms/step - loss: 0.6388 - val_loss: 1.0899\n",
      "Epoch 36/50\n",
      "37306/37306 [==============================] - 72s 2ms/step - loss: 0.6113 - val_loss: 1.0943\n",
      "Epoch 37/50\n",
      "37306/37306 [==============================] - 72s 2ms/step - loss: 0.5701 - val_loss: 1.1637\n",
      "Epoch 38/50\n",
      "37306/37306 [==============================] - 72s 2ms/step - loss: 0.5511 - val_loss: 1.1408\n",
      "Epoch 39/50\n",
      "37306/37306 [==============================] - 72s 2ms/step - loss: 0.5322 - val_loss: 1.1084\n",
      "Epoch 40/50\n",
      "37306/37306 [==============================] - 72s 2ms/step - loss: 0.4955 - val_loss: 1.1289\n",
      "Epoch 41/50\n",
      "37306/37306 [==============================] - 72s 2ms/step - loss: 0.4754 - val_loss: 1.1309\n",
      "Epoch 42/50\n",
      "37306/37306 [==============================] - 73s 2ms/step - loss: 0.4650 - val_loss: 1.1456\n",
      "Epoch 43/50\n",
      "37306/37306 [==============================] - 79s 2ms/step - loss: 0.4453 - val_loss: 1.1522\n",
      "Epoch 44/50\n",
      "37306/37306 [==============================] - 75s 2ms/step - loss: 0.4300 - val_loss: 1.1532\n",
      "Epoch 45/50\n",
      "37306/37306 [==============================] - 74s 2ms/step - loss: 0.4141 - val_loss: 1.1333\n",
      "Epoch 46/50\n",
      "37306/37306 [==============================] - 74s 2ms/step - loss: 0.3988 - val_loss: 1.1700\n",
      "Epoch 47/50\n",
      "37306/37306 [==============================] - 73s 2ms/step - loss: 0.3914 - val_loss: 1.1764\n",
      "Epoch 48/50\n",
      "37306/37306 [==============================] - 74s 2ms/step - loss: 0.3707 - val_loss: 1.1946\n",
      "Epoch 49/50\n",
      "37306/37306 [==============================] - 74s 2ms/step - loss: 0.3610 - val_loss: 1.2098\n",
      "Epoch 50/50\n",
      "37306/37306 [==============================] - 74s 2ms/step - loss: 0.3589 - val_loss: 1.1486\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WDC test_accuracy 0.4818181818181818\n"
     ]
    }
   ],
   "source": [
    "stocks=['OI','NI','UPS','HSIC','CCE','IBM','MAS','EFX','AAPL','WDC']\n",
    "trainX=np.empty((0,44,14))\n",
    "trainy=np.empty((0,))\n",
    "testX=np.empty((0,44,14))\n",
    "testy=np.empty((0,))\n",
    "Scaled2={}\n",
    "status1='reg'\n",
    "status2= 'timeseriesgen'\n",
    "for Stock in stocks:\n",
    "    scaled = data_prepare(Stock)\n",
    "    scaled2, scaled, train_X, train_y, test_X, test_y = data_gen(scaled, 44, status1, status2, 44)\n",
    "    trainX = np.append(trainX, train_X, axis=0)\n",
    "    trainy = np.append(trainy, train_y.reshape(-1), axis=0)\n",
    "    testX = np.append(testX, test_X, axis=0)\n",
    "    testy = np.append(testy, test_y.reshape(-1), axis=0)\n",
    "    Scaled2[Stock] = scaled2\n",
    "model = MODEL(status1)\n",
    "model, history = train_model(model, trainX, trainy,status1)\n",
    "plot_loss(history)\n",
    "model.save_weights(Stock+status2+'_'+status1+'.h5')\n",
    "# for scaled2 in Scaled2:\n",
    "#     model_accuracy = acc(model, Scaled2[scaled2],status1, 44)\n",
    "#     print(scaled2+' accuracy', model_accuracy)\n",
    "model_test_accuracy = test_acc(model, testX, testy,status1)\n",
    "print(Stock +' test_accuracy', model_test_accuracy)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OI accuracy 0.4\n",
      "NI accuracy 0.6\n",
      "UPS accuracy 0.6\n",
      "HSIC accuracy 0.6666666666666666\n",
      "CCE accuracy 0.5333333333333333\n",
      "IBM accuracy 0.6\n",
      "MAS accuracy 0.6\n",
      "EFX accuracy 0.4666666666666667\n",
      "AAPL accuracy 0.6\n",
      "WDC accuracy 0.6\n",
      "WDC test_accuracy 0.8866666666666667\n"
     ]
    }
   ],
   "source": [
    "for scaled2 in Scaled2:\n",
    "    model_accuracy = acc(model, Scaled2[scaled2],status1, 15)\n",
    "    print(scaled2+' accuracy', model_accuracy)\n",
    "model_test_accuracy = test_acc(model, testX, testy,status1)\n",
    "print(Stock +' test_accuracy', model_test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aalahgholipour160413\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:189: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4127, 15, 56) (4127,)\n",
      "Train on 3714 samples, validate on 413 samples\n",
      "Epoch 1/2\n",
      "3714/3714 [==============================] - 9s 2ms/step - loss: 6.6482 - val_loss: 1.5815\n",
      "Epoch 2/2\n",
      "3714/3714 [==============================] - 4s 956us/step - loss: 3.9093 - val_loss: 1.3367\n",
      "OI accuracy -0.00195905602995\n",
      "(4127, 15, 56) (4127,)\n",
      "Train on 3714 samples, validate on 413 samples\n",
      "Epoch 1/2\n",
      "3714/3714 [==============================] - 9s 2ms/step - loss: 8.3682 - val_loss: 1.6051\n",
      "Epoch 2/2\n",
      "3714/3714 [==============================] - 3s 935us/step - loss: 3.4188 - val_loss: 0.3667\n",
      "NI accuracy 0.00816602727034\n",
      "(4127, 15, 56) (4127,)\n",
      "Train on 3714 samples, validate on 413 samples\n",
      "Epoch 1/2\n",
      "3714/3714 [==============================] - 11s 3ms/step - loss: 3.4832 - val_loss: 0.4290\n",
      "Epoch 2/2\n",
      "3714/3714 [==============================] - 4s 952us/step - loss: 1.9429 - val_loss: 0.2691\n",
      "UPS accuracy 0.00574176863165\n",
      "(4127, 15, 56) (4127,)\n",
      "Train on 3714 samples, validate on 413 samples\n",
      "Epoch 1/2\n",
      "3714/3714 [==============================] - 10s 3ms/step - loss: 3.7447 - val_loss: 0.0442\n",
      "Epoch 2/2\n",
      "3714/3714 [==============================] - 3s 933us/step - loss: 1.5307 - val_loss: 0.0162\n",
      "HSIC accuracy 0.00621171408659\n",
      "(4127, 15, 56) (4127,)\n",
      "Train on 3714 samples, validate on 413 samples\n",
      "Epoch 1/2\n",
      "3714/3714 [==============================] - 11s 3ms/step - loss: 3.9192 - val_loss: 0.5345\n",
      "Epoch 2/2\n",
      "3714/3714 [==============================] - 3s 921us/step - loss: 1.7473 - val_loss: 0.3432\n",
      "CCE accuracy 0.0107805090725\n",
      "(4127, 15, 56) (4127,)\n",
      "Train on 3714 samples, validate on 413 samples\n",
      "Epoch 1/2\n",
      "3714/3714 [==============================] - 11s 3ms/step - loss: 5.0130 - val_loss: 0.9314\n",
      "Epoch 2/2\n",
      "3714/3714 [==============================] - 4s 948us/step - loss: 2.6428 - val_loss: 0.3781\n",
      "IBM accuracy 0.0179804575752\n",
      "(4127, 15, 56) (4127,)\n",
      "Train on 3714 samples, validate on 413 samples\n",
      "Epoch 1/2\n",
      "3714/3714 [==============================] - 11s 3ms/step - loss: 5.2004 - val_loss: 1.6531\n",
      "Epoch 2/2\n",
      "3714/3714 [==============================] - 3s 935us/step - loss: 2.8440 - val_loss: 0.8738\n",
      "MAS accuracy 0.000232361244653\n",
      "(4127, 15, 56) (4127,)\n",
      "Train on 3714 samples, validate on 413 samples\n",
      "Epoch 1/2\n",
      "3714/3714 [==============================] - 13s 4ms/step - loss: 3.4388 - val_loss: 0.0371\n",
      "Epoch 2/2\n",
      "3714/3714 [==============================] - 4s 971us/step - loss: 1.4417 - val_loss: 0.0194\n",
      "EFX accuracy 0.0050761501\n",
      "(4127, 15, 56) (4127,)\n",
      "Train on 3714 samples, validate on 413 samples\n",
      "Epoch 1/2\n",
      "3714/3714 [==============================] - 13s 3ms/step - loss: 5.2291 - val_loss: 0.6084\n",
      "Epoch 2/2\n",
      "3714/3714 [==============================] - 4s 978us/step - loss: 2.6325 - val_loss: 0.2708\n",
      "AAPL accuracy -0.0304549738895\n",
      "(4127, 15, 56) (4127,)\n",
      "Train on 3714 samples, validate on 413 samples\n",
      "Epoch 1/2\n",
      "3714/3714 [==============================] - 13s 4ms/step - loss: 7.6433 - val_loss: 1.6492\n",
      "Epoch 2/2\n",
      "3714/3714 [==============================] - 4s 979us/step - loss: 3.9048 - val_loss: 1.3003\n",
      "WDC accuracy -0.00730642437233\n",
      "(4127, 15, 56) (4127,)\n",
      "Train on 3714 samples, validate on 413 samples\n",
      "Epoch 1/2\n",
      "3714/3714 [==============================] - 14s 4ms/step - loss: 3.8280 - val_loss: 0.4516\n",
      "Epoch 2/2\n",
      "3714/3714 [==============================] - 4s 952us/step - loss: 1.8304 - val_loss: 0.1919\n",
      "OI accuracy -0.00908277794897\n",
      "(4127, 15, 56) (4127,)\n",
      "Train on 3714 samples, validate on 413 samples\n",
      "Epoch 1/2\n",
      "3714/3714 [==============================] - 14s 4ms/step - loss: 5.3178 - val_loss: 0.1851\n",
      "Epoch 2/2\n",
      "3714/3714 [==============================] - 4s 952us/step - loss: 2.3517 - val_loss: 0.0745\n",
      "NI accuracy 0.0102075851258\n",
      "(4127, 15, 56) (4127,)\n",
      "Train on 3714 samples, validate on 413 samples\n",
      "Epoch 1/2\n",
      "3714/3714 [==============================] - 15s 4ms/step - loss: 10.8434 - val_loss: 2.8137\n",
      "Epoch 2/2\n",
      "3714/3714 [==============================] - 4s 974us/step - loss: 6.0957 - val_loss: 2.5895\n",
      "UPS accuracy -0.00793405390753\n",
      "(4127, 15, 56) (4127,)\n",
      "Train on 3714 samples, validate on 413 samples\n",
      "Epoch 1/2\n",
      "3714/3714 [==============================] - 16s 4ms/step - loss: 4.6671 - val_loss: 0.1567\n",
      "Epoch 2/2\n",
      "3714/3714 [==============================] - 4s 992us/step - loss: 1.7186 - val_loss: 0.0498\n",
      "HSIC accuracy 0.0121890585865\n",
      "(4127, 15, 56) (4127,)\n",
      "Train on 3714 samples, validate on 413 samples\n",
      "Epoch 1/2\n",
      "3714/3714 [==============================] - 16s 4ms/step - loss: 4.1647 - val_loss: 1.3340\n",
      "Epoch 2/2\n",
      "3714/3714 [==============================] - 4s 970us/step - loss: 2.0223 - val_loss: 0.6709\n",
      "CCE accuracy -0.00388098326608\n",
      "(4127, 15, 56) (4127,)\n",
      "Train on 3714 samples, validate on 413 samples\n",
      "Epoch 1/2\n",
      "3714/3714 [==============================] - 17s 4ms/step - loss: 7.0462 - val_loss: 0.1796\n",
      "Epoch 2/2\n",
      "3714/3714 [==============================] - 4s 974us/step - loss: 3.2693 - val_loss: 0.0567\n",
      "IBM accuracy 0.0293660232185\n",
      "(4127, 15, 56) (4127,)\n",
      "Train on 3714 samples, validate on 413 samples\n",
      "Epoch 1/2\n",
      "3714/3714 [==============================] - 17s 5ms/step - loss: 5.3671 - val_loss: 0.6813\n",
      "Epoch 2/2\n",
      "3714/3714 [==============================] - 4s 990us/step - loss: 2.5497 - val_loss: 0.2928\n",
      "MAS accuracy -0.0120803918797\n",
      "(4127, 15, 56) (4127,)\n",
      "Train on 3714 samples, validate on 413 samples\n",
      "Epoch 1/2\n",
      "3714/3714 [==============================] - 18s 5ms/step - loss: 8.9775 - val_loss: 2.4999\n",
      "Epoch 2/2\n",
      "3714/3714 [==============================] - 4s 971us/step - loss: 3.7043 - val_loss: 1.0873\n",
      "EFX accuracy -0.00265518165264\n",
      "(4127, 15, 56) (4127,)\n",
      "Train on 3714 samples, validate on 413 samples\n",
      "Epoch 1/2\n",
      "3714/3714 [==============================] - 19s 5ms/step - loss: 4.1025 - val_loss: 0.4836\n",
      "Epoch 2/2\n",
      "3714/3714 [==============================] - 4s 980us/step - loss: 2.2627 - val_loss: 0.5442\n",
      "AAPL accuracy -0.0541905250657\n",
      "(4127, 15, 56) (4127,)\n",
      "Train on 3714 samples, validate on 413 samples\n",
      "Epoch 1/2\n",
      "3714/3714 [==============================] - 20s 5ms/step - loss: 6.0762 - val_loss: 0.0901\n",
      "Epoch 2/2\n",
      "3714/3714 [==============================] - 4s 1ms/step - loss: 2.8429 - val_loss: 0.0164\n",
      "WDC accuracy -0.0117615611847\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Length mismatch: Expected axis has 4 elements, new values have 2 elements",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-6be9d785d128>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0maccuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstocks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'reg'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'scaled2'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-22-691bb214271d>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(stocks, status1, status2)\u001b[0m\n\u001b[0;32m     19\u001b[0m             \u001b[1;31m#test_accuracy = test_accuracy.append(pd.DataFrame([[Stock, model_test_accuracy]]),ignore_index=True)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0maccuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mMul\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mMul\u001b[0m\u001b[1;33m>=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mMul\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m         \u001b[0maccuracy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Stock'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m     \u001b[1;31m#test_accuracy.columns=['Stock','test_accuracy']\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[1;31m#, test_accuracy\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m__setattr__\u001b[1;34m(self, name, value)\u001b[0m\n\u001b[0;32m   4383\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4384\u001b[0m             \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4385\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4386\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4387\u001b[0m             \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\properties.pyx\u001b[0m in \u001b[0;36mpandas._libs.properties.AxisProperty.__set__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_set_axis\u001b[1;34m(self, axis, labels)\u001b[0m\n\u001b[0;32m    643\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    644\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_set_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 645\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    646\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_clear_item_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    647\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\internals.py\u001b[0m in \u001b[0;36mset_axis\u001b[1;34m(self, axis, new_labels)\u001b[0m\n\u001b[0;32m   3321\u001b[0m             raise ValueError(\n\u001b[0;32m   3322\u001b[0m                 \u001b[1;34m'Length mismatch: Expected axis has {old} elements, new '\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3323\u001b[1;33m                 'values have {new} elements'.format(old=old_len, new=new_len))\n\u001b[0m\u001b[0;32m   3324\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3325\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_labels\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Length mismatch: Expected axis has 4 elements, new values have 2 elements"
     ]
    }
   ],
   "source": [
    "accuracy = train(stocks, 'reg', 'scaled2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
